BEGIN:VCALENDAR
VERSION:1.0
BEGIN:VEVENT
SUMMARY:Improved Dropout for Shallow and Deep Learning | Zhe Li \, Boqing 
 Gong \, Tianbao Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Improved Dropout for Shallow and Deep Learning\nZhe Li 
 \, Boqing Gong \, Tianbao Yang\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=7905\n\nDropout has been witnessed with great success in training
  deep neural networks by independently   zeroing  out the outputs of neuro
 ns at random. It has also received a surge of interest for shallow learnin
 g\, e.g.\, logistic regression.  However\, the independent  sampling for d
 ropout could be suboptimal for the sake of convergence. In this paper\, we
  propose to use multinomial  sampling for dropout\, i.e.\, sampling featur
 es or neurons according to  a multinomial distribution with different prob
 abilities for different features/neurons. To exhibit the optimal dropout p
 robabilities\, we analyze the shallow learning with multinomial  dropout a
 nd establish the risk bound for stochastic optimization. By minimizing a s
 ampling dependent factor in the risk bound\, we obtain a distribution-depe
 ndent dropout with sampling probabilities dependent on the second order st
 atistics of the data distribution. To tackle the issue of evolving  distri
 bution of neurons in deep learning\, we propose an efficient adaptive  dro
 pout (named \\textbf{evolutional dropout}) that computes the sampling prob
 abilities on-the-fly from a mini-batch of examples. Empirical studies on s
 everal benchmark datasets demonstrate that the proposed dropouts achieve  
 not only much faster convergence and  but also a smaller testing error tha
 n the standard dropout.  For example\, on the CIFAR-100 data\, the evoluti
 onal  dropout achieves relative improvements  over 10\\% on the prediction
  performance and over 50\\% on the convergence speed compared to the stand
 ard dropout.
LOCATION:Area 5+6+7+8 #1
END:VEVENT
BEGIN:VEVENT
SUMMARY:Communication-Optimal Distributed Clustering | Jiecao Chen \, He S
 un \, He Sun \, David Woodruff \, David Woodruff \, Qin Zhang \, Qin Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Communication-Optimal Distributed Clustering\nJiecao Ch
 en \, He Sun \, He Sun \, David Woodruff \, David Woodruff \, Qin Zhang \,
  Qin Zhang\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7906\n\nClu
 stering large datasets is a fundamental problem with a number of applicati
 ons in machine learning. Data is often collected on different sites and cl
 ustering needs to be performed in a distributed manner with low communicat
 ion. We would like the quality of the clustering in the distributed settin
 g to match that in the centralized setting for which all the data resides 
 on a single server. In this work\, we study both graph and geometric clust
 ering problems in two distributed models: (1) a point-to-point model\, and
  (2) a model with a broadcast channel. We give protocols in both models wh
 ich we show are nearly optimal by proving almost matching communication lo
 wer bounds. Our work highlights the surprising power of a broadcast channe
 l for clustering problems\; roughly speaking\, to cluster n points or n ve
 rtices in a graph distributed across s servers\, for a worst-case partitio
 ning the communication complexity in a point-to-point model is ns\, while 
 in the broadcast model it is n + s. We implement our algorithms and demons
 trate this phenomenon on real life datasets\, showing that our algorithms 
 are also very efficient in practice.
LOCATION:Area 5+6+7+8 #2
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Robustness of Kernel Clustering | Bowei Yan \, Purnamrita Sarka
 r \, Purnamrita Sarkar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:On Robustness of Kernel Clustering\nBowei Yan \, Purnam
 rita Sarkar \, Purnamrita Sarkar\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7907\n\nClustering is one of the most important unsupervised pr
 oblems in machine learning and statistics. Among many existing algorithms\
 , kernel \\km has drawn much research attention due to its ability to find
  non-linear cluster boundaries and inherent simplicity. There are two main
  approaches for kernel k-means: SVD of the kernel matrix and convex relaxa
 tions. Despite the attention kernel clustering has received both from theo
 retical and applied quarters\, not much is known about robustness of the m
 ethods. In this paper we first introduce a semidefinite programming relaxa
 tion for the kernel clustering problem\, then prove that under a suitable 
 model specification\, both the K-SVD and SDP approaches are consistent in 
 the limit\, albeit SDP is strongly consistent\, i.e. achieves exact recove
 ry\, whereas K-SVD is weakly consistent\, i.e. the fraction of misclassifi
 ed nodes vanish.
LOCATION:Area 5+6+7+8 #3
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combinatorial semi-bandit with known covariance | Rémy Degenne \,
  Vianney Perchet \, Vianney Perchet
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Combinatorial semi-bandit with known covariance\nRémy 
 Degenne \, Vianney Perchet \, Vianney Perchet\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=7909\n\nThe combinatorial stochastic semi-bandit p
 roblem is an extension of the classical multi-armed bandit problem in whic
 h an algorithm pulls more than one arm at each stage and the rewards of al
 l pulled arms are revealed. One difference with the single arm variant is 
 that the dependency structure of the arms is crucial. Previous works on th
 is setting either used a worst-case approach or imposed independence of th
 e arms. We introduce a way to quantify the dependency structure of the pro
 blem and design an algorithm that adapts to it. The algorithm is based on 
 linear regression and the analysis uses techniques from the linear bandit 
 literature. By comparing its performance to a new lower bound\, we prove t
 hat it is optimal\, up to a poly-logarithmic factor in the number of arms 
 pulled.
LOCATION:Area 5+6+7+8 #4
END:VEVENT
BEGIN:VEVENT
SUMMARY:A posteriori error bounds for joint matrix decomposition problems 
 | Nicolo Colombo \, Nikos Vlassis \, Nikos Vlassis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:A posteriori error bounds for joint matrix decompositio
 n problems\nNicolo Colombo \, Nikos Vlassis \, Nikos Vlassis\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=7911\n\nJoint matrix decomposition 
 problems appear frequently in statistics and engineering\, with notable ap
 plications in learning latent variable models and tensor factorization.  J
 oint triangularization of nearly diagonalizable matrices can be performed 
 via orthogonal matrices\, and the related nonconvex optimization problem i
 s more tractable than alternative approaches.  We carry out a perturbation
  analysis of the noisy joint matrix triangularization problem\, and we der
 ive a bound on the distance between any feasible approximate triangularize
 r and its noise-free counterpart.  The bound is an a posteriori one\, in t
 he sense that it is based on quantities that are observed (input matrices 
 and functions thereof)\, and moreover it is oblivious to the algorithm use
 d and/or the properties of the feasible solution (e.g.\, proximity to crit
 ical points) that are typical of existing bounds in the literature. To our
  knowledge\, this is the first a posteriori bound for joint matrix decompo
 sition. We discuss possible applications and we compare with analogous a p
 riori bounds for the same problem.
LOCATION:Area 5+6+7+8 #5
END:VEVENT
BEGIN:VEVENT
SUMMARY:Object based Scene Representations using Fisher Scores of Local Su
 bspace Projections | Mandar D Dixit \, Nuno Vasconcelos \, Nuno Vasconcelo
 s
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Object based Scene Representations using Fisher Scores 
 of Local Subspace Projections\nMandar D Dixit \, Nuno Vasconcelos \, Nuno 
 Vasconcelos\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7913\n\nSe
 veral works have shown that deep CNN classifiers can be easily transferred
  across datasets\, e.g. the transfer of a CNN trained to recognize objects
  on ImageNET to an object detector on Pascal VOC. Less clear\, however\, i
 s the ability of CNNs to transfer knowledge {\\it across\\/} tasks. A comm
 on example of such transfer is the problem of scene classification that sh
 ould leverage localized object detections to recognize holistic visual con
 cepts. While this problem is currently addressed with Fisher vector repres
 entations\, these are now shown ineffective for the high-dimensional and h
 ighly non-linear features extracted by modern CNNs. It is argued that this
  is mostly due to the reliance on a model\, the Gaussian mixture of diagon
 al covariances\, which has a very limited ability to capture the second or
 der statistics of CNN features. This problem is addressed by the adoption 
 of a better model\, the mixture of factor analyzers (MFA)\, which approxim
 ates the non-linear data manifold by a collection of local subspaces. The 
 Fisher score with respect to the MFA (MFA-FS) is derived and proposed as a
 n image representation for holistic image classifiers. Extensive experimen
 ts show that the MFA-FS has state of the art performance for object-to-sce
 ne transfer and this transfer actually {\\it outperforms\\/} the training 
 of a scene CNN from a large scene dataset. The two representations are als
 o shown to be {\\it complementary\,\\/} in the sense that their combinatio
 n outperforms each of the representations by itself. When combined\, they 
 produce a state of the art scene classifier.
LOCATION:Area 5+6+7+8 #6
END:VEVENT
BEGIN:VEVENT
SUMMARY:MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild 
 | Gregory Rogez \, Cordelia Schmid \, Cordelia Schmid
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:MoCap-guided Data Augmentation for 3D Pose Estimation i
 n the Wild\nGregory Rogez \, Cordelia Schmid \, Cordelia Schmid\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=7915\n\nIn this paper\, we addre
 ss the problem of 3D human pose understanding in the wild. The lack of lar
 ge enough training datasets has prevented the development of end-to-end ar
 chitectures based on deep Convolutionnal Neural Networks (CNN). Such metho
 ds require millions of training images that are very difficult to collect 
 and annotate with accurate 3D poses. We propose a solution to generate a l
 arge set of photorealistic synthetic images of humans with 3D pose annotat
 ions. At the heart of our approach is our image-based synthesis engine tha
 t artificially augments a dataset of real images with 2D annotations of bo
 dy poses using a library of 3D Motion Capture (MoCap) data. These syntheti
 c images are then used to train an end-to-end CNN for full-body 3D pose es
 timation. We cluster the training data into a large number K of pose class
 es and tackle pose estimation as a K-way classification problem. Such appr
 oach is tractable only with very large training datasets such as ours. Our
  method outperforms state-of-the-art results in terms of 3D pose estimatio
 n in controlled environments and show promising results for in-the-wild im
 ages.
LOCATION:Area 5+6+7+8 #7
END:VEVENT
BEGIN:VEVENT
SUMMARY:Regret of Queueing Bandits | Subhashini Krishnasamy \, Rajat Sen \
 , Rajat Sen \, Ramesh Johari \, Ramesh Johari \, Sanjay Shakkottai \, Sanj
 ay Shakkottai
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Regret of Queueing Bandits\nSubhashini Krishnasamy \, R
 ajat Sen \, Rajat Sen \, Ramesh Johari \, Ramesh Johari \, Sanjay Shakkott
 ai \, Sanjay Shakkottai\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7917\n\nWe consider a variant of the multiarmed bandit problem where job
 s queue for service\, and service rates of different servers may be unknow
 n.  We study algorithms that minimize queue-regret: the (expected) differe
 nce between the queue-lengths obtained by the algorithm\, and those obtain
 ed by a genie-aided matching algorithm that knows exact service rates.  A 
 naive view of this problem would suggest that queue-regret should grow log
 arithmically: since queue-regret cannot be larger than classical regret\, 
 results for the standard MAB problem give algorithms that ensure queue-reg
 ret increases no more than logarithmically in time. Our paper shows surpri
 singly more complex behavior.  In particular\, the naive intuition is corr
 ect as long as the bandit algorithm's queues have relatively long regenera
 tive cycles: in this case queue-regret is similar to cumulative regret\, a
 nd scales (essentially) logarithmically.  However\, we show that this "ear
 ly stage" of the queueing bandit eventually gives way to a "late stage"\, 
 where the optimal queue-regret scaling is O(1/t).  We demonstrate an algor
 ithm that (order-wise) achieves this asymptotic queue-regret\, and also ex
 hibits close to optimal switching time from the early stage to the late st
 age.
LOCATION:Area 5+6+7+8 #8
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient Nonparametric Smoothness Estimation | Shashank Singh \, 
 Simon S Du \, Simon S Du \, Barnabas Poczos \, Barnabas Poczos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Efficient Nonparametric Smoothness Estimation\nShashank
  Singh \, Simon S Du \, Simon S Du \, Barnabas Poczos \, Barnabas Poczos\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7919\n\nSobolev quantit
 ies (norms\, inner products\, and distances) of probability density functi
 ons are important in the theory of nonparametric statistics\, but have rar
 ely been used in practice\, partly due to a lack of practical estimators. 
 They also include\, as special cases\, L^2 quantities which are used in ma
 ny applications. We propose and analyze a family of estimators for Sobolev
  quantities of unknown probability density functions. We bound the bias an
 d variance of our estimators over finite samples\, finding that they are g
 enerally minimax rate-optimal. Our estimators are significantly more compu
 tationally tractable than previous estimators\, and exhibit a statistical/
 computational trade-off allowing them to adapt to computational constraint
 s. We also draw theoretical connections to recent work on fast two-sample 
 testing. Finally\, we empirically validate our estimators on synthetic dat
 a.
LOCATION:Area 5+6+7+8 #9
END:VEVENT
BEGIN:VEVENT
SUMMARY:Completely random measures for modelling block-structured sparse n
 etworks | Tue Herlau \, Mikkel N Schmidt \, Mikkel N Schmidt \, Morten Mø
 rup \, Morten Mørup
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Completely random measures for modelling block-structur
 ed sparse networks\nTue Herlau \, Mikkel N Schmidt \, Mikkel N Schmidt \, 
 Morten Mørup \, Morten Mørup\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=7921\n\nStatistical methods for network data often parameterize t
 he edge-probability by attributing latent traits to the vertices such as b
 lock structure and assume exchangeability in the sense of the Aldous-Hoove
 r representation theorem. These assumptions are however incompatible with 
 traits found in real-world network such as a power-law degree-distribution
 . Recently Caron &amp\; Fox (2014) proposed the use of a different notion 
 of exchangeability due to Kallenberg (2005) and obtained a network model w
 hich models edge-inhomogeneity such as power-law degree-distribution while
  retaining desirable statistical properties. However\, this model does not
  capture latent vertex traits such as block-structure. In this work we re-
 introduce the use of block-structure for network models obeying Kallenberg
 's notion of exchangeability and thereby obtain a collapsed model which ad
 mits the inference of block-structure and edge inhomogeneity. We derive a 
 simple expression for the likelihood and an efficient sampling method. The
  obtained model is not significantly more difficult to implement than exis
 ting approaches to block-modelling and performs well on real network datas
 ets.
LOCATION:Area 5+6+7+8 #10
END:VEVENT
BEGIN:VEVENT
SUMMARY:DISCO Nets : DISsimilarity COefficients Networks | Diane Bouchacou
 rt \, Pawan K Mudigonda \, Pawan K Mudigonda \, Sebastian Nowozin \, Sebas
 tian Nowozin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:DISCO Nets : DISsimilarity COefficients Networks\nDiane
  Bouchacourt \, Pawan K Mudigonda \, Pawan K Mudigonda \, Sebastian Nowozi
 n \, Sebastian Nowozin\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7923\n\nWe present a new type of probabilistic model which we call DISsim
 ilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficien
 tly sample from a posterior distribution parametrised by a neural network.
  During training\, DISCO Nets are learned by minimising the dissimilarity 
 coefficient between the true distribution and the estimated distribution. 
 This allows us to tailor the training to the loss related to the task at h
 and. We empirically show that (i) by modeling uncertainty on the output va
 lue\, DISCO Nets outperform equivalent non-probabilistic predictive networ
 ks and (ii) DISCO Nets accurately model the uncertainty of the output\, ou
 tperforming existing probabilistic models based on deep neural networks.
LOCATION:Area 5+6+7+8 #11
END:VEVENT
BEGIN:VEVENT
SUMMARY:An Architecture for Deep\, Hierarchical Generative Models | Philip
  Bachman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:An Architecture for Deep\, Hierarchical Generative Mode
 ls\nPhilip Bachman\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=792
 5\n\nWe present an architecture which makes it easy to train deep\, direct
 ed generative models with many layers of latent variables. We facilitate l
 earning by providing deterministic connections between latent variables an
 d the generated output\, and by providing a richer set of connections betw
 een computations for inference and generation\, which enables more effecti
 ve communication of information throughout the model during training. Our 
 approach permits end-to-end training of models with 10+ hierarchical layer
 s of latent variables. We present experiments showing that our approach ac
 hieves state of the art performance on standard image modelling benchmarks
 \, can expose latent class structure in the absence of label information\,
  and can provide convincing imputations of occluded regions in natural ima
 ges.
LOCATION:Area 5+6+7+8 #12
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Multi-Batch L-BFGS Method for Machine Learning | Albert S Beraha
 s \, Jorge Nocedal \, Jorge Nocedal \, Martin Takac \, Martin Takac
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:A Multi-Batch L-BFGS Method for Machine Learning\nAlber
 t S Berahas \, Jorge Nocedal \, Jorge Nocedal \, Martin Takac \, Martin Ta
 kac\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7927\n\nThe questi
 on of how to parallelize the stochastic gradient descent (SGD) method has 
 received much attention in the literature. In this paper\, we focus instea
 d on batch methods that use a sizeable fraction of the training set at eac
 h iteration to facilitate parallelism\, and that employ second-order infor
 mation. In order to improve the learning process\, we follow a multi-batch
  approach in which the batch changes at each iteration. This inherently gi
 ves the algorithm a stochastic flavor that can cause instability in L-BFGS
 \, a popular batch method in machine learning. These difficulties arise be
 cause L-BFGS employs gradient differences to update the Hessian approximat
 ions\; when these gradients are computed using different data points the p
 rocess can be unstable. This paper shows how to perform stable quasi-Newto
 n updating in the multi-batch setting\, illustrates the behavior of the al
 gorithm in a distributed computing platform\, and studies its convergence 
 properties for both the convex and nonconvex cases.
LOCATION:Area 5+6+7+8 #13
END:VEVENT
BEGIN:VEVENT
SUMMARY:Higher-Order Factorization Machines | Mathieu Blondel \, Akinori F
 ujino \, Akinori Fujino \, Naonori Ueda \, Naonori Ueda \, Masakazu Ishiha
 ta \, Masakazu Ishihata
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Higher-Order Factorization Machines\nMathieu Blondel \,
  Akinori Fujino \, Akinori Fujino \, Naonori Ueda \, Naonori Ueda \, Masak
 azu Ishihata \, Masakazu Ishihata\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7929\n\nFactorization machines (FMs) are a supervised learning
  approach that can use second-order feature combinations even when the dat
 a is very high-dimensional. Unfortunately\, despite increasing interest in
  FMs\, there exists to date no efficient training algorithm for higher-ord
 er FMs (HOFMs). In this paper\, we present the first generic yet efficient
  algorithms for training arbitrary-order HOFMs. We also present new varian
 ts of HOFMs with shared parameters\, which greatly reduce model size and p
 rediction times while maintaining similar accuracy.  We demonstrate the pr
 oposed approaches on four different link prediction tasks.
LOCATION:Area 5+6+7+8 #14
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Bio-inspired Redundant Sensing Architecture | Anh Tuan Nguyen \,
  Jian Xu \, Jian Xu \, Zhi Yang \, Zhi Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:A Bio-inspired Redundant Sensing Architecture\nAnh Tuan
  Nguyen \, Jian Xu \, Jian Xu \, Zhi Yang \, Zhi Yang\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=7931\n\nSensing is the process of deriving
  signals from the environment that allows artificial systems to interact w
 ith the physical world. The Shannon theorem specifies the maximum rate at 
 which information can be acquired. However\, this upper bound is hard to a
 chieve in many man-made systems. The biological visual systems\, on the ot
 her hand\, have highly efficient signal representation and processing mech
 anisms that allow precise sensing. In this work\, we argue that redundancy
  is one of the critical characteristics for such superior performance. We 
 show architectural advantages by utilizing redundant sensing\, including c
 orrection of mismatch error and significant precision enhancement. For a p
 roof-of-concept demonstration\, we have designed a heuristic-based analog-
 to-digital converter - a zero-dimensional quantizer. Through Monte Carlo s
 imulation with the error probabilistic distribution as a priori\, the perf
 ormance approaching the Shannon limit is feasible. In actual measurements 
 without knowing the error distribution\, we observe at least 2-bit extra p
 recision. The results may also help explain biological processes including
  the dominance of binocular vision\, the functional roles of the fixationa
 l eye movements\, and the structural mechanisms allowing hyperacuity.
LOCATION:Area 5+6+7+8 #15
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Supervised PageRank with Gradient-Based and Gradient-Free
  Optimization Methods | Lev Bogolubsky \, Pavel Dvurechensky \, Pavel Dvur
 echensky \, Alexander Gasnikov \, Alexander Gasnikov \, Gleb Gusev \, Gleb
  Gusev \, Yurii Nesterov \, Yurii Nesterov \, Andrei M Raigorodskii \, And
 rei M Raigorodskii \, Aleksey Tikhonov \, Aleksey Tikhonov \, Maksim Zhuko
 vskii
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Learning Supervised PageRank with Gradient-Based and Gr
 adient-Free Optimization Methods\nLev Bogolubsky \, Pavel Dvurechensky \, 
 Pavel Dvurechensky \, Alexander Gasnikov \, Alexander Gasnikov \, Gleb Gus
 ev \, Gleb Gusev \, Yurii Nesterov \, Yurii Nesterov \, Andrei M Raigorods
 kii \, Andrei M Raigorodskii \, Aleksey Tikhonov \, Aleksey Tikhonov \, Ma
 ksim Zhukovskii\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7934\n
 \nIn this paper\, we consider a non-convex loss-minimization problem of le
 arning Supervised PageRank models\, which can account for some properties 
 not considered by classical approaches such as the classical PageRank mode
 l. We propose gradient-based and random gradient-free methods to solve thi
 s problem. Our algorithms are based on the concept of an inexact oracle an
 d unlike the state state-of-the-art gradient-based method we manage to pro
 vide theoretically the convergence rate guarantees for both of them. Final
 ly\, we apply proposed optimization algorithms to the web page ranking pro
 blem and compare proposed and state-of-the-art algorithms in terms of the 
 considered loss function.
LOCATION:Area 5+6+7+8 #16
END:VEVENT
BEGIN:VEVENT
SUMMARY:Linear Relaxations for Finding Diverse Elements in Metric Spaces |
  Aditya Bhaskara \, Mehrdad Ghadiri \, Mehrdad Ghadiri \, Vahab Mirrokni \
 , Vahab Mirrokni \, Ola Svensson \, Ola Svensson
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Linear Relaxations for Finding Diverse Elements in Metr
 ic Spaces\nAditya Bhaskara \, Mehrdad Ghadiri \, Mehrdad Ghadiri \, Vahab 
 Mirrokni \, Vahab Mirrokni \, Ola Svensson \, Ola Svensson\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7935\n\nChoosing a diverse subset of 
 a large collection of points in a metric space is a fundamental problem\, 
 with applications in feature selection\, recommender systems\, web search\
 , data summarization\, etc. Various notions of diversity have been propose
 d\, tailored to different applications. The general algorithmic goal is to
  find a subset of points that maximize diversity\, while obeying a cardina
 lity (or more generally\, matroid) constraint.  The goal of this paper is 
 to develop a novel linear programming (LP) framework that allows us to des
 ign approximation algorithms for such problems. We study an objective know
 n as {\\em sum-min} diversity\, which is known to be effective in many app
 lications\, and give the first constant factor approximation algorithm. Ou
 r LP framework allows us to easily incorporate additional constraints\, as
  well as secondary objectives. We also prove a hardness result for two nat
 ural diversity objectives\, under the  so-called {\\em planted clique} ass
 umption. Finally\, we study the empirical performance of our algorithm on 
 several standard datasets. We first study the approximation quality of the
  algorithm by comparing with the LP objective. Then\, we compare the quali
 ty of the solutions produced by our method with other popular diversity ma
 ximization algorithms.
LOCATION:Area 5+6+7+8 #17
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Optimization for Large-scale Optimal Transport | Aude G
 enevay \, Aude Genevay \, Marco Cuturi \, Marco Cuturi \, Gabriel Peyré \
 , Gabriel Peyré \, Francis Bach \, Francis Bach
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Stochastic Optimization for Large-scale Optimal Transpo
 rt\nAude Genevay \, Aude Genevay \, Marco Cuturi \, Marco Cuturi \, Gabrie
 l Peyré \, Gabriel Peyré \, Francis Bach \, Francis Bach\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7937\n\nOptimal transport (OT) define
 s a powerful framework to compare probability distributions in a geometric
 ally faithful way. However\, the practical impact of OT is still limited b
 ecause of its computational burden. We propose a new class of stochastic o
 ptimization algorithms to cope with large-scale problems routinely encount
 ered in machine learning applications. These methods are able to manipulat
 e arbitrary distributions (either discrete or continuous) by simply requir
 ing to be able to draw samples from them\, which is the typical setup in h
 igh-dimensional learning problems. This alleviates the need to discretize 
 these densities\, while giving access to provably convergent methods that 
 output the correct distance without discretization error. These algorithms
  rely on two main ideas: (a) the dual OT problem can be re-cast as the max
 imization of an expectation\; (b) entropic regularization of the primal OT
  problem results in a smooth dual optimization optimization which can be a
 ddressed with algorithms that have a provably faster convergence. We insta
 ntiate these ideas in three different computational setups: (i) when compa
 ring a discrete distribution to another\, we show that incremental stochas
 tic optimization schemes can beat the current state of the art finite dime
 nsional OT solver (Sinkhorn's algorithm) \; (ii) when comparing a discrete
  distribution to a continuous density\, a re-formulation (semi-discrete) o
 f the dual program is amenable to averaged stochastic gradient descent\, l
 eading to better performance than approximately solving the problem by dis
 cretization \; (iii) when dealing with two continuous densities\, we propo
 se a stochastic gradient descent over a reproducing kernel Hilbert space (
 RKHS). This is currently the only known method to solve this problem\, and
  is more efficient than discretizing beforehand the two densities. We back
 up these claims on a set of discrete\, semi-discrete and continuous benchm
 ark problems.
LOCATION:Area 5+6+7+8 #18
END:VEVENT
BEGIN:VEVENT
SUMMARY:Threshold Bandits\, With and Without Censored Feedback | Jacob D A
 bernethy \, Kareem Amin \, Kareem Amin \, Ruihao Zhu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Threshold Bandits\, With and Without Censored Feedback\
 nJacob D Abernethy \, Kareem Amin \, Kareem Amin \, Ruihao Zhu\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7939\n\nWe consider the \\emph{Th
 reshold Bandit} setting\, a variant of the classical multi-armed bandit pr
 oblem in which the reward on each round depends on a piece of side informa
 tion known as a \\emph{threshold value}. The learner selects one of $K$ ac
 tions (arms)\, this action generates a random sample from a fixed distribu
 tion\, and the action then receives a unit payoff in the event that this s
 ample exceeds the threshold value. We consider two versions of this proble
 m\, the \\emph{uncensored} and \\emph{censored} case\, that determine whet
 her the sample is always observed or only when the threshold is not met. U
 sing new tools to understand the popular UCB algorithm\, we show that the 
 uncensored case is essentially no more difficult than the classical multi-
 armed bandit setting. Finally we show that the censored case exhibits more
  challenges\, but we give guarantees in the event that the sequence of thr
 eshold values is generated optimistically.
LOCATION:Area 5+6+7+8 #19
END:VEVENT
BEGIN:VEVENT
SUMMARY:Mistake Bounds for Binary Matrix Completion | Mark Herbster \, Ste
 phen Pasteris \, Stephen Pasteris \, Massimiliano Pontil
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Mistake Bounds for Binary Matrix Completion\nMark Herbs
 ter \, Stephen Pasteris \, Stephen Pasteris \, Massimiliano Pontil\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7941\n\nWe study the problem 
 of completing a binary matrix in an online learning setting.   On each tri
 al we predict a matrix entry and then receive the true entry.  We propose 
 a Matrix Exponentiated Gradient algorithm[1] to solve this problem. We pro
 vide a mistake bound for the algorithm\, which scales with the {\\em margi
 n complexity} [2\,3] of the underlying matrix. The bound suggests an inter
 pretation where each row of the matrix is a prediction task over a finite 
 set of objects\, the columns. Using this we show that the algorithm makes 
 a number of mistakes which is comparable up to a logarithmic factor to the
  number of mistakes made by the Kernel Perceptron with an optimal kernel i
 n hindsight. We discuss applications of the algorithm to predicting as wel
 l as the best biclustering and to the problem of predicting the labeling o
 f a graph without knowing the graph in advance.
LOCATION:Area 5+6+7+8 #20
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Sound Representations from Unlabeled Video | Yusuf Aytar 
 \, Carl Vondrick \, Carl Vondrick \, Antonio Torralba \, Antonio Torralba
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Learning Sound Representations from Unlabeled Video\nYu
 suf Aytar \, Carl Vondrick \, Carl Vondrick \, Antonio Torralba \, Antonio
  Torralba\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7943\n\nIn t
 his paper we propose to learn semantically rich natural sound representati
 ons using big sound data collected in the wild. Harnessing from the natura
 l synchronization between vision and sound\, we learn an acoustic represen
 tation from a large collection of (2M videos) unlabeled videos. Unlabeled 
 video has the advantage that it can be economically acquired at massive sc
 ales\, yet contains useful signals about natural sound. We propose a stude
 nt-teacher training procedure which transfers this discriminative visual k
 nowledge from well established visual models (e.g. ImageNet and Places2 CN
 Ns) into sound domain using unlabeled video as a bridge. Our sound represe
 ntation yields significant performance improvements over the state-of-the-
 art results on standard benchmarks for acoustic scene/object classificatio
 n.
LOCATION:Area 5+6+7+8 #21
END:VEVENT
BEGIN:VEVENT
SUMMARY:Doubly Convolutional Neural Networks | Shuangfei Zhai \, Yu Cheng 
 \, Yu Cheng \, Zhongfei (Mark) Zhang \, Zhongfei (Mark) Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Doubly Convolutional Neural Networks\nShuangfei Zhai \,
  Yu Cheng \, Yu Cheng \, Zhongfei (Mark) Zhang \, Zhongfei (Mark) Zhang\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7945\n\nBuilding large m
 odels with parameter sharing accounts for most of the success of deep conv
 olutional neural networks (CNNs). In this paper\, we propose doubly convol
 utional neural networks (DCNNs)\, which significantly improve the performa
 nce of CNNs by further exploring this idea. In stead of allocating a set o
 f convolutional filters that are independently learned\, a DCNN maintains 
 groups of filters where filters within each group are translated versions 
 of each other. Practically\, a DCNN can be easily implemented by a two-ste
 p convolution procedure\, which is supported by most modern deep learning 
 libraries. We perform extensive experiments on three image classification 
 benchmarks: CIFAR-10\, CIFAR-100 and ImageNet\, and show that DCNNs consis
 tently outperform other competing architectures\, with a margin. We have a
 lso verified that replacing a convolutional layer with a doubly convolutio
 nal layer at any depth of a CNN can improve its performance. Moreover\, va
 rious design choices of DCNNs are demonstrated\, which shows that DCNN can
  serve the dual purpose of building more accurate models and/or reducing t
 he memory footprint without sacrificing the accuracy.
LOCATION:Area 5+6+7+8 #22
END:VEVENT
BEGIN:VEVENT
SUMMARY:Maximizing Influence in an Ising Network: A Mean-Field Optimal Sol
 ution | Christopher Lynn \, Daniel D Lee \, Daniel D Lee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Maximizing Influence in an Ising Network: A Mean-Field 
 Optimal Solution\nChristopher Lynn \, Daniel D Lee \, Daniel D Lee\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7947\n\nThe problem of influe
 nce maximization in social networks has typically been studied in the cont
 ext of contagion models and irreversible processes. In this paper\, we con
 sider an alternate model that treats individual opinions as spins in an Is
 ing network at dynamic equilibrium. We formalize the Ising influence maxim
 ization (IIM) problem\, which has a physical interpretation as the maximiz
 ation of the magnetization given a budget of external magnetic field. Unde
 r the mean-field (MF) approximation\, we develop a number of sufficient co
 nditions for when the problem is convex and exactly solvable\, and we prov
 ide a gradient ascent algorithm that efficiently achieves an $\\epsilon$-a
 pproximation to the optimal solution. We show that optimal strategies exhi
 bit a phase transition from focusing influence on high-degree individuals 
 at high interaction strengths to spreading influence among low-degree indi
 viduals at low interaction strengths. We also establish a number of novel 
 results about the structure of steady-states in the ferromagnetic MF Ising
  model on general graphs\, which are of independent interest.
LOCATION:Area 5+6+7+8 #23
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning from Rational Behavior: Predicting Solutions to Unknown L
 inear Programs | Shahin Jabbari \, Ryan M Rogers \, Ryan M Rogers \, Aaron
  Roth \, Aaron Roth \, Steven Z. Wu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Learning from Rational Behavior: Predicting Solutions t
 o Unknown Linear Programs\nShahin Jabbari \, Ryan M Rogers \, Ryan M Roger
 s \, Aaron Roth \, Aaron Roth \, Steven Z. Wu\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=7949\n\nWe define and study the problem of predict
 ing the solution to a linear program (LP) given only partial information a
 bout its objective and constraints. This generalizes the problem of learni
 ng to predict the purchasing behavior of a rational agent who has an unkno
 wn objective function\, that has been studied under the name “Learning f
 rom Revealed Preferences". We give mistake bound learning algorithms in tw
 o settings: in the first\, the objective of the LP is known to the learner
  but there is an arbitrary\, fixed set of constraints which are unknown. E
 ach example is defined by an additional known constraint and the goal of t
 he learner is to predict the optimal solution of the LP given the union of
  the known and unknown constraints. This models the problem of predicting 
 the behavior of a rational agent whose goals are known\, but whose resourc
 es are unknown. In the second setting\, the objective of the LP is unknown
 \, and changing in a controlled way. The constraints of the LP may also ch
 ange every day\, but are known. An example is given by a set of constraint
 s and partial information about the objective\, and the task of the learne
 r is again to predict the optimal solution of the partially known LP.
LOCATION:Area 5+6+7+8 #24
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fairness in Learning: Classic and Contextual Bandits | Matthew Jos
 eph \, Michael Kearns \, Michael Kearns \, Jamie H Morgenstern \, Jamie H 
 Morgenstern \, Aaron Roth \, Aaron Roth
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Fairness in Learning: Classic and Contextual Bandits\nM
 atthew Joseph \, Michael Kearns \, Michael Kearns \, Jamie H Morgenstern \
 , Jamie H Morgenstern \, Aaron Roth \, Aaron Roth\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7951\n\nWe introduce the study of fairness in 
 contextual multi-armed bandit problems. Our fairness definition can be int
 erpreted as demanding that given a pool of applicants (say\, for college a
 dmission or mortgages)\, a worse applicant is never favored over a better 
 one\, despite a learning algorithm’s uncertainty over the true payoffs. 
 Our main results prove a tight connection between fairness and the KWIK (K
 nows What It Knows) learning model: a KWIK algorithm for a class of functi
 ons can be transformed into a provably fair contextual bandit algorithm\, 
 and conversely any fair contextual bandit algorithm can be transformed int
 o a KWIK learning algorithm. This tight connection allows us to provide a 
 provably fair algorithm for the linear contextual bandit problem with a po
 lynomial dependence on the dimension\, and to show (for a different class 
 of functions) a worst-case exponential gap in regret between fair and non-
 fair learning algorithms.
LOCATION:Area 5+6+7+8 #25
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Powerful Generative Model Using Random Weights for the Deep Imag
 e Representation | Kun He \, Yan Wang \, Yan Wang \, John Hopcroft \, John
  Hopcroft
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:A Powerful Generative Model Using Random Weights for th
 e Deep Image Representation\nKun He \, Yan Wang \, Yan Wang \, John Hopcro
 ft \, John Hopcroft\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=79
 53\n\nTo what extent is the success of deep visualization due to the train
 ing? Could we do deep visualization using untrained\, random weight networ
 ks? To address this issue\, we explore new and powerful generative models 
 for three popular deep visualization tasks using untrained\, random weight
  convolutional neural networks. First we invert representations in feature
  spaces and reconstruct images from white noise inputs. The reconstruction
  quality is statistically higher than that of the same method applied on w
 ell trained networks with the same architecture. Next we synthesize textur
 es using scaled correlations of representations in multiple layers and our
  results are almost indistinguishable with the original natural texture an
 d the synthesized textures based on the trained network. Third\, by recast
 ing the content of an image in the style of various artworks\, we create a
 rtistic images with high perceptual quality\, highly competitive to the pr
 ior work of Gatys et al. on pretrained networks. To our knowledge this is 
 the first demonstration of image representations using untrained deep neur
 al networks. Our work provides a new and fascinating tool to study the rep
 resentation of deep network architecture and sheds light on new understand
 ings on deep visualization. It may possibly lead to a way to compare netwo
 rk architectures without training.
LOCATION:Area 5+6+7+8 #26
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improved Error Bounds for Tree Representations of Metric Spaces | 
 Samir Chowdhury \, Facundo Mémoli \, Facundo Mémoli \, Zane T Smith \, Z
 ane T Smith
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T130000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T154500
DESCRIPTION:Poster:Improved Error Bounds for Tree Representations of Metri
 c Spaces\nSamir Chowdhury \, Facundo Mémoli \, Facundo Mémoli \, Zane T 
 Smith \, Zane T Smith\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=
 7955\n\nEstimating optimal phylogenetic trees or hierarchical clustering t
 rees from metric data is an important problem in evolutionary biology and 
 data analysis. Intuitively\, the goodness-of-fit of a metric space to a tr
 ee depends on its inherent treeness\, as well as other metric properties s
 uch as intrinsic dimension. Existing algorithms for embedding metric space
 s into tree metrics provide distortion bounds depending on cardinality. Be
 cause cardinality is a simple property of any set\, we argue that such bou
 nds do not fully capture the rich structure endowed by the metric. We cons
 ider an embedding of a metric space into a tree proposed by Gromov. By pro
 ving a stability result\, we obtain an improved additive distortion bound 
 depending only on the hyperbolicity and doubling dimension of the metric. 
 We observe that Gromov's method is dual to the well-known single linkage h
 ierarchical clustering (SLHC) method. By means of this duality\, we are ab
 le to transport our results to the setting of SLHC\, where such additive d
 istortion bounds were previously unknown.
LOCATION:Area 5+6+7+8 #27
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improved Dropout for Shallow and Deep Learning | Zhe Li \, Boqing 
 Gong \, Tianbao Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Improved Dropout for Shallow and Deep Learning\nZhe Li 
 \, Boqing Gong \, Tianbao Yang\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=6861\n\nDropout has been witnessed with great success in training
  deep neural networks by independently   zeroing  out the outputs of neuro
 ns at random. It has also received a surge of interest for shallow learnin
 g\, e.g.\, logistic regression.  However\, the independent  sampling for d
 ropout could be suboptimal for the sake of convergence. In this paper\, we
  propose to use multinomial  sampling for dropout\, i.e.\, sampling featur
 es or neurons according to  a multinomial distribution with different prob
 abilities for different features/neurons. To exhibit the optimal dropout p
 robabilities\, we analyze the shallow learning with multinomial  dropout a
 nd establish the risk bound for stochastic optimization. By minimizing a s
 ampling dependent factor in the risk bound\, we obtain a distribution-depe
 ndent dropout with sampling probabilities dependent on the second order st
 atistics of the data distribution. To tackle the issue of evolving  distri
 bution of neurons in deep learning\, we propose an efficient adaptive  dro
 pout (named \\textbf{evolutional dropout}) that computes the sampling prob
 abilities on-the-fly from a mini-batch of examples. Empirical studies on s
 everal benchmark datasets demonstrate that the proposed dropouts achieve  
 not only much faster convergence and  but also a smaller testing error tha
 n the standard dropout.  For example\, on the CIFAR-100 data\, the evoluti
 onal  dropout achieves relative improvements  over 10\\% on the prediction
  performance and over 50\\% on the convergence speed compared to the stand
 ard dropout.
LOCATION:Area 5+6+7+8 #1
END:VEVENT
BEGIN:VEVENT
SUMMARY:Communication-Optimal Distributed Clustering | Jiecao Chen \, He S
 un \, David Woodruff \, Qin Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Communication-Optimal Distributed Clustering\nJiecao Ch
 en \, He Sun \, David Woodruff \, Qin Zhang\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=6862\n\nClustering large datasets is a fundamental p
 roblem with a number of applications in machine learning. Data is often co
 llected on different sites and clustering needs to be performed in a distr
 ibuted manner with low communication. We would like the quality of the clu
 stering in the distributed setting to match that in the centralized settin
 g for which all the data resides on a single site. In this work\, we study
  both graph and geometric clustering problems in two distributed models: (
 1) a point-to-point model\, and (2) a model with a broadcast channel. We g
 ive protocols in both models which we show are nearly optimal by proving a
 lmost matching communication lower bounds. Our work highlights the surpris
 ing power of a broadcast channel for clustering problems\; roughly speakin
 g\, to cluster n points or n vertices in a graph distributed across s serv
 ers\, for a worst-case partitioning the communication complexity in a poin
 t-to-point model is n*s\, while in the broadcast model it is n + s. We imp
 lement our algorithms and demonstrate this phenomenon on real life dataset
 s\, showing that our algorithms are also very efficient in practice.
LOCATION:Area 5+6+7+8 #2
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Robustness of Kernel Clustering | Bowei Yan \, Purnamrita Sarka
 r
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:On Robustness of Kernel Clustering\nBowei Yan \, Purnam
 rita Sarkar\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6863\n\nCl
 ustering is an important unsupervised learning problem in machine learning
  and statistics. Among many existing algorithms\, kernel \\km has drawn mu
 ch research attention due to its ability to find non-linear cluster bounda
 ries and its inherent simplicity. There are two main approaches for kernel
  k-means: SVD of the kernel matrix and convex relaxations. Despite the att
 ention kernel clustering has received both from theoretical and applied qu
 arters\, not much is known about robustness of the methods. In this paper 
 we first introduce a semidefinite programming relaxation for the kernel cl
 ustering problem\, then prove that under a suitable model specification\, 
 both K-SVD and SDP approaches are consistent in the limit\, albeit SDP is 
 strongly consistent\, i.e. achieves exact recovery\, whereas K-SVD is weak
 ly consistent\, i.e. the fraction of misclassified nodes vanish. Also the 
 error bounds suggest that SDP is more resilient towards outliers\, which w
 e also demonstrate with experiments.
LOCATION:Area 5+6+7+8 #3
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combinatorial semi-bandit with known covariance | Rémy Degenne \,
  Vianney Perchet
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Combinatorial semi-bandit with known covariance\nRémy 
 Degenne \, Vianney Perchet\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=6864\n\nThe combinatorial stochastic semi-bandit problem is an extens
 ion of the classical multi-armed bandit problem in which an algorithm pull
 s more than one arm at each stage and the rewards of all pulled arms are r
 evealed. One difference with the single arm variant is that the dependency
  structure of the arms is crucial. Previous works on this setting either u
 sed a worst-case approach or imposed independence of the arms. We introduc
 e a way to quantify the dependency structure of the problem and design an 
 algorithm that adapts to it. The algorithm is based on linear regression a
 nd the analysis uses techniques from the linear bandit literature. By comp
 aring its performance to a new lower bound\, we prove that it is optimal\,
  up to a poly-logarithmic factor in the number of arms pulled.
LOCATION:Area 5+6+7+8 #4
END:VEVENT
BEGIN:VEVENT
SUMMARY:A posteriori error bounds for joint matrix decomposition problems 
 | nicolo colombo \, Nicolo Colombo \, Nikos Vlassis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A posteriori error bounds for joint matrix decompositio
 n problems\nnicolo colombo \, Nicolo Colombo \, Nikos Vlassis\nhttp://nips
 .cc/Conferences/2016/Schedule?showEvent=6865\n\nJoint matrix triangulariza
 tion is often used for estimating the joint eigenstructure of a set M of m
 atrices\, with applications in signal processing and machine learning. We 
 consider the problem of approximate joint matrix triangularization when th
 e matrices in M are jointly diagonalizable and real\, but we only observe 
 a set M' of noise perturbed versions of the matrices in M. Our main result
  is a first-order upper bound on the distance between any approximate join
 t triangularizer of the matrices in M' and any exact joint triangularizer 
 of the matrices in M. The bound depends only on the observable matrices in
  M' and the noise level. In particular\, it does not depend on optimizatio
 n specific properties of the triangularizer\, such as its proximity to cri
 tical points\, that are typical of existing bounds in the literature. To o
 ur knowledge\, this is the first a posteriori bound for joint matrix decom
 position. We demonstrate the bound on synthetic data for which the ground 
 truth is known.
LOCATION:Area 5+6+7+8 #5
END:VEVENT
BEGIN:VEVENT
SUMMARY:Object based Scene Representations using Fisher Scores of Local Su
 bspace Projections | Mandar D Dixit \, Nuno Vasconcelos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Object based Scene Representations using Fisher Scores 
 of Local Subspace Projections\nMandar D Dixit \, Nuno Vasconcelos\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=6866\n\nSeveral works have sho
 wn that deep CNN classifiers can be easily transferred across datasets\, e
 .g. the transfer of a CNN trained to recognize objects on ImageNET to an o
 bject detector on Pascal VOC. Less clear\, however\, is the ability of CNN
 s to transfer knowledge across tasks. A common example of such transfer is
  the problem of scene classification that should leverage localized object
  detections to recognize holistic visual concepts. While this problem is c
 urrently addressed with Fisher vector representations\, these are now show
 n ineffective for the high-dimensional and highly non-linear features extr
 acted by modern CNNs. It is argued that this is mostly due to the reliance
  on a model\, the Gaussian mixture of diagonal covariances\, which has a v
 ery limited ability to capture the second order statistics of CNN features
 . This problem is addressed by the adoption of a better model\, the mixtur
 e of factor analyzers (MFA)\, which approximates the non-linear data manif
 old by a collection of local subspaces. The Fisher score with respect to t
 he MFA (MFA-FS) is derived and proposed as an image representation for hol
 istic image classifiers. Extensive experiments show that the MFA-FS has st
 ate of the art performance for object-to-scene transfer and this transfer 
 actually  outperforms the training of a scene CNN from a large scene datas
 et. The two representations are also shown to be complementary\, in the se
 nse that their combination outperforms each of the representations by itse
 lf. When combined\, they produce a state of the art scene classifier.
LOCATION:Area 5+6+7+8 #6
END:VEVENT
BEGIN:VEVENT
SUMMARY:MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild 
 | Gregory Rogez \, Cordelia Schmid
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:MoCap-guided Data Augmentation for 3D Pose Estimation i
 n the Wild\nGregory Rogez \, Cordelia Schmid\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=6867\n\nThis paper addresses the problem of 3D huma
 n pose estimation in the wild. A significant challenge is the lack of trai
 ning data\, i.e.\, 2D images of humans annotated with 3D poses. Such data 
 is necessary to train state-of-the-art CNN architectures. Here\, we propos
 e a solution to generate a large set of photorealistic synthetic images of
  humans with 3D pose annotations. We introduce an image-based synthesis en
 gine that artificially augments a dataset of real images with 2D human pos
 e annotations using 3D Motion Capture (MoCap) data. Given a candidate 3D p
 ose our algorithm selects for each joint an image whose 2D pose locally ma
 tches the projected 3D pose. The selected images are then combined to gene
 rate a new synthetic image by stitching local image patches in a kinematic
 ally constrained manner. The resulting images are used to train an end-to-
 end CNN for full-body 3D pose estimation. We cluster the training data int
 o a large number of pose classes and tackle pose estimation as a K-way cla
 ssification problem. Such an approach is viable only with large training s
 ets such as ours. Our method outperforms the state of the art in terms of 
 3D pose estimation in controlled environments (Human3.6M) and shows promis
 ing results for in-the-wild images (LSP). This demonstrates that CNNs trai
 ned on artificial images generalize well to real images.
LOCATION:Area 5+6+7+8 #7
END:VEVENT
BEGIN:VEVENT
SUMMARY:Regret of Queueing Bandits | Subhashini Krishnasamy \, Rajat Sen \
 , Ramesh Johari \, Sanjay Shakkottai
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Regret of Queueing Bandits\nSubhashini Krishnasamy \, R
 ajat Sen \, Ramesh Johari \, Sanjay Shakkottai\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=6868\n\nWe consider a variant of the multiarmed b
 andit problem where jobs queue for service\, and service rates of differen
 t servers may be unknown.  We study algorithms that minimize queue-regret:
  the (expected) difference between the queue-lengths obtained by the algor
 ithm\, and those obtained by a genie-aided matching algorithm that knows e
 xact service rates.  A naive view of this problem would suggest that queue
 -regret should grow logarithmically: since queue-regret cannot be larger t
 han classical regret\, results for the standard MAB problem give algorithm
 s that ensure queue-regret increases no more than logarithmically in time.
  Our paper shows surprisingly more complex behavior.  In particular\, the 
 naive intuition is correct as long as the bandit algorithm's queues have r
 elatively long regenerative cycles: in this case queue-regret is similar t
 o cumulative regret\, and scales (essentially) logarithmically.  However\,
  we show that this "early stage" of the queueing bandit eventually gives w
 ay to a "late stage"\, where the optimal queue-regret scaling is O(1/t).  
 We demonstrate an algorithm that (order-wise) achieves this asymptotic que
 ue-regret\, and also exhibits close to optimal switching time from the ear
 ly stage to the late stage.
LOCATION:Area 5+6+7+8 #8
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient Nonparametric Smoothness Estimation | Shashank Singh \, 
 Simon S Du \, Barnabas Poczos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Efficient Nonparametric Smoothness Estimation\nShashank
  Singh \, Simon S Du \, Barnabas Poczos\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=6869\n\nSobolev quantities (norms\, inner products\, and
  distances) of probability density functions are important in the theory o
 f nonparametric statistics\, but have rarely been used in practice\, partl
 y due to a lack of practical estimators. They also include\, as special ca
 ses\, L^2 quantities which are used in many applications. We propose and a
 nalyze a family of estimators for Sobolev quantities of unknown probabilit
 y density functions. We bound the finite-sample bias and variance of our e
 stimators\, finding that they are generally minimax rate-optimal. Our esti
 mators are significantly more computationally tractable than previous esti
 mators\, and exhibit a statistical/computational trade-off allowing them t
 o adapt to computational constraints. We also draw theoretical connections
  to recent work on fast two-sample testing and empirically validate our es
 timators on synthetic data.
LOCATION:Area 5+6+7+8 #9
END:VEVENT
BEGIN:VEVENT
SUMMARY:Completely random measures for modelling block-structured sparse n
 etworks | Tue Herlau \, Mikkel N Schmidt \, Morten Mørup
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Completely random measures for modelling block-structur
 ed sparse networks\nTue Herlau \, Mikkel N Schmidt \, Morten Mørup\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=6870\n\nStatistical methods 
 for network data often parameterize the edge-probability by attributing la
 tent traits such as block structure to the vertices and assume exchangeabi
 lity in the sense of the Aldous-Hoover representation theorem. These assum
 ptions are however incompatible with traits found in real-world networks s
 uch as a power-law degree-distribution. Recently\, Caron &amp\; Fox (2014)
  proposed the use of a different notion of exchangeability after Kallenber
 g (2005) and obtained a network model which permits edge-inhomogeneity\, s
 uch as a power-law degree-distribution whilst retaining desirable statisti
 cal properties. However\, this model does not capture latent vertex traits
  such as block-structure. In this work we re-introduce the use of block-st
 ructure for network models obeying Kallenberg’s notion of exchangeabilit
 y and thereby obtain a collapsed model which both admits the inference of 
 block-structure and edge inhomogeneity. We derive a simple expression for 
 the likelihood and an efficient sampling method. The obtained model is not
  significantly more difficult to implement than existing approaches to blo
 ck-modelling and performs well on real network datasets.
LOCATION:Area 5+6+7+8 #10
END:VEVENT
BEGIN:VEVENT
SUMMARY:DISCO Nets : DISsimilarity COefficients Networks | Diane Bouchacou
 rt \, Pawan K Mudigonda \, Sebastian Nowozin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:DISCO Nets : DISsimilarity COefficients Networks\nDiane
  Bouchacourt \, Pawan K Mudigonda \, Sebastian Nowozin\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=6871\n\nWe present a new type of probabil
 istic model which we call DISsimilarity COefficient Networks (DISCO Nets).
  DISCO Nets allow us to efficiently sample from a posterior distribution p
 arametrised by a neural network. During training\, DISCO Nets are learned 
 by minimising the dissimilarity coefficient between the true distribution 
 and the estimated distribution. This allows us to tailor the training to t
 he loss related to the task at hand. We empirically show that (i) by model
 ing uncertainty on the output value\, DISCO Nets outperform equivalent non
 -probabilistic predictive networks and (ii) DISCO Nets accurately model th
 e uncertainty of the output\, outperforming existing probabilistic models 
 based on deep neural networks.
LOCATION:Area 5+6+7+8 #11
END:VEVENT
BEGIN:VEVENT
SUMMARY:An Architecture for Deep\, Hierarchical Generative Models | Philip
  Bachman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:An Architecture for Deep\, Hierarchical Generative Mode
 ls\nPhilip Bachman\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=687
 2\n\nWe present an architecture which lets us train deep\, directed genera
 tive models with many layers of latent variables. We include deterministic
  paths between all latent variables and the generated output\, and provide
  a richer set of connections between computations for inference and genera
 tion\, which enables more effective communication of information throughou
 t the model during training. To improve performance on natural images\, we
  incorporate a lightweight autoregressive model in the reconstruction dist
 ribution. These techniques permit end-to-end training of models with 10+ l
 ayers of latent variables. Experiments show that our approach achieves sta
 te-of-the-art performance on standard image modelling benchmarks\, can exp
 ose latent class structure in the absence of label information\, and can p
 rovide convincing imputations of occluded regions in natural images.
LOCATION:Area 5+6+7+8 #12
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Multi-Batch L-BFGS Method for Machine Learning | Albert S Beraha
 s \, Jorge Nocedal \, Martin Takac
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Multi-Batch L-BFGS Method for Machine Learning\nAlber
 t S Berahas \, Jorge Nocedal \, Martin Takac\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=6873\n\nThe question of how to parallelize the stoc
 hastic gradient descent (SGD) method has received much attention in the li
 terature. In this paper\, we focus instead on batch methods that use a siz
 eable fraction of the training set at each iteration to facilitate paralle
 lism\, and that employ second-order information. In order to improve the l
 earning process\, we follow a multi-batch approach in which the batch chan
 ges at each iteration. This can cause difficulties because L-BFGS employs 
 gradient differences to update the Hessian approximations\, and when these
  gradients are computed using different data points the process can be uns
 table. This paper shows how to perform stable quasi-Newton updating in the
  multi-batch setting\, illustrates the behavior of the algorithm in a dist
 ributed computing platform\, and studies its convergence properties for bo
 th the convex and nonconvex cases.
LOCATION:Area 5+6+7+8 #13
END:VEVENT
BEGIN:VEVENT
SUMMARY:Higher-Order Factorization Machines | Mathieu Blondel \, Akinori F
 ujino \, Naonori Ueda \, Masakazu Ishihata
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Higher-Order Factorization Machines\nMathieu Blondel \,
  Akinori Fujino \, Naonori Ueda \, Masakazu Ishihata\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=6874\n\nFactorization machines (FMs) are a 
 supervised learning approach that can use second-order feature combination
 s even when the data is very high-dimensional. Unfortunately\, despite inc
 reasing interest in FMs\, there exists to date no efficient training algor
 ithm for higher-order FMs (HOFMs). In this paper\, we present the first ge
 neric yet efficient algorithms for training arbitrary-order HOFMs. We also
  present new variants of HOFMs with shared parameters\, which greatly redu
 ce model size and prediction times while maintaining similar accuracy.  We
  demonstrate the proposed approaches on four different link prediction tas
 ks.
LOCATION:Area 5+6+7+8 #14
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Bio-inspired Redundant Sensing Architecture | Anh Tuan Nguyen \,
  Jian Xu \, Zhi Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Bio-inspired Redundant Sensing Architecture\nAnh Tuan
  Nguyen \, Jian Xu \, Zhi Yang\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=6875\n\nSensing is the process of deriving signals from the envir
 onment that allows artificial systems to interact with the physical world.
  The Shannon theorem specifies the maximum rate at which information can b
 e acquired. However\, this upper bound is hard to achieve in many man-made
  systems. The biological visual systems\, on the other hand\, have highly 
 efficient signal representation and processing mechanisms that allow preci
 se sensing. In this work\, we argue that redundancy is one of the critical
  characteristics for such superior performance. We show architectural adva
 ntages by utilizing redundant sensing\, including correction of mismatch e
 rror and significant precision enhancement. For a proof-of-concept demonst
 ration\, we have designed a heuristic-based analog-to-digital converter - 
 a zero-dimensional quantizer. Through Monte Carlo simulation with the erro
 r probabilistic distribution as a priori\, the performance approaching the
  Shannon limit is feasible. In actual measurements without knowing the err
 or distribution\, we observe at least 2-bit extra precision. The results m
 ay also help explain biological processes including the dominance of binoc
 ular vision\, the functional roles of the fixational eye movements\, and t
 he structural mechanisms allowing hyperacuity.
LOCATION:Area 5+6+7+8 #15
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Supervised PageRank with Gradient-Based and Gradient-Free
  Optimization Methods | Lev Bogolubsky \, Pavel Dvurechensky \, Alexander 
 Gasnikov \, Gleb Gusev \, Yurii Nesterov \, Andrei M Raigorodskii \, Aleks
 ey Tikhonov \, Maksim Zhukovskii
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning Supervised PageRank with Gradient-Based and Gr
 adient-Free Optimization Methods\nLev Bogolubsky \, Pavel Dvurechensky \, 
 Alexander Gasnikov \, Gleb Gusev \, Yurii Nesterov \, Andrei M Raigorodski
 i \, Aleksey Tikhonov \, Maksim Zhukovskii\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=6876\n\nIn this paper\, we consider a non-convex loss
 -minimization problem of learning Supervised PageRank models\, which can a
 ccount for features of nodes and edges. We propose gradient-based and rand
 om gradient-free methods to solve this problem. Our algorithms are based o
 n the concept of an inexact oracle and unlike the state-of-the-art gradien
 t-based method we manage to provide theoretically the convergence rate gua
 rantees for both of them. Finally\, we compare the performance of the prop
 osed optimization methods with the state of the art applied to a ranking t
 ask.
LOCATION:Area 5+6+7+8 #16
END:VEVENT
BEGIN:VEVENT
SUMMARY:Linear Relaxations for Finding Diverse Elements in Metric Spaces |
  Aditya Bhaskara \, Mehrdad Ghadiri \, Vahab Mirrokni \, Ola Svensson
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Linear Relaxations for Finding Diverse Elements in Metr
 ic Spaces\nAditya Bhaskara \, Mehrdad Ghadiri \, Vahab Mirrokni \, Ola Sve
 nsson\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6877\n\nChoosing
  a diverse subset of a large collection of points in a metric space is a f
 undamental problem\, with applications in feature selection\, recommender 
 systems\, web search\, data summarization\, etc. Various notions of divers
 ity have been proposed\, tailored to different applications. The general a
 lgorithmic goal is to find a subset of points that maximize diversity\, wh
 ile obeying a cardinality (or more generally\, matroid) constraint.  The g
 oal of this paper is to develop a novel linear programming (LP) framework 
 that allows us to design approximation algorithms for such problems. We st
 udy an objective known as {\\em sum-min} diversity\, which is known to be 
 effective in many applications\, and give the first constant factor approx
 imation algorithm. Our LP framework allows us to easily incorporate additi
 onal constraints\, as well as secondary objectives. We also prove a hardne
 ss result for two natural diversity objectives\, under the  so-called {\\e
 m planted clique} assumption. Finally\, we study the empirical performance
  of our algorithm on several standard datasets. We first study the approxi
 mation quality of the algorithm by comparing with the LP objective. Then\,
  we compare the quality of the solutions produced by our method with other
  popular diversity maximization algorithms.
LOCATION:Area 5+6+7+8 #17
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Optimization for Large-scale Optimal Transport | Aude G
 enevay \, Marco Cuturi \, Gabriel Peyré \, Francis Bach
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Stochastic Optimization for Large-scale Optimal Transpo
 rt\nAude Genevay \, Marco Cuturi \, Gabriel Peyré \, Francis Bach\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=6878\n\nOptimal transport (OT
 ) defines a powerful framework to compare probability distributions in a g
 eometrically faithful way. However\, the practical impact of OT is still l
 imited because of its computational burden. We propose a new class of stoc
 hastic optimization algorithms to cope with large-scale problems routinely
  encountered in machine learning applications. These methods are able to m
 anipulate arbitrary distributions (either discrete or continuous) by simpl
 y requiring to be able to draw samples from them\, which is the typical se
 tup in high-dimensional learning problems. This alleviates the need to dis
 cretize these densities\, while giving access to provably convergent metho
 ds that output the correct distance without discretization error. These al
 gorithms rely on two main ideas: (a) the dual OT problem can be re-cast as
  the maximization of an expectation\; (b) entropic regularization of the p
 rimal OT problem results in a smooth dual optimization optimization which 
 can be addressed with algorithms that have a provably faster convergence. 
 We instantiate these ideas in three different computational setups: (i) wh
 en comparing a discrete distribution to another\, we show that incremental
  stochastic optimization schemes can beat the current state of the art fin
 ite dimensional OT solver (Sinkhorn's algorithm) \; (ii) when comparing a 
 discrete distribution to a continuous density\, a re-formulation (semi-dis
 crete) of the dual program is amenable to averaged stochastic gradient des
 cent\, leading to better performance than approximately solving the proble
 m by discretization \; (iii) when dealing with two continuous densities\, 
 we propose a stochastic gradient descent over a reproducing kernel Hilbert
  space (RKHS). This is currently the only known method to solve this probl
 em\, and is more efficient than discretizing beforehand the two densities.
  We backup these claims on a set of discrete\, semi-discrete and continuou
 s benchmark problems.
LOCATION:Area 5+6+7+8 #18
END:VEVENT
BEGIN:VEVENT
SUMMARY:Threshold Bandits\, With and Without Censored Feedback | Jacob D A
 bernethy \, Kareem Amin \, Ruihao Zhu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Threshold Bandits\, With and Without Censored Feedback\
 nJacob D Abernethy \, Kareem Amin \, Ruihao Zhu\nhttp://nips.cc/Conference
 s/2016/Schedule?showEvent=6879\n\nWe consider the \\emph{Threshold Bandit}
  setting\, a variant of the classical multi-armed bandit problem in which 
 the reward on each round depends on a piece of side information known as a
  \\emph{threshold value}. The learner selects one of $K$ actions (arms)\, 
 this action generates a random sample from a fixed distribution\, and the 
 action then receives a unit payoff in the event that this sample exceeds t
 he threshold value. We consider two versions of this problem\, the \\emph{
 uncensored} and \\emph{censored} case\, that determine whether the sample 
 is always observed or only when the threshold is not met. Using new tools 
 to understand the popular UCB algorithm\, we show that the uncensored case
  is essentially no more difficult than the classical multi-armed bandit se
 tting. Finally we show that the censored case exhibits more challenges\, b
 ut we give guarantees in the event that the sequence of threshold values i
 s generated optimistically.
LOCATION:Area 5+6+7+8 #19
END:VEVENT
BEGIN:VEVENT
SUMMARY:Mistake Bounds for Binary Matrix Completion | Mark Herbster \, Ste
 phen Pasteris \, Massimiliano Pontil
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Mistake Bounds for Binary Matrix Completion\nMark Herbs
 ter \, Stephen Pasteris \, Massimiliano Pontil\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=6880\n\nWe study the problem of completing a bina
 ry matrix in an online learning setting. On each trial we predict a matrix
  entry and then receive the true entry. We propose a Matrix Exponentiated 
 Gradient algorithm [1] to solve this problem. We provide a mistake bound f
 or the algorithm\, which scales with the margin complexity [2\, 3] of the 
 underlying matrix. The bound suggests an interpretation where each row of 
 the matrix is a prediction task over a finite set of objects\, the columns
 . Using this we show that the algorithm makes a number of mistakes which i
 s comparable up to a logarithmic factor to the number of mistakes made by 
 the Kernel Perceptron with an optimal kernel in hindsight. We discuss appl
 ications of the algorithm to predicting as well as the best biclustering a
 nd to the problem of predicting the labeling of a graph without knowing th
 e graph in advance.
LOCATION:Area 5+6+7+8 #20
END:VEVENT
BEGIN:VEVENT
SUMMARY:SoundNet: Learning Sound Representations from Unlabeled Video | Yu
 suf Aytar \, Carl Vondrick \, Antonio Torralba
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:SoundNet: Learning Sound Representations from Unlabeled
  Video\nYusuf Aytar \, Carl Vondrick \, Antonio Torralba\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=6881\n\nWe learn rich natural sound rep
 resentations by capitalizing on large amounts of unlabeled sound data coll
 ected in the wild. We leverage the natural synchronization between vision 
 and sound to learn an acoustic representation using two-million unlabeled 
 videos. Unlabeled video has the advantage that it can be economically acqu
 ired at massive scales\, yet contains useful signals about natural sound. 
 We propose a student-teacher training procedure which transfers discrimina
 tive visual knowledge from well established visual recognition models into
  the sound modality using unlabeled video as a bridge. Our sound represent
 ation yields significant performance improvements over the state-of-the-ar
 t results on standard benchmarks for acoustic scene/object classification.
  Visualizations suggest some high-level semantics automatically emerge in 
 the sound network\, even though it is trained without ground truth labels.
LOCATION:Area 5+6+7+8 #21
END:VEVENT
BEGIN:VEVENT
SUMMARY:Doubly Convolutional Neural Networks | Shuangfei Zhai \, Yu Cheng 
 \, Weining Lu \, Zhongfei (Mark) Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Doubly Convolutional Neural Networks\nShuangfei Zhai \,
  Yu Cheng \, Weining Lu \, Zhongfei (Mark) Zhang\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=6882\n\nBuilding large models with parameter sh
 aring accounts for most of the success of deep convolutional neural networ
 ks (CNNs). In this paper\, we propose doubly convolutional neural networks
  (DCNNs)\, which significantly improve the performance of CNNs by further 
 exploring this idea. In stead of allocating a set of convolutional filters
  that are independently learned\, a DCNN maintains groups of filters where
  filters within each group are translated versions of each other. Practica
 lly\, a DCNN can be easily implemented by a two-step convolution procedure
 \, which is supported by most modern deep learning libraries. We perform e
 xtensive experiments on three image classification benchmarks: CIFAR-10\, 
 CIFAR-100 and ImageNet\, and show that DCNNs consistently outperform other
  competing architectures. We have also verified that replacing a convoluti
 onal layer with a doubly convolutional layer at any depth of a CNN can imp
 rove its performance. Moreover\, various design choices of DCNNs are demon
 strated\, which shows that DCNN can serve the dual purpose of building mor
 e accurate models and/or reducing the memory footprint without sacrificing
  the accuracy.
LOCATION:Area 5+6+7+8 #22
END:VEVENT
BEGIN:VEVENT
SUMMARY:Maximizing Influence in an Ising Network: A Mean-Field Optimal Sol
 ution | Christopher Lynn \, Daniel D Lee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Maximizing Influence in an Ising Network: A Mean-Field 
 Optimal Solution\nChristopher Lynn \, Daniel D Lee\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=6883\n\nInfluence maximization in social netw
 orks has typically been studied in the context of contagion models and irr
 eversible processes. In this paper\, we consider an alternate model that t
 reats individual opinions as spins in an Ising system at dynamic equilibri
 um. We formalize the \\textit{Ising influence maximization} problem\, whic
 h has a natural physical interpretation as maximizing the magnetization gi
 ven a budget of external magnetic field. Under the mean-field (MF) approxi
 mation\, we present a gradient ascent algorithm that uses the susceptibili
 ty to efficiently calculate local maxima of the magnetization\, and we dev
 elop a number of sufficient conditions for when the MF magnetization is co
 ncave and our algorithm converges to a global optimum. We apply our algori
 thm on random and real-world networks\, demonstrating\, remarkably\, that 
 the MF optimal external fields (i.e.\, the external fields which maximize 
 the MF magnetization) exhibit a phase transition from focusing on high-deg
 ree individuals at high temperatures to focusing on low-degree individuals
  at low temperatures. We also establish a number of novel results about th
 e structure of steady-states in the ferromagnetic MF Ising model on genera
 l graphs\, which are of independent interest.
LOCATION:Area 5+6+7+8 #23
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning from Rational Behavior: Predicting Solutions to Unknown L
 inear Programs | Shahin Jabbari \, Ryan M Rogers \, Aaron Roth \, Steven Z
 . Wu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning from Rational Behavior: Predicting Solutions t
 o Unknown Linear Programs\nShahin Jabbari \, Ryan M Rogers \, Aaron Roth \
 , Steven Z. Wu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6884\n\
 nWe define and study the problem of predicting the solution to a linear pr
 ogram (LP) given only partial information about its objective and constrai
 nts. This generalizes the problem of learning to predict the purchasing be
 havior of a rational agent who has an unknown objective function\, that ha
 s been studied under the name “Learning from Revealed Preferences". We g
 ive mistake bound learning algorithms in two settings: in the first\, the 
 objective of the LP is known to the learner but there is an arbitrary\, fi
 xed set of constraints which are unknown. Each example is defined by an ad
 ditional known constraint and the goal of the learner is to predict the op
 timal solution of the LP given the union of the known and unknown constrai
 nts. This models the problem of predicting the behavior of a rational agen
 t whose goals are known\, but whose resources are unknown. In the second s
 etting\, the objective of the LP is unknown\, and changing in a controlled
  way. The constraints of the LP may also change every day\, but are known.
  An example is given by a set of constraints and partial information about
  the objective\, and the task of the learner is again to predict the optim
 al solution of the partially known LP.
LOCATION:Area 5+6+7+8 #24
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fairness in Learning: Classic and Contextual Bandits | Matthew Jos
 eph \, Michael Kearns \, Jamie H Morgenstern \, Aaron Roth
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Fairness in Learning: Classic and Contextual Bandits\nM
 atthew Joseph \, Michael Kearns \, Jamie H Morgenstern \, Aaron Roth\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=6885\n\nWe introduce the st
 udy of fairness in multi-armed bandit problems. Our fairness definition de
 mands that\, given a pool of applicants\, a worse applicant is never favor
 ed over a better one\, despite a learning algorithm’s uncertainty over t
 he true payoffs. In the classic stochastic bandits problem we provide a pr
 ovably fair algorithm based on “chained” confidence intervals\, and pr
 ove a cumulative regret bound with a cubic dependence on the number of arm
 s. We further show that any fair algorithm must have such a dependence\, p
 roviding a strong separation between fair and unfair learning that extends
  to the general contextual case. In the general contextual case\, we prove
  a tight connection between fairness and the KWIK (Knows What It Knows) le
 arning model: a KWIK algorithm for a class of functions can be transformed
  into a provably fair contextual bandit algorithm and vice versa. This tig
 ht connection allows us to provide a provably fair algorithm for the linea
 r contextual bandit problem with a polynomial dependence on the dimension\
 , and to show (for a different class of functions) a worst-case exponentia
 l gap in regret between fair and non-fair learning algorithms.
LOCATION:Area 5+6+7+8 #25
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Powerful Generative Model Using Random Weights for the Deep Imag
 e Representation | Kun He \, Yan Wang \, John Hopcroft
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Powerful Generative Model Using Random Weights for th
 e Deep Image Representation\nKun He \, Yan Wang \, John Hopcroft\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=6886\n\nTo what extent is the s
 uccess of deep visualization due to the training? Could we do deep visuali
 zation using untrained\, random weight networks? To address this issue\, w
 e explore new and powerful generative models for three popular deep visual
 ization tasks using untrained\, random weight convolutional neural network
 s. First we invert representations in feature spaces and reconstruct image
 s from white noise inputs. The reconstruction quality is statistically hig
 her than that of the same method applied on well trained networks with the
  same architecture. Next we synthesize textures using scaled correlations 
 of representations in multiple layers and our results are almost indisting
 uishable with the original natural texture and the synthesized textures ba
 sed on the trained network. Third\, by recasting the content of an image i
 n the style of various artworks\, we create artistic images with high perc
 eptual quality\, highly competitive to the prior work of Gatys et al. on p
 retrained networks. To our knowledge this is the first demonstration of im
 age representations using untrained deep neural networks. Our work provide
 s a new and fascinating tool to study the representation of deep network a
 rchitecture and sheds light on new understandings on deep visualization. I
 t may possibly lead to a way to compare network architectures without trai
 ning.
LOCATION:Area 5+6+7+8 #26
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improved Error Bounds for Tree Representations of Metric Spaces | 
 Samir Chowdhury \, Facundo Mémoli \, Zane T Smith
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Improved Error Bounds for Tree Representations of Metri
 c Spaces\nSamir Chowdhury \, Facundo Mémoli \, Zane T Smith\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=6887\n\nEstimating optimal phylogen
 etic trees or hierarchical clustering trees from metric data is an importa
 nt problem in evolutionary biology and data analysis. Intuitively\, the go
 odness-of-fit of a metric space to a tree depends on its inherent treeness
 \, as well as other metric properties such as intrinsic dimension. Existin
 g algorithms for embedding metric spaces into tree metrics provide distort
 ion bounds depending on cardinality. Because cardinality is a simple prope
 rty of any set\, we argue that such bounds do not fully capture the rich s
 tructure endowed by the metric. We consider an embedding of a metric space
  into a tree proposed by Gromov. By proving a stability result\, we obtain
  an improved additive distortion bound depending only on the hyperbolicity
  and doubling dimension of the metric. We observe that Gromov's method is 
 dual to the well-known single linkage hierarchical clustering (SLHC) metho
 d. By means of this duality\, we are able to transport our results to the 
 setting of SLHC\, where such additive distortion bounds were previously un
 known.
LOCATION:Area 5+6+7+8 #27
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive optimal training of animal behavior | Ji Hyun Bak \, Jung
  Choi \, Athena Akrami \, Ilana Witten \, Jonathan W Pillow
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Adaptive optimal training of animal behavior\nJi Hyun B
 ak \, Jung Choi \, Athena Akrami \, Ilana Witten \, Jonathan W Pillow\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=6888\n\nNeuroscience exper
 iments often require training animals to perform tasks designed to elicit 
 various sensory\, cognitive\, and motor behaviors. Training typically invo
 lves a series of gradual adjustments of stimulus conditions and rewards in
  order to bring about learning. However\, training protocols are usually h
 and-designed\, relying on a combination of intuition\, guesswork\, and tri
 al-and-error\, and often require weeks or months to achieve a desired leve
 l of task performance. Here we combine ideas from reinforcement learning a
 nd adaptive optimal experimental design to formulate methods for adaptive 
 optimal training of animal behavior. Our work addresses two intriguing pro
 blems at once: first\, it seeks to infer the learning rules underlying an 
 animal's behavioral changes during training\; second\, it seeks to exploit
  these rules to select stimuli that will maximize the rate of learning tow
 ard a desired objective.  We develop and test these methods using data col
 lected from rats during training on a two-interval sensory discrimination 
 task.  We show that we can accurately infer the parameters of a policy-gra
 dient-based learning algorithm that describes how the animal's internal mo
 del of the task evolves over the course of training.  We then formulate a 
 theory for optimal training\, which involves selecting sequences of stimul
 i that will drive the animal's internal policy toward a desired location i
 n the parameter space. Simulations show that our method can in theory prov
 ide a substantial speedup over standard training methods. We feel these re
 sults will hold considerable theoretical and practical implications both f
 or researchers in reinforcement learning and for experimentalists seeking 
 to train animals.
LOCATION:Area 5+6+7+8 #28
END:VEVENT
BEGIN:VEVENT
SUMMARY:PAC-Bayesian Theory Meets Bayesian Inference | Pascal Germain \, F
 rancis Bach \, Alexandre Lacoste \, Simon Lacoste-Julien
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:PAC-Bayesian Theory Meets Bayesian Inference\nPascal Ge
 rmain \, Francis Bach \, Alexandre Lacoste \, Simon Lacoste-Julien\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=6889\n\nWe exhibit a strong l
 ink between frequentist PAC-Bayesian bounds and the Bayesian marginal like
 lihood. That is\, for the negative log-likelihood loss function\, we show 
 that the minimization of PAC-Bayesian generalization bounds maximizes the 
 Bayesian marginal likelihood. This provides an alternative explanation to 
 the Bayesian Occam's razor criteria\, under the assumption that the data i
 s generated by an i.i.d. distribution. Moreover\, as the negative log-like
 lihood is an unbounded loss function\, we motivate and propose a PAC-Bayes
 ian theorem tailored for the sub-gamma loss family\, and we show that our 
 approach is sound on classical Bayesian linear regression tasks.
LOCATION:Area 5+6+7+8 #29
END:VEVENT
BEGIN:VEVENT
SUMMARY:Nearly Isometric Embedding by Relaxation | James McQueen \, Marina
  Meila \, Dominique Joncas
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Nearly Isometric Embedding by Relaxation\nJames McQueen
  \, Marina Meila \, Dominique Joncas\nhttp://nips.cc/Conferences/2016/Sche
 dule?showEvent=6890\n\nMany manifold learning algorithms aim to create emb
 eddings with low or no distortion (i.e. isometric). If the data has intrin
 sic dimension d\, it is often impossible to obtain an isometric embedding 
 in d dimensions\, but possible in s &gt\; d dimensions. Yet\, most geometr
 y preserving algorithms cannot do the latter. This paper proposes an embed
 ding algorithm that overcomes this problem. The algorithm directly compute
 s\, for any data embedding Y\, a distortion loss(Y)\, and iteratively upda
 tes Y in order to decrease it. The distortion measure we propose is based 
 on the push-forward Riemannian metric associated with the coordinates Y. T
 he experiments confirm the superiority of our algorithm in obtaining low d
 istortion embeddings.
LOCATION:Area 5+6+7+8 #30
END:VEVENT
BEGIN:VEVENT
SUMMARY:Graph Clustering: Block-models and model free results | Yali Wan \
 , Marina Meila
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Graph Clustering: Block-models and model free results\n
 Yali Wan \, Marina Meila\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=6891\n\nClustering graphs under the Stochastic Block Model (SBM) and ex
 tensions are well studied. Guarantees of correctness exist under the assum
 ption that the data is sampled from a model. In this paper\, we propose a 
 framework\, in which we obtain "correctness" guarantees without assuming t
 he data comes from a model. The guarantees we obtain depend instead on the
  statistics of the data that can be checked. We also show that this framew
 ork ties in with the existing model-based framework\, and that we can expl
 oit results in model-based recovery\, as well as strengthen the results ex
 isting in that area of research.
LOCATION:Area 5+6+7+8 #31
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Transferrable Representations for Unsupervised Domain Ada
 ptation | Ozan Sener \, Hyun Oh Song \, Ashutosh Saxena \, Silvio Savarese
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning Transferrable Representations for Unsupervised
  Domain Adaptation\nOzan Sener \, Hyun Oh Song \, Ashutosh Saxena \, Silvi
 o Savarese\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6892\n\nSup
 ervised learning with large scale labelled datasets and deep layered model
 s has caused a paradigm shift in diverse areas in learning and recognition
 . However\, this approach still suffers from generalization issues under t
 he presence of a domain shift between the training and the test data distr
 ibution. Since unsupervised domain adaptation algorithms directly address 
 this domain shift problem between a labelled source dataset and an unlabel
 led target dataset\, recent papers have shown promising results by fine-tu
 ning the networks with domain adaptation loss functions which try to align
  the mismatch between the training and testing data distributions.  Nevert
 heless\, these recent deep learning based domain adaptation approaches sti
 ll suffer from issues such as high sensitivity to the gradient reversal hy
 perparameters and overfitting during the fine-tuning stage. In this paper\
 , we propose a unified deep learning framework where the representation\, 
 cross domain transformation\, and target label inference are all jointly o
 ptimized in an end-to-end fashion for unsupervised domain adaptation. Our 
 experiments show that the proposed method significantly outperforms state-
 of-the-art algorithms in both object recognition and digit classification 
 experiments by a large margin. We will make our learned models as well as 
 the source code available immediately upon acceptance.
LOCATION:Area 5+6+7+8 #32
END:VEVENT
BEGIN:VEVENT
SUMMARY:Measuring Neural Net Robustness with Constraints | Osbert Bastani 
 \, Yani Ioannou \, Leonidas Lampropoulos \, Dimitrios Vytiniotis \, Aditya
  Nori \, Antonio Criminisi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Measuring Neural Net Robustness with Constraints\nOsber
 t Bastani \, Yani Ioannou \, Leonidas Lampropoulos \, Dimitrios Vytiniotis
  \, Aditya Nori \, Antonio Criminisi\nhttp://nips.cc/Conferences/2016/Sche
 dule?showEvent=6893\n\nDespite having high accuracy\, neural nets have bee
 n shown to be susceptible to adversarial examples\, where a small perturba
 tion to an input can cause it to become mislabeled. We propose metrics for
  measuring the robustness of a neural net and devise a novel algorithm for
  approximating these metrics based on an encoding of robustness as a linea
 r program. We show how our metrics can be used to evaluate the robustness 
 of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. O
 ur algorithm generates more informative estimates of robustness metrics co
 mpared to estimates based on existing algorithms. Furthermore\, we show ho
 w existing approaches to improving robustness “overfit” to adversarial
  examples generated using a specific algorithm. Finally\, we show that our
  techniques can be used to additionally improve neural net robustness both
  according to the metrics that we propose\, but also according to previous
 ly proposed metrics.
LOCATION:Area 5+6+7+8 #33
END:VEVENT
BEGIN:VEVENT
SUMMARY:A forward model at Purkinje cell synapses facilitates cerebellar a
 nticipatory control | Ivan Herreros \, Xerxes Arsiwalla \, Paul Verschure
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A forward model at Purkinje cell synapses facilitates c
 erebellar anticipatory control\nIvan Herreros \, Xerxes Arsiwalla \, Paul 
 Verschure\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6894\n\nHow 
 does our motor system solve the problem of anticipatory control in spite o
 f a wide spectrum of response dynamics from different musculo-skeletal sys
 tems\, transport delays as well as response latencies throughout the centr
 al nervous system? To a great extent\, our highly-skilled motor responses 
 are a result of a reactive feedback system\, originating in the brain-stem
  and spinal cord\, combined with a feed-forward anticipatory system\, that
  is adaptively fine-tuned by sensory experience and originates in the cere
 bellum. Based on that interaction we design the counterfactual predictive 
 control (CFPC) architecture\, an anticipatory adaptive motor control schem
 e in which a feed-forward module\, based on the cerebellum\, steers an err
 or feedback controller with counterfactual error signals. Those are signal
 s that trigger reactions as actual errors would\, but that do not code for
  any current of forthcoming errors. In order to determine the optimal lear
 ning strategy\, we derive a novel learning rule for the feed-forward modul
 e that involves an eligibility trace and operates at the synaptic level. I
 n particular\, our eligibility trace provides a mechanism beyond co-incide
 nce detection in that it convolves a history of prior synaptic inputs with
  error signals. In the context of cerebellar physiology\, this solution im
 plies that Purkinje cell synapses should generate eligibility traces using
  a forward model of the system being controlled. From an engineering persp
 ective\, CFPC provides a general-purpose anticipatory control architecture
  equipped with a learning rule that exploits the full dynamics of the clos
 ed-loop system.
LOCATION:Area 5+6+7+8 #34
END:VEVENT
BEGIN:VEVENT
SUMMARY:Estimating Nonlinear Neural Response Functions using GP Priors and
  Kronecker Methods | Cristina Savin \, Gasper Tkacik
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Estimating Nonlinear Neural Response Functions using GP
  Priors and Kronecker Methods\nCristina Savin \, Gasper Tkacik\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=6895\n\nJointly characterizing ne
 ural responses in terms of several external variables promises novel insig
 hts into circuit function\, but remains computationally prohibitive in pra
 ctice. Here we use gaussian process (GP) priors and exploit recent advance
 s in fast GP inference and learning based on Kronecker methods\, to effici
 ently estimate multidimensional nonlinear tuning functions. Our estimator 
 require considerably less data than traditional methods and further provid
 es principled uncertainty estimates. We apply these tools to hippocampal r
 ecordings during open field exploration and use them to characterize the j
 oint dependence of CA1 responses on the position of the animal and several
  other variables\, including the animal's speed\, direction of motion\, an
 d network oscillations.Our results provide an unprecedentedly detailed qua
 ntification of the tuning of hippocampal neurons. The model's generality s
 uggests that our approach can be used to estimate neural response properti
 es in other brain regions.
LOCATION:Area 5+6+7+8 #35
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Bayesian method for reducing bias in neural representational sim
 ilarity analysis | Mingbo Cai \, Nicolas W Schuck \, Jonathan W Pillow \, 
 Yael Niv
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Bayesian method for reducing bias in neural represent
 ational similarity analysis\nMingbo Cai \, Nicolas W Schuck \, Jonathan W 
 Pillow \, Yael Niv\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=689
 6\n\nIn neuroscience\, the similarity matrix of neural activity patterns i
 n response to different sensory stimuli or under different cognitive state
 s reflects the structure of neural representational space. Existing method
 s derive point estimations of neural activity patterns from noisy neural i
 maging data\, and the similarity is calculated from these point estimation
 s. We show that this approach translates structured noise from estimated p
 atterns into spurious bias structure in the resulting similarity matrix\, 
 which is especially severe when signal-to-noise ratio is low and experimen
 tal conditions cannot be fully randomized in a cognitive task. We propose 
 an alternative Bayesian framework for computing representational similarit
 y in which we treat the covariance structure of neural activity patterns a
 s a hyper-parameter in a generative model of the neural data\, and directl
 y estimate this covariance structure from imaging data while marginalizing
  over the unknown activity patterns. Converting the estimated covariance s
 tructure into a correlation matrix offers a much less biased estimate of n
 eural representational similarity. Our method can also simultaneously esti
 mate a signal-to-noise map that informs where the learned representational
  structure is supported more strongly\, and the learned covariance matrix 
 can be used as a structured prior to constrain Bayesian estimation of neur
 al activity patterns. Our code is freely available in Brainiak (https://gi
 thub.com/IntelPNI/brainiak)\, a python toolkit for brain imaging analysis.
LOCATION:Area 5+6+7+8 #36
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning to Communicate with Deep Multi-Agent Reinforcement Learni
 ng | Jakob Foerster \, Yannis M. Assael \, Nando de Freitas \, Shimon Whit
 eson
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning to Communicate with Deep Multi-Agent Reinforce
 ment Learning\nJakob Foerster \, Yannis M. Assael \, Nando de Freitas \, S
 himon Whiteson\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6897\n\
 nWe consider the problem of multiple agents sensing and acting in environm
 ents with the goal of maximising their shared utility. In these environmen
 ts\, agents must learn communication protocols in order to share informati
 on that is needed to solve the tasks. By embracing deep neural networks\, 
 we are able to demonstrate end-to-end learning of protocols in complex env
 ironments inspired by communication riddles and multi-agent computer visio
 n problems with partial observability. We propose two approaches for learn
 ing in these domains: Reinforced Inter-Agent Learning (RIAL) and Different
 iable Inter-Agent Learning (DIAL). The former uses deep Q-learning\, while
  the latter exploits the fact that\, during learning\, agents can backprop
 agate error derivatives through (noisy) communication channels. Hence\, th
 is approach uses centralised learning but decentralised execution. Our exp
 eriments introduce new environments for studying the learning of communica
 tion protocols and present a set of engineering innovations that are essen
 tial for success in these domains.
LOCATION:Area 5+6+7+8 #37
END:VEVENT
BEGIN:VEVENT
SUMMARY:Total Variation Classes Beyond 1d: Minimax Rates\, and the Limitat
 ions of Linear Smoothers | Veeranjaneyulu Sadhanala \, Yu-Xiang Wang \, Ry
 an J Tibshirani
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Total Variation Classes Beyond 1d: Minimax Rates\, and 
 the Limitations of Linear Smoothers\nVeeranjaneyulu Sadhanala \, Yu-Xiang 
 Wang \, Ryan J Tibshirani\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=6898\n\nWe consider the problem of estimating a function defined over 
 $n$ locations on a $d$-dimensional grid (having all side lengths equal to 
 $n^{1/d}$).  When the function is constrained to have discrete total varia
 tion bounded by $Cn$\, we derive the minimax optimal (squared) $\\ell2$ es
 timation error rate\, parametrized by $n\, C_n$. Total variation denoising
 \, also known as the fused lasso\, is seen to be rate optimal.  Several si
 mpler estimators exist\, such as Laplacian smoothing and Laplacian eigenma
 ps.  A natural question is: can these simpler estimators perform just as w
 ell?  We prove that these estimators\, and more broadly all estimators giv
 en by linear transformations of the input data\, are suboptimal over the c
 lass of functions with bounded variation. This extends fundamental finding
 s of Donoho and Johnstone (1998) on 1-dimensional total variation spaces t
 o higher dimensions.  The implication is that the computationally simpler 
 methods cannot be used for such sophisticated denoising tasks\, without sa
 crificing statistical accuracy. We also derive minimax rates for discrete 
 Sobolev spaces over $d$-dimensional grids\, which are\, in some sense\, sm
 aller than the total variation function spaces.  Indeed\, these are small 
 enough spaces that linear estimators can be optimal---and a few well-known
  ones are\, such as Laplacian smoothing and Laplacian eigenmaps\, as we sh
 ow.  Lastly\, we investigate the adaptivity of the total variation denoise
 r to these smaller Sobolev function spaces.
LOCATION:Area 5+6+7+8 #38
END:VEVENT
BEGIN:VEVENT
SUMMARY:Exponential Family Embeddings | Maja Rudolph \, Francisco Ruiz \, 
 Stephan Mandt \, David Blei
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Exponential Family Embeddings\nMaja Rudolph \, Francisc
 o Ruiz \, Stephan Mandt \, David Blei\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=6899\n\nWord embeddings are a powerful approach to capturi
 ng semantic similarity among terms in a vocabulary. In this paper\, we dev
 elop exponential family embeddings\, which extends the idea of word embedd
 ings to other types of high-dimensional data. As examples\, we studied sev
 eral types of data: neural data with real-valued observations\, count data
  from a market basket analysis\, and ratings data from a movie recommendat
 ion system. The main idea is that each observation is modeled conditioned 
 on a set of latent embeddings and other observations\, called the context\
 , where the way the context is defined depends on the problem. In language
  the context is the surrounding words\; in neuroscience the context is clo
 se-by neurons\; in market basket data the context is other items in the sh
 opping cart. Each instance of an embedding defines the context\, the expon
 ential family of conditional distributions\, and how the embedding vectors
  are shared across data. We infer the embeddings with stochastic gradient 
 descent\, with an algorithm that connects closely to generalized linear mo
 dels. On all three of our applications—neural activity of zebrafish\, us
 ers’ shopping behavior\, and movie ratings—we found that exponential f
 amily embedding models are more effective than other dimension reduction m
 ethods. They better reconstruct held-out data and find interesting qualita
 tive structure.
LOCATION:Area 5+6+7+8 #39
END:VEVENT
BEGIN:VEVENT
SUMMARY:k*-Nearest Neighbors: From Global to Local | Oren Anava \, Kfir Le
 vy
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:k*-Nearest Neighbors: From Global to Local\nOren Anava 
 \, Kfir Levy\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6900\n\nT
 he weighted k-nearest neighbors  algorithm is one of the most fundamental 
 non-parametric methods in pattern recognition and machine learning.  The q
 uestion of setting the optimal number of neighbors as well as the optimal 
 weights has received much attention throughout the years\, nevertheless th
 is problem seems to  have remained unsettled. In this paper we offer a sim
 ple approach to locally weighted regression/classification\, where we make
  the bias-variance tradeoff explicit.  Our formulation enables us to phras
 e a notion of optimal weights\, and to efficiently find these weights as w
 ell as the optimal number of neighbors  efficiently and adaptively\, for e
 ach data point whose value we wish to estimate. The applicability of our a
 pproach is demonstrated on several datasets\, showing superior performance
  over standard locally weighted methods.
LOCATION:Area 5+6+7+8 #40
END:VEVENT
BEGIN:VEVENT
SUMMARY:Reward Augmented Maximum Likelihood for Neural Structured Predicti
 on | Mohammad Norouzi \, Samy Bengio \, zhifeng Chen \, Navdeep Jaitly \, 
 Mike Schuster \, Yonghui Wu \, Dale Schuurmans
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Reward Augmented Maximum Likelihood for Neural Structur
 ed Prediction\nMohammad Norouzi \, Samy Bengio \, zhifeng Chen \, Navdeep 
 Jaitly \, Mike Schuster \, Yonghui Wu \, Dale Schuurmans\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=6901\n\nA key problem in structured out
 put prediction is enabling direct optimization of the task reward function
  that matters for test evaluation. This paper presents a simple and comput
 ationally efficient method that incorporates task reward into maximum like
 lihood training. We establish a connection between maximum likelihood and 
 regularized expected reward\, showing that they are approximately equivale
 nt in the vicinity of the optimal solution. Then we show how maximum likel
 ihood can be generalized by optimizing the conditional probability of auxi
 liary outputs that are sampled proportional to their exponentiated scaled 
 rewards. We apply this framework to optimize edit distance in the output s
 pace\, by sampling from edited targets. Experiments on speech recognition 
 and machine translation for neural sequence to sequence models show notabl
 e improvements over maximum likelihood baseline by simply sampling from ta
 rget output augmentations.
LOCATION:Area 5+6+7+8 #41
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Probabilistic Model of Social Decision Making based on Reward Ma
 ximization | Koosha Khalvati \, Seongmin A. Park \, Jean-Claude Dreher \, 
 Rajesh P Rao
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Probabilistic Model of Social Decision Making based o
 n Reward Maximization\nKoosha Khalvati \, Seongmin A. Park \, Jean-Claude 
 Dreher \, Rajesh P Rao\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =6902\n\nA fundamental problem in cognitive neuroscience is how humans mak
 e decisions\, act\, and behave in relation to other humans. Here we adopt 
 the hypothesis that when we are in an interactive social setting\, our bra
 ins perform Bayesian inference of the intentions and cooperativeness of ot
 hers using probabilistic representations. We employ the framework of parti
 ally observable Markov decision processes (POMDPs) to model human decision
  making in a social context\, focusing specifically on the volunteer's dil
 emma in a version of the classic Public Goods Game. We show that the POMDP
  model explains both the behavior of subjects as well as neural activity r
 ecorded using fMRI during the game. The decisions of subjects can be model
 ed across all trials using two interpretable parameters. Furthermore\, the
  expected reward predicted by the model for each subject was correlated wi
 th the activation of brain areas related to reward expectation in social i
 nteractions. Our results suggest a probabilistic basis for human social de
 cision making within the framework of expected reward maximization.
LOCATION:Area 5+6+7+8 #42
END:VEVENT
BEGIN:VEVENT
SUMMARY:Active Learning with Oracle Epiphany | Tzu-Kuo Huang \, Lihong Li 
 \, Ara Vartanian \, Saleema Amershi \, Xiaojin Zhu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Active Learning with Oracle Epiphany\nTzu-Kuo Huang \, 
 Lihong Li \, Ara Vartanian \, Saleema Amershi \, Xiaojin Zhu\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=6903\n\nWe present a theoretical an
 alysis of active learning with more realistic interactions with human orac
 les. Previous empirical studies have shown oracles abstaining on difficult
  queries until accumulating enough information to make label decisions. We
  formalize this phenomenon with an “oracle epiphany model” and analyze
  active learning query complexity under such oracles for both the realizab
 le and the agnos- tic cases. Our analysis shows that active learning is po
 ssible with oracle epiphany\, but incurs an additional cost depending on w
 hen the epiphany happens. Our results suggest new\, principled active lear
 ning approaches with realistic oracles.
LOCATION:Area 5+6+7+8 #43
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Regularizing Rademacher Observation Losses | Richard Nock
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:On Regularizing Rademacher Observation Losses\nRichard 
 Nock\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6904\n\nIt has re
 cently been shown that supervised learning linear classifiers with two of 
 the most popular losses\, the logistic and square loss\, is equivalent to 
 optimizing an equivalent loss over sufficient statistics about the class: 
 Rademacher observations (rados). It has also been shown that learning over
  rados brings solutions to two prominent problems for which the state of t
 he art of learning from examples can be comparatively inferior and in fact
  less convenient: protecting and learning from private examples\, learning
  from distributed datasets without entity resolution.   Bis repetita place
 nt: the two proofs of equivalence are different and rely on specific prope
 rties of the corresponding losses\, so whether these can be unified and ge
 neralized inevitably comes to mind. This is our first contribution: we sho
 w how they can be fit into the same theory for the equivalence between exa
 mple and rado losses. As a second contribution\, we show that the generali
 zation unveils a surprising new connection to regularized learning\, and i
 n particular a sufficient condition under which regularizing the loss over
  examples is equivalent to regularizing the rados (i.e. the data) in the e
 quivalent rado loss\, in such a way that an efficient algorithm for one re
 gularized rado loss may be as efficient when changing the regularizer. Thi
 s is our third contribution: we give a formal boosting algorithm for the r
 egularized exponential rado-loss which boost with any of the ridge\, lasso
 \, \\slope\, l_\\infty\, or elastic nets\, using the same master routine f
 or all. Because the regularized exponential rado-loss is the equivalent of
  the regularized logistic loss over examples we obtain the first efficient
  proxy to the minimisation of the regularized logistic loss over examples 
 using such a wide spectrum of regularizers. Experiments with a readily ava
 ilable code display that regularization significantly improves rado-based 
 learning and compares favourably with example-based learning.
LOCATION:Area 5+6+7+8 #44
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Non-generative Framework and Convex Relaxations for Unsupervised
  Learning | Elad Hazan \, Tengyu Ma
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Non-generative Framework and Convex Relaxations for U
 nsupervised Learning\nElad Hazan \, Tengyu Ma\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=6905\n\nWe give a novel formal theoretical framewo
 rk for unsupervised learning with two distinctive characteristics. First\,
  it does not assume any generative model and based on a worst-case perform
 ance metric. Second\, it is comparative\, namely performance is measured w
 ith respect to a given hypothesis class. This allows to avoid known comput
 ational hardness results and improper algorithms based on convex relaxatio
 ns.  We show how several families of unsupervised learning models\, which 
 were previously only analyzed under probabilistic assumptions and are othe
 rwise provably intractable\, can be efficiently learned in our framework b
 y convex optimization.
LOCATION:Area 5+6+7+8 #45
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Tree Structured Potential Games | Vikas Garg \, Tommi Jaa
 kkola
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning Tree Structured Potential Games\nVikas Garg \,
  Tommi Jaakkola\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6906\n
 \nMany real phenomena\, including behaviors\, involve strategic interactio
 ns that can be learned from data. We focus on learning tree structured pot
 ential games where equilibria are represented by local maxima of an underl
 ying potential function. We cast the learning problem within a max margin 
 setting and show that the problem is NP-hard even when the strategic inter
 actions form a tree. We develop a variant of dual decomposition to estimat
 e the underlying game and demonstrate with synthetic and real decision/vot
 ing data that the game theoretic perspective (carving out local maxima) en
 ables meaningful recovery.
LOCATION:Area 5+6+7+8 #46
END:VEVENT
BEGIN:VEVENT
SUMMARY:Equality of Opportunity in Supervised Learning | Moritz Hardt \,  
  \, Eric Price \, Nati Srebro
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Equality of Opportunity in Supervised Learning\nMoritz 
 Hardt \,   \, Eric Price \, Nati Srebro\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=6907\n\nWe propose a criterion for discrimination agains
 t a specified sensitive attribute in supervised learning\, where the goal 
 is to predict some target based on available features. Assuming data about
  the predictor\, target\, and membership in the protected group are availa
 ble\, we show how to optimally adjust any learned predictor so as to remov
 e discrimination according to our definition. Our framework also improves 
 incentives by shifting the cost of poor classification from disadvantaged 
 groups to the decision maker\, who can respond by improving the classifica
 tion accuracy.
LOCATION:Area 5+6+7+8 #47
END:VEVENT
BEGIN:VEVENT
SUMMARY:Interaction Networks for Learning about Objects\, Relations and Ph
 ysics | Peter Battaglia \, Razvan Pascanu \, Matthew Lai \, Danilo Jimenez
  Rezende \, koray kavukcuoglu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Interaction Networks for Learning about Objects\, Relat
 ions and Physics\nPeter Battaglia \, Razvan Pascanu \, Matthew Lai \, Dani
 lo Jimenez Rezende \, koray kavukcuoglu\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=6908\n\nReasoning about objects\, relations\, and physic
 s is central to human intelligence\, and a key goal of artificial intellig
 ence. Here we introduce the interaction network\, a model which can reason
  about how objects in complex systems interact\, supporting dynamical pred
 ictions\, as well as inferences about the abstract properties of the syste
 m. Our model takes graphs as input\, performs object- and relation-centric
  reasoning in a way that is analogous to a simulation\, and is implemented
  using deep neural networks. We evaluate its ability to reason about sever
 al challenging physical domains: n-body problems\, rigid-body collision\, 
 and non-rigid dynamics. Our results show it can be trained to accurately s
 imulate the physical trajectories of dozens of objects over thousands of t
 ime steps\, estimate abstract quantities such as energy\, and generalize a
 utomatically to systems with different numbers and configurations of objec
 ts and relations. Our interaction network implementation is the first gene
 ral-purpose\, learnable physics engine\, and a powerful general framework 
 for reasoning about object and relations in a wide variety of complex real
 -world domains.
LOCATION:Area 5+6+7+8 #48
END:VEVENT
BEGIN:VEVENT
SUMMARY:beta-risk: a New Surrogate Risk for Learning from Weakly Labeled D
 ata | Valentina Zantedeschi \, Rémi Emonet \, Marc Sebban
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:beta-risk: a New Surrogate Risk for Learning from Weakl
 y Labeled Data\nValentina Zantedeschi \, Rémi Emonet \, Marc Sebban\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=6909\n\nDuring the past few
  years\, the machine learning community has paid attention to developping 
 new methods for learning from weakly labeled data. This field covers diffe
 rent settings like semi-supervised learning\, learning with label proporti
 ons\, multi-instance learning\, noise-tolerant learning\, etc. This paper 
 presents a generic framework to deal with these weakly labeled scenarios. 
 We introduce the beta-risk as a generalized formulation of the standard em
 pirical risk based on surrogate margin-based loss functions. This risk all
 ows us to express the reliability on the labels and to derive different ki
 nds of learning algorithms. We specifically focus on SVMs and propose a so
 ft margin beta-svm algorithm  which behaves better that the state of the a
 rt.
LOCATION:Area 5+6+7+8 #49
END:VEVENT
BEGIN:VEVENT
SUMMARY:Binarized Neural Networks | Itay Hubara \, Matthieu Courbariaux \,
  Daniel Soudry \, Ran El-Yaniv \, Yoshua Bengio
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Binarized Neural Networks\nItay Hubara \, Matthieu Cour
 bariaux \, Daniel Soudry \, Ran El-Yaniv \, Yoshua Bengio\nhttp://nips.cc/
 Conferences/2016/Schedule?showEvent=6910\n\nWe introduce a method to train
  Binarized Neural Networks (BNNs) - neural networks with binary weights an
 d activations at run-time. At train-time the binary weights and activation
 s are used for computing the parameter gradients. During the forward pass\
 , BNNs drastically reduce memory size and accesses\, and replace most arit
 hmetic operations with bit-wise operations\, which is expected to  substan
 tially improve power-efficiency. To validate the effectiveness of BNNs\, w
 e conducted two sets of experiments on the Torch7 and Theano frameworks. O
 n both\, BNNs achieved nearly state-of-the-art results over the MNIST\, CI
 FAR-10 and SVHN datasets. We also report our preliminary results on the ch
 allenging ImageNet dataset. Last but not least\, we wrote a binary matrix 
 multiplication GPU kernel with which it is possible to run our MNIST BNN 7
  times faster  than with an unoptimized GPU kernel\, without suffering any
  loss in classification accuracy. The code for training and running our BN
 Ns is available on-line.
LOCATION:Area 5+6+7+8 #50
END:VEVENT
BEGIN:VEVENT
SUMMARY:Regularization With Stochastic Transformations and Perturbations f
 or Deep Semi-Supervised Learning | Mehdi Sajjadi \, Mehran Javanmardi \, T
 olga Tasdizen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Regularization With Stochastic Transformations and Pert
 urbations for Deep Semi-Supervised Learning\nMehdi Sajjadi \, Mehran Javan
 mardi \, Tolga Tasdizen\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=6911\n\nEffective convolutional neural networks are trained on large set
 s of labeled data. However\, creating large labeled datasets is a very cos
 tly and time-consuming task. Semi-supervised learning uses unlabeled data 
 to train a model with higher accuracy when there is a limited set of label
 ed data available. In this paper\, we consider the problem of semi-supervi
 sed learning with convolutional neural networks. Techniques such as random
 ized data augmentation\, dropout and random max-pooling provide better gen
 eralization and stability for classifiers that are trained using gradient 
 descent. Multiple passes of an individual sample through the network might
  lead to different predictions due to the non-deterministic behavior of th
 ese techniques. We propose an unsupervised loss function that takes advant
 age of the stochastic nature of these methods and minimizes the difference
  between the predictions of multiple passes of a training sample through t
 he network. We evaluate the proposed method on several benchmark datasets.
LOCATION:Area 5+6+7+8 #51
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generating Images with Perceptual Similarity Metrics based on Deep
  Networks | Alexey Dosovitskiy \, Thomas Brox
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Generating Images with Perceptual Similarity Metrics ba
 sed on Deep Networks\nAlexey Dosovitskiy \, Thomas Brox\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=6912\n\nWe propose a class of loss funct
 ions\, which we call deep perceptual similarity metrics (DeePSiM)\, allowi
 ng to generate sharp high resolution images from compressed abstract repre
 sentations. Instead of computing distances in the image space\, we compute
  distances between image features extracted by deep neural networks. This 
 metric reflects perceptual similarity of images much better and\, thus\, l
 eads to better results. We demonstrate two examples of use cases of the pr
 oposed loss: (1) networks that invert the AlexNet convolutional network\; 
 (2) a modified version of a variational autoencoder that generates realist
 ic high-resolution random images.
LOCATION:Area 5+6+7+8 #52
END:VEVENT
BEGIN:VEVENT
SUMMARY:Exploiting Tradeoffs for Exact Recovery in Heterogeneous Stochasti
 c Block Models | Amin Jalali \, Qiyang Han \, Ioana Dumitriu \, Maryam Faz
 el
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Exploiting Tradeoffs for Exact Recovery in Heterogeneou
 s Stochastic Block Models\nAmin Jalali \, Qiyang Han \, Ioana Dumitriu \, 
 Maryam Fazel\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6913\n\nT
 he Stochastic Block Model (SBM) is a widely used random graph model for ne
 tworks with communities. Despite the recent burst of interest in community
  detection under the SBM from statistical and computational points of view
 \, there are still gaps in understanding the fundamental limits of recover
 y. In this paper\, we consider the SBM in its full generality\, where ther
 e is no restriction on the number and sizes of communities or how they gro
 w with the number of nodes\, as well as on the connectivity probabilities 
 inside or across communities. For such stochastic block models\, we provid
 e guarantees for exact recovery via a semidefinite program as well as uppe
 r and lower bounds on SBM parameters for exact recoverability. Our results
  exploit the tradeoffs among the various parameters of heterogenous SBM an
 d provide recovery guarantees for many new interesting SBM configurations.
LOCATION:Area 5+6+7+8 #53
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tensor Switching Networks | Chuan-Yung Tsai \, Andrew M Saxe \, Da
 vid Cox
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Tensor Switching Networks\nChuan-Yung Tsai \, Andrew M 
 Saxe \, David Cox\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6914
 \n\nWe present a novel neural network algorithm\, the Tensor Switching (TS
 ) network\, which generalizes the Rectified Linear Unit (ReLU) nonlinearit
 y to tensor-valued hidden units. The TS network copies its entire input ve
 ctor to different locations in an expanded representation\, with the locat
 ion determined by its hidden unit activity. In this way\, even a simple li
 near readout from the TS representation can implement a highly expressive 
 deep-network-like function. The TS network hence avoids the vanishing grad
 ient problem by construction\, at the cost of larger representation size. 
 We develop several methods to train the TS network\, including equivalent 
 kernels for infinitely wide and deep TS networks\, a one-pass linear learn
 ing algorithm\, and two backpropagation-inspired representation learning a
 lgorithms. Our experimental results demonstrate that the TS network is ind
 eed more expressive and consistently learns faster than standard ReLU netw
 orks.
LOCATION:Area 5+6+7+8 #54
END:VEVENT
BEGIN:VEVENT
SUMMARY:Finite-Dimensional BFRY Priors and Variational Bayesian Inference 
 for Power Law Models | Juho Lee \, Lancelot F James \, Seungjin Choi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Finite-Dimensional BFRY Priors and Variational Bayesian
  Inference for Power Law Models\nJuho Lee \, Lancelot F James \, Seungjin 
 Choi\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6915\n\nBayesian 
 nonparametric  methods based on the Dirichlet process (DP)\, gamma process
  and beta process\, have proven effective in capturing aspects of various 
 datasets arising in machine learning.  However\, it is now recognized that
  such processes have their limitations in terms of the ability to capture 
 power law behavior. As such there is now considerable interest in models b
 ased on the Stable Processs (SP)\, Generalized Gamma process (GGP) and Sta
 ble-beta process (SBP). These models present new challenges in terms of pr
 actical statistical implementation. In analogy to tractable processes such
  as the finite-dimensional Dirichlet process\, we describe a class of rand
 om processes\, we call iid finite-dimensional BFRY processes\, that enable
 s one to begin to develop efficient posterior inference algorithms such as
  variational Bayes that readily scale to massive datasets. For illustrativ
 e purposes\, we describe a simple variational Bayes algorithm for normaliz
 ed SP mixture models\, and demonstrate its usefulness with experiments on 
 synthetic and real-world datasets.
LOCATION:Area 5+6+7+8 #55
END:VEVENT
BEGIN:VEVENT
SUMMARY:Temporal Regularized Matrix Factorization for High-dimensional Tim
 e Series Prediction | Hsiang-Fu Yu \, Nikhil Rao \, Inderjit S Dhillon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Temporal Regularized Matrix Factorization for High-dime
 nsional Time Series Prediction\nHsiang-Fu Yu \, Nikhil Rao \, Inderjit S D
 hillon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6916\n\nTime se
 ries prediction problems are becoming increasingly high-dimensional in mod
 ern applications\, such as climatology and demand forecasting. For example
 \, in the latter problem\, the number of items for which demand needs to b
 e forecast might be as large as 50\,000. In addition\, the data is general
 ly noisy and full of missing values. Thus\, modern applications require me
 thods that are  highly scalable\, and can deal with noisy data in terms of
  corruptions or missing values. However\, classical time series methods us
 ually fall short of handling these issues.  In this paper\, we present a t
 emporal regularized matrix factorization  (TRMF) framework which supports 
 data-driven temporal learning and  forecasting. We develop novel regulariz
 ation schemes and use scalable matrix factorization methods that are emine
 ntly suited for high-dimensional time series data that has many missing va
 lues.  Our proposed TRMF is highly general\, and subsumes many existing ap
 proaches for time series analysis.  We make interesting connections to gra
 ph regularization methods in the context of learning the dependencies in a
 n autoregressive framework. Experimental results show the superiority of T
 RMF in terms of scalability and prediction quality. In particular\,  TRMF 
 is two orders of magnitude  faster than other methods on a problem of dime
 nsion 50\,000\, and generates better forecasts on real-world datasets such
  as Wal-mart E-commerce datasets.
LOCATION:Area 5+6+7+8 #56
END:VEVENT
BEGIN:VEVENT
SUMMARY:Composing graphical models with neural networks for structured rep
 resentations and fast inference | Matthew Johnson \, David Duvenaud \, Ale
 x Wiltschko \, Ryan P Adams \, Sandeep R Datta
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Composing graphical models with neural networks for str
 uctured representations and fast inference\nMatthew Johnson \, David Duven
 aud \, Alex Wiltschko \, Ryan P Adams \, Sandeep R Datta\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=6917\n\nWe propose a general modeling a
 nd inference framework that combines the complementary strengths of probab
 ilistic graphical models and deep learning methods. Our model family compo
 ses latent graphical models with neural network observation likelihoods. F
 or inference\, we use recognition networks to produce local evidence poten
 tials\, then combine them with the model distribution using efficient mess
 age-passing algorithms. All components are trained simultaneously with a s
 ingle stochastic variational inference objective. We illustrate this frame
 work by automatically segmenting and categorizing mouse behavior from raw 
 depth video\, and demonstrate several other example models.
LOCATION:Area 5+6+7+8 #57
END:VEVENT
BEGIN:VEVENT
SUMMARY:PAC Reinforcement Learning with Rich Observations | Akshay Krishna
 murthy \, Alekh Agarwal \, John Langford
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:PAC Reinforcement Learning with Rich Observations\nAksh
 ay Krishnamurthy \, Alekh Agarwal \, John Langford\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=6918\n\nWe propose and study a new model for 
 reinforcement learning with rich observations\, generalizing contextual ba
 ndits to sequential decision making.  These models require an agent to tak
 e actions based on observations (features) with the goal of achieving long
 -term performance competitive with a large set of policies.  To avoid barr
 iers to sample-efficient learning associated with large observation spaces
  and general POMDPs\, we focus on problems that can be summarized by a sma
 ll number of hidden states and have long-term rewards that are predictable
  by a reactive function class.  In this setting\, we design and analyze a 
 new reinforcement learning algorithm\, Least Squares Value Elimination by 
 Exploration. We prove that the algorithm learns near optimal behavior afte
 r a number of episodes that is polynomial in all relevant parameters\, log
 arithmic in the number of policies\, and independent of the size of the ob
 servation space. Our result provides theoretical justification for reinfor
 cement learning with function approximation.
LOCATION:Area 5+6+7+8 #58
END:VEVENT
BEGIN:VEVENT
SUMMARY:Algorithms and matching lower bounds for approximately-convex opti
 mization | Andrej Risteski \, Yuanzhi Li
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Algorithms and matching lower bounds for approximately-
 convex optimization\nAndrej Risteski \, Yuanzhi Li\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=6919\n\nIn recent years\, a rapidly increasin
 g number of applications in practice requires solving non-convex objective
 s\, like training neural networks\, learning graphical models\, maximum li
 kelihood estimation etc. Though simple heuristics such as gradient descent
  with very few modifications tend to work well\, theoretical understanding
  is very weak.   We consider possibly the most natural class of non-convex
  functions where one could hope to obtain provable guarantees: functions t
 hat are ``approximately convex''\, i.e. functions $\\tf: \\Real^d \\to \\R
 eal$ for which there exists a \\emph{convex function} $f$ such that for al
 l $x$\, $|\\tf(x) - f(x)| \\le \\errnoise$ for a fixed value $\\errnoise$.
  We then want to minimize $\\tf$\, i.e. output a point $\\tx$ such that $\
 \tf(\\tx) \\le \\min_{x} \\tf(x) + \\err$.   It is quite natural to conjec
 ture that for fixed $\\err$\, the problem gets harder for larger $\\errnoi
 se$\, however\, the exact dependency of $\\err$ and $\\errnoise$ is not kn
 own. In this paper\, we strengthen the known \\emph{information theoretic}
  lower bounds on the trade-off between $\\err$ and $\\errnoise$ substantia
 lly\, and exhibit an algorithm that matches these lower bounds for a large
  class of convex bodies.
LOCATION:Area 5+6+7+8 #59
END:VEVENT
BEGIN:VEVENT
SUMMARY:Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Opt
 imization | Sashank J. Reddi \, Suvrit Sra \, Barnabas Poczos \, Alexander
  J Smola
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Proximal Stochastic Methods for Nonsmooth Nonconvex Fin
 ite-Sum Optimization\nSashank J. Reddi \, Suvrit Sra \, Barnabas Poczos \,
  Alexander J Smola\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=692
 0\n\nWe analyze stochastic algorithms for optimizing nonconvex\, nonsmooth
  finite-sum problems\, where the nonsmooth part is convex.  Surprisingly\,
  unlike the smooth case\, our knowledge of this fundamental problem is ver
 y limited. For example\, it is not known whether the proximal stochastic g
 radient method with constant minibatch converges to a stationary point. To
  tackle this issue\, we develop fast stochastic algorithms that provably c
 onverge to a stationary point for constant minibatches. Furthermore\, usin
 g a variant of these algorithms\, we obtain provably faster convergence th
 an batch proximal gradient descent. Our results are based on the recent va
 riance reduction techniques for convex optimization but with a novel analy
 sis for handling nonconvex and nonsmooth functions. We also prove global l
 inear convergence rate for an interesting subclass of nonsmooth nonconvex 
 functions\, which subsumes several recent works.
LOCATION:Area 5+6+7+8 #60
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Simple Practical Accelerated Method for Finite Sums | Aaron Defa
 zio
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Simple Practical Accelerated Method for Finite Sums\n
 Aaron Defazio\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6921\n\n
 Abstract We describe a novel optimization method for finite sums (such as 
 empirical risk minimization problems) building on the recently introduced 
 SAGA method. Our method achieves an accelerated convergence rate on strong
 ly convex smooth problems. Our method has only one parameter (a step size)
 \, and is radically simpler than other accelerated methods for finite sums
 . Additionally it can be applied when the terms are non-smooth\, yielding 
 a method applicable in many areas where operator splitting methods would t
 raditionally be applied.
LOCATION:Area 5+6+7+8 #61
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Learning for Physical Interaction through Video Predi
 ction | Chelsea Finn \, Ian Goodfellow \, Sergey Levine
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Unsupervised Learning for Physical Interaction through 
 Video Prediction\nChelsea Finn \, Ian Goodfellow \, Sergey Levine\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=6922\n\nA core challenge for a
 n agent learning to interact with the world is to predict how its actions 
 affect objects in its environment. Many existing methods for learning the 
 dynamics of physical interactions require labeled object information. Howe
 ver\, to scale real-world interaction learning to a variety of scenes and 
 objects\, acquiring labeled data becomes increasingly impractical. To lear
 n about physical object motion without labels\, we develop an action-condi
 tioned video prediction model that explicitly models pixel motion\, by pre
 dicting a distribution over pixel motion from previous frames. Because our
  model explicitly predicts motion\, it is partially invariant to object ap
 pearance\, enabling it to generalize to previously unseen objects. To expl
 ore video prediction for real-world interactive agents\, we also introduce
  a dataset of 59\,000 robot interactions involving pushing motions\, inclu
 ding a test set with novel objects. In this dataset\, accurate prediction 
 of videos conditioned on the robot's future actions amounts to learning a 
 "visual imagination" of different futures based on different courses of ac
 tion. Our experiments show that our proposed method produces more accurate
  video predictions both quantitatively and qualitatively\, when compared t
 o prior methods.
LOCATION:Area 5+6+7+8 #62
END:VEVENT
BEGIN:VEVENT
SUMMARY:Threshold Learning for Optimal Decision Making | Nathan F Lepora
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Threshold Learning for Optimal Decision Making\nNathan 
 F Lepora\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6923\n\nDecis
 ion making under uncertainty is commonly modelled as a process of competit
 ive stochastic evidence accumulation to threshold (the drift-diffusion mod
 el). However\, it is unknown how animals learn these decision thresholds. 
 We examine threshold learning by constructing a reward function that avera
 ges over many trials to Wald's cost function that defines decision optimal
 ity. These rewards are highly stochastic and hence challenging to optimize
 \, which we address in two ways: first\, a simple two-factor reward-modula
 ted learning rule derived from Williams' REINFORCE method for neural netwo
 rks\; and second\, Bayesian optimization of the reward function with a Gau
 ssian process. Bayesian optimization converges in fewer trials than REINFO
 RCE but is slower computationally with greater variance. The REINFORCE met
 hod is also a better model of acquisition behaviour in animals and a simil
 ar learning rule has been proposed for modelling basal ganglia function.
LOCATION:Area 5+6+7+8 #63
END:VEVENT
BEGIN:VEVENT
SUMMARY:Collaborative Recurrent Autoencoder: Recommend while Learning to F
 ill in the Blanks | Hao Wang \, Xingjian SHI \, Dit-Yan Yeung
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Collaborative Recurrent Autoencoder: Recommend while Le
 arning to Fill in the Blanks\nHao Wang \, Xingjian SHI \, Dit-Yan Yeung\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=6924\n\nHybrid methods t
 hat utilize both content and rating information are commonly used in many 
 recommender systems. However\, most of them use either handcrafted feature
 s or the bag-of-words representation as a surrogate for the content inform
 ation but they are neither effective nor natural enough. To address this p
 roblem\, we develop a collaborative recurrent autoencoder (CRAE) which is 
 a denoising recurrent autoencoder (DRAE) that models the generation of con
 tent sequences in the collaborative filtering (CF) setting. The model gene
 ralizes recent advances in recurrent deep learning from i.i.d. input to no
 n-i.i.d. (CF-based) input and provides a new denoising scheme along with a
  novel learnable pooling scheme for the recurrent autoencoder. To do this\
 , we first develop a hierarchical Bayesian model for the DRAE and then gen
 eralize it to the CF setting. The synergy between denoising and CF enables
  CRAE to make accurate recommendations while learning to fill in the blank
 s in sequences. Experiments on real-world datasets from different domains 
 (CiteULike and Netflix) show that\, by jointly modeling the order-aware ge
 neration of sequences for the content information and performing CF for th
 e ratings\, CRAE is able to significantly outperform the state of the art 
 on both the recommendation task based on ratings and the sequence generati
 on task based on content information.
LOCATION:Area 5+6+7+8 #64
END:VEVENT
BEGIN:VEVENT
SUMMARY:Finding significant combinations of features in the presence of ca
 tegorical covariates | Laetitia Papaxanthos \, Felipe Llinares-Lopez \, De
 an Bodenham \, Karsten Borgwardt
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Finding significant combinations of features in the pre
 sence of categorical covariates\nLaetitia Papaxanthos \, Felipe Llinares-L
 opez \, Dean Bodenham \, Karsten Borgwardt\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=6925\n\nIn high-dimensional settings\, where the numb
 er of features p is typically much larger than the number of samples n\, m
 ethods which can systematically examine arbitrary combinations of features
 \, a huge 2^p-dimensional space\, have recently begun to be explored. Howe
 ver\, none of the current methods is able to assess the association betwee
 n feature combinations and a target variable while conditioning on a categ
 orical covariate\, in order to correct for potential confounding effects. 
  We propose the Fast Automatic Conditional Search (FACS) algorithm\, a sig
 nificant discriminative itemset mining method which conditions on categori
 cal covariates and only scales as O(k log k)\, where k is the number of st
 ates of the categorical covariate. Based on the Cochran-Mantel-Haenszel Te
 st\, FACS demonstrates superior speed and statistical power on simulated a
 nd real-world datasets compared to the state of the art\, opening the door
  to numerous applications in biomedicine.
LOCATION:Area 5+6+7+8 #65
END:VEVENT
BEGIN:VEVENT
SUMMARY:Synthesizing the preferred inputs for neurons in neural networks v
 ia deep generator networks | Anh Nguyen \, Alexey Dosovitskiy \, Jason Yos
 inski \, Thomas Brox \, Jeff Clune
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Synthesizing the preferred inputs for neurons in neural
  networks via deep generator networks\nAnh Nguyen \, Alexey Dosovitskiy \,
  Jason Yosinski \, Thomas Brox \, Jeff Clune\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=6926\n\nDeep neural networks (DNNs) have demonstrat
 ed state-of-the-art results on many pattern recognition tasks\, especially
  vision classification problems. Understanding the inner workings of such 
 computational brains is both fascinating basic science that is interesting
  in its own right---similar to why we study the human brain---and will ena
 ble researchers to further improve DNNs. One path to understanding how a n
 eural network functions internally is to study what each of its neurons ha
 s learned to detect. One such method is called activation maximization\, w
 hich synthesizes an input (e.g. an image) that highly activates a neuron. 
 Here we dramatically improve the qualitative state of the art of activatio
 n maximization by harnessing a powerful\, learned prior: a deep generator 
 network. The algorithm (1) generates qualitatively state-of-the-art synthe
 tic images that look almost real\, (2) reveals the features learned by eac
 h neuron in an interpretable way\, (3) generalizes well to new datasets an
 d somewhat well to different network architectures without requiring the p
 rior to be relearned\, and (4) can be considered as a high-quality generat
 ive method (in this case\, by generating novel\, creative\, interesting\, 
 recognizable images).
LOCATION:Area 5+6+7+8 #66
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Infinite RBMs with Frank-Wolfe | Wei Ping \, Qiang Liu \,
  Alexander Ihler
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning Infinite RBMs with Frank-Wolfe\nWei Ping \, Qi
 ang Liu \, Alexander Ihler\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=6927\n\nIn this work\, we propose an infinite restricted Boltzmann ma
 chine (RBM)\, whose maximum likelihood estimation (MLE) corresponds to a c
 onstrained convex optimization.  We consider the Frank-Wolfe algorithm to 
 solve the program\, which provides a sparse solution that can be interpret
 ed as inserting a hidden unit at each iteration\, so that the optimization
  process takes the form of a sequence of finite models of increasing compl
 exity.  As a side benefit\, this can be used to easily and efficiently ide
 ntify an appropriate number of hidden units during the optimization. The r
 esulting model can also be used as an initialization for typical state-of-
 the-art RBM training algorithms such as contrastive divergence\, leading t
 o models with consistently higher test likelihood than random initializati
 on.
LOCATION:Area 5+6+7+8 #67
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sorting out typicality with the inverse moment matrix SOS polynomi
 al | Edouard Pauwels \, Jean B Lasserre
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Sorting out typicality with the inverse moment matrix S
 OS polynomial\nEdouard Pauwels \, Jean B Lasserre\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=6928\n\nWe study a surprising phenomenon relat
 ed to the representation of a cloud of data points using polynomials. We s
 tart with the previously unnoticed empirical observation that\, given a co
 llection (a cloud) of data points\, the sublevel sets of a certain disting
 uished polynomial capture the shape of the cloud very accurately. This dis
 tinguished polynomial is a sum-of-squares (SOS) derived in a simple manner
  from the inverse of the empirical moment matrix. In fact\, this SOS polyn
 omial is directly related to orthogonal polynomials and the Christoffel fu
 nction. This allows to generalize and interpret extremality properties of 
 orthogonal polynomials and to provide a mathematical rationale for the obs
 erved phenomenon. Among diverse potential applications\, we illustrate the
  relevance of our results on a network intrusion detection task for which 
 we obtain performances similar to existing dedicated methods reported in t
 he literature.
LOCATION:Area 5+6+7+8 #68
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improving PAC Exploration Using the Median Of Means | Jason Pazis 
 \, Ronald E Parr \, Jonathan P How
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Improving PAC Exploration Using the Median Of Means\nJa
 son Pazis \, Ronald E Parr \, Jonathan P How\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=6929\n\nWe present the first application of the med
 ian of means in a PAC exploration algorithm for MDPs. Using the median of 
 means allows us to significantly reduce the dependence of our bounds on th
 e range of values that the value function can take\, while introducing a d
 ependence on the (potentially much smaller) variance of the Bellman operat
 or. Additionally\, our algorithm is the first algorithm with PAC bounds th
 at can be applied to MDPs with unbounded rewards.
LOCATION:Area 5+6+7+8 #69
END:VEVENT
BEGIN:VEVENT
SUMMARY:Reconstructing Parameters of Spreading Models from Partial Observa
 tions | Andrey Lokhov
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Reconstructing Parameters of Spreading Models from Part
 ial Observations\nAndrey Lokhov\nhttp://nips.cc/Conferences/2016/Schedule?
 showEvent=6930\n\nSpreading processes are often modelled as a stochastic d
 ynamics occurring on top of a given network with edge weights correspondin
 g to the transmission probabilities. Knowledge of veracious transmission p
 robabilities is essential for prediction\, optimization\, and control of d
 iffusion dynamics. Unfortunately\, in most cases the transmission rates ar
 e unknown and need to be reconstructed from the spreading data. Moreover\,
  in realistic settings it is impossible to monitor the state of each node 
 at every time\, and thus the data is highly incomplete. We introduce an ef
 ficient dynamic message-passing algorithm\, which is able to reconstruct p
 arameters of the spreading model given only partial information on the act
 ivation times of nodes in the network. The method is generalizable to a la
 rge class of dynamic models\, as well to the case of temporal graphs.
LOCATION:Area 5+6+7+8 #70
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dynamic Filter Networks | Xu Jia \, Bert De Brabandere \, Tinne Tu
 ytelaars \, Luc V Gool
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Dynamic Filter Networks\nXu Jia \, Bert De Brabandere \
 , Tinne Tuytelaars \, Luc V Gool\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=6931\n\nIn a traditional convolutional layer\, the learned filt
 ers stay fixed after training. In contrast\, we introduce a new framework\
 , the Dynamic Filter Network\, where filters are generated dynamically con
 ditioned on an input. We show that this architecture is a powerful one\, w
 ith increased flexibility thanks to its adaptive nature\, yet without an e
 xcessive increase in the number of model parameters. A wide variety of fil
 tering operation can be learned this way\, including local spatial transfo
 rmations\, but also others like selective (de)blurring or adaptive feature
  extraction. Moreover\, multiple such layers can be combined\, e.g. in a r
 ecurrent architecture. We demonstrate the effectiveness of the dynamic fil
 ter network on the tasks of video and stereo prediction\, and reach state-
 of-the-art performance on the moving MNIST dataset with a much smaller mod
 el. By visualizing the learned filters\, we illustrate that the network ha
 s picked up flow information by only looking at unlabelled training data. 
 This suggests that the network can be used to pretrain networks for variou
 s supervised tasks in an unsupervised way\, like optical flow and depth es
 timation.
LOCATION:Area 5+6+7+8 #71
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generating Long-term Trajectories Using Deep Hierarchical Networks
  | Stephan Zheng \, Yisong Yue \, Jennifer Hobbs
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Generating Long-term Trajectories Using Deep Hierarchic
 al Networks\nStephan Zheng \, Yisong Yue \, Jennifer Hobbs\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=6932\n\nWe study the problem of model
 ing spatiotemporal trajectories over long time horizons using expert demon
 strations. For instance\, in sports\, agents often choose action sequences
  with long-term goals in mind\, such as achieving a certain strategic posi
 tion. Conventional policy learning approaches\, such as those based on Mar
 kov decision processes\, generally fail at learning cohesive long-term beh
 avior in such high-dimensional state spaces\, and are only effective when 
 fairly myopic decision-making yields the desired behavior. The key difficu
 lty is that conventional models are ``single-scale'' and only learn a sing
 le state-action policy. We instead propose a hierarchical policy class tha
 t automatically reasons about both long-term and short-term goals\, which 
 we instantiate as a hierarchical neural network. We showcase our approach 
 in a case study on learning to imitate demonstrated basketball trajectorie
 s\, and show that it generates significantly more realistic trajectories c
 ompared to non-hierarchical baselines as judged by professional sports ana
 lysts.
LOCATION:Area 5+6+7+8 #72
END:VEVENT
BEGIN:VEVENT
SUMMARY:Cooperative Inverse Reinforcement Learning | Dylan Hadfield-Menell
  \, Stuart J Russell \, Pieter Abbeel \, Anca Dragan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Cooperative Inverse Reinforcement Learning\nDylan Hadfi
 eld-Menell \, Stuart J Russell \, Pieter Abbeel \, Anca Dragan\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=6933\n\nFor an autonomous system 
 to be helpful to humans and to pose no unwarranted risks\, it needs to ali
 gn its values with those of the humans in its environment in such a way th
 at its actions contribute to the maximization of value for the humans. We 
 propose a formal definition of the value alignment problem as cooperative 
 inverse reinforcement learning (CIRL). A CIRL problem is a cooperative\, p
 artial- information game with two agents\, human and robot\; both are rewa
 rded according to the human’s reward function\, but the robot does not i
 nitially know what this is. In contrast to classical IRL\, where the human
  is assumed to act optimally in isolation\, optimal CIRL solutions produce
  behaviors such as active teaching\, active learning\, and communicative a
 ctions that are more effective in achieving value alignment. We show that 
 computing optimal joint policies in CIRL games can be reduced to solving a
  POMDP\, prove that optimality in isolation is suboptimal in CIRL\, and de
 rive an approximate CIRL algorithm.
LOCATION:Area 5+6+7+8 #73
END:VEVENT
BEGIN:VEVENT
SUMMARY:Review Networks for Caption Generation | Zhilin Yang \, Ye Yuan \,
  Yuexin Wu \, William W Cohen \, Ruslan Salakhutdinov
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Review Networks for Caption Generation\nZhilin Yang \, 
 Ye Yuan \, Yuexin Wu \, William W Cohen \, Ruslan Salakhutdinov\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=6934\n\nWe propose a novel exten
 sion of the encoder-decoder framework\, called a review network. The revie
 w network is generic and can enhance any existing encoder- decoder model: 
 in this paper\, we consider RNN decoders with both CNN and RNN encoders. T
 he review network performs a number of review steps with attention mechani
 sm on the encoder hidden states\, and outputs a thought vector after each 
 review step\; the thought vectors are used as the input of the attention m
 echanism in the decoder. We show that conventional encoder-decoders are a 
 special case of our framework. Empirically\, we show that our framework im
 proves over state-of- the-art encoder-decoder systems on the tasks of imag
 e captioning and source code captioning.
LOCATION:Area 5+6+7+8 #74
END:VEVENT
BEGIN:VEVENT
SUMMARY:Gradient-based Sampling: An Adaptive Importance Sampling for Least
 -squares | Rong Zhu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Gradient-based Sampling: An Adaptive Importance Samplin
 g for Least-squares\nRong Zhu\nhttp://nips.cc/Conferences/2016/Schedule?sh
 owEvent=6935\n\nIn modern data analysis\, random sampling is an efficient 
 and widely-used strategy to overcome the computational difficulties brough
 t by large sample size. In previous studies\, researchers conducted random
  sampling which is according to the input data but independent on the resp
 onse variable\, however the response variable may also be informative for 
 sampling. In this paper we propose an adaptive sampling called the gradien
 t-based sampling which is dependent on both the input data and the output 
 for fast solving of least-square (LS) problems. We draw the data points by
  random sampling from the full data according to their gradient values. Th
 is sampling is computationally saving\, since the running time of computin
 g the sampling probabilities is reduced to O(nd) where n is the full sampl
 e size and d is the dimension of the input. Theoretically\, we establish a
 n error bound analysis of the general importance sampling with respect to 
 LS solution from full data. The result establishes an improved performance
  of the use of our gradient-based sampling. Synthetic and real data sets a
 re used to empirically argue that the gradient-based sampling has an obvio
 us advantage over existing sampling methods from two aspects of statistica
 l efficiency and computational saving.
LOCATION:Area 5+6+7+8 #75
END:VEVENT
BEGIN:VEVENT
SUMMARY:Robust k-means: a Theoretical Revisit | ALEXANDROS GEORGOGIANNIS
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Robust k-means: a Theoretical Revisit\nALEXANDROS GEORG
 OGIANNIS\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6936\n\nOver 
 the last years\, many variations of the quadratic k-means clustering proce
 dure have been proposed\, all aiming to robustify the performance of the a
 lgorithm in the presence of outliers. In general terms\, two main approach
 es have been developed: one based on penalized regularization methods\, an
 d one based on trimming functions. In this work\, we present a theoretical
  analysis of the robustness and consistency properties of a variant of the
  classical quadratic k-means algorithm\, the robust k-means\, which borrow
 s ideas from outlier detection in regression. We show that two outliers in
  a dataset are enough to breakdown this clustering procedure. However\, if
  we focus on “well-structured” datasets\, then robust k-means can reco
 ver the underlying cluster structure in spite of the outliers. Finally\, w
 e show that\, with slight modifications\, the most general non-asymptotic 
 results for consistency of quadratic k-means remain valid for this robust 
 variant.
LOCATION:Area 5+6+7+8 #76
END:VEVENT
BEGIN:VEVENT
SUMMARY:Boosting with Abstention | Corinna Cortes \, Giulia DeSalvo \, Meh
 ryar Mohri
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Boosting with Abstention\nCorinna Cortes \, Giulia DeSa
 lvo \, Mehryar Mohri\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6
 937\n\nWe present a new boosting algorithm for the key scenario of binary 
 classification with abstention where the algorithm can abstain from predic
 ting the label of a point\, at the price of a fixed cost.  At each round\,
  our algorithm selects a pair of functions\, a base predictor and a base a
 bstention function.  We define convex upper bounds for the natural loss fu
 nction associated to this problem\, which we prove to be calibrated with r
 espect to the Bayes solution. Our algorithm benefits from general margin-b
 ased learning guarantees which we derive for ensembles of pairs of base pr
 edictor and abstention functions\, in terms of the Rademacher complexities
  of the corresponding function classes.  We give convergence guarantees fo
 r our algorithm along with a linear-time weak-learning algorithm for abste
 ntion stumps. We also report the results of several experiments suggesting
  that our algorithm provides a significant improvement in practice over tw
 o confidence-based algorithms.
LOCATION:Area 5+6+7+8 #77
END:VEVENT
BEGIN:VEVENT
SUMMARY:Estimating the class prior and posterior from noisy positives and 
 unlabeled data | Shantanu J Jain \, Martha White \, Predrag Radivojac
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Estimating the class prior and posterior from noisy pos
 itives and unlabeled data\nShantanu J Jain \, Martha White \, Predrag Radi
 vojac\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6938\n\nWe devel
 op a classification algorithm for estimating posterior distributions from 
 positive-unlabeled data\, that is robust to noise in the positive labels a
 nd effective for high-dimensional data. In recent years\, several algorith
 ms have been proposed to learn from positive-unlabeled data\; however\, ma
 ny of these contributions remain theoretical\, performing poorly on real h
 igh-dimensional data that is typically contaminated with noise. We build o
 n this previous work to develop two practical classification algorithms th
 at explicitly model the noise in the positive labels and utilize univariat
 e transforms built on discriminative classifiers. We prove that these univ
 ariate transforms preserve the class prior\, enabling estimation in the un
 ivariate space and avoiding kernel density estimation for high-dimensional
  data. The theoretical development and parametric and nonparametric algori
 thms proposed here constitute an important step towards wide-spread use of
  robust classification algorithms for positive-unlabeled data.
LOCATION:Area 5+6+7+8 #78
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bootstrap Model Aggregation for Distributed Statistical Learning |
  JUN HAN \, Qiang Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Bootstrap Model Aggregation for Distributed Statistical
  Learning\nJUN HAN \, Qiang Liu\nhttp://nips.cc/Conferences/2016/Schedule?
 showEvent=6939\n\nIn distributed\, or privacy-preserving learning\, we are
  often given a set of probabilistic models estimated from different local 
 repositories\, and asked to combine them into a single model that gives ef
 ficient statistical estimation. A simple method is to linearly average the
  parameters of the local models\, which\, however\, tends to be degenerate
  or not applicable on non-convex models\, or models with different paramet
 er dimensions. One more practical strategy is to generate bootstrap sample
 s from the local models\, and then learn a joint model based on the combin
 ed bootstrap set. Unfortunately\, the bootstrap procedure introduces addit
 ional noise and can significantly deteriorate the performance. In this wor
 k\, we propose two variance reduction methods to correct the bootstrap noi
 se\, including a weighted M-estimator that is both statistically efficient
  and practically powerful. Both theoretical and empirical analysis is prov
 ided to demonstrate our methods.
LOCATION:Area 5+6+7+8 #79
END:VEVENT
BEGIN:VEVENT
SUMMARY:Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling |
  Maria-Florina Balcan \, Hongyang Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Noise-Tolerant Life-Long Matrix Completion via Adaptive
  Sampling\nMaria-Florina Balcan \, Hongyang Zhang\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=6940\n\nWe study the problem of recovering an 
 incomplete $m\\times n$ matrix of rank $r$ with columns arriving online ov
 er time. This is known as the problem of life-long matrix completion\, and
  is widely applied to recommendation system\, computer vision\, system ide
 ntification\, etc. The challenge is to design provable algorithms tolerant
  to a large amount of noises\, with small sample complexity. In this work\
 , we give algorithms achieving strong guarantee under two realistic noise 
 models. In bounded deterministic noise\, an adversary can add any bounded 
 yet unstructured noise to each column. For this problem\, we present an al
 gorithm that returns a matrix of a small error\, with sample complexity al
 most as small as the best prior results in the noiseless case. For sparse 
 random noise\, where the corrupted columns are sparse and drawn randomly\,
  we give an algorithm that exactly recovers an $\\mu0$-incoherent matrix b
 y probability at least $1-\\delta$ with sample complexity as small as $O(\
 \mu0rn\\log(r/\\delta))$. This result advances the state-of-the-art work a
 nd matches the lower bound in a worst case. We also study the scenario whe
 re the hidden matrix lies on a mixture of subspaces and show that the samp
 le complexity can be even smaller. Our proposed algorithms perform well ex
 perimentally in both synthetic and real-world datasets.
LOCATION:Area 5+6+7+8 #80
END:VEVENT
BEGIN:VEVENT
SUMMARY:FPNN: Field Probing Neural Networks for 3D Data | Yangyan Li \, So
 eren Pirk \, Hao Su \, Charles R Qi \, Leonidas J Guibas
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:FPNN: Field Probing Neural Networks for 3D Data\nYangya
 n Li \, Soeren Pirk \, Hao Su \, Charles R Qi \, Leonidas J Guibas\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=6941\n\nBuilding discriminati
 ve representations for 3D data has been an important task in computer grap
 hics and computer vision research. Convolutional Neural Networks (CNNs) ha
 ve shown to operate on 2D images with great success for a variety of tasks
 . Lifting convolution operators to 3D (3DCNNs) seems like a plausible and 
 promising next step. Unfortunately\, the computational complexity of 3D CN
 Ns grows cubically with respect to voxel resolution. Moreover\, since most
  3D geometry representations are boundary based\, occupied regions do not 
 increase proportionately with the size of the discretization\, resulting i
 n wasted computation. In this work\, we represent 3D spaces as volumetric 
 fields\, and propose a novel design that employs field probing filters to 
 efficiently extract features from them. Each field probing filter is a set
  of probing points -- sensors that perceive the space. Our learning algori
 thm optimizes not only the weights associated with the probing points\, bu
 t also their locations\, which deforms the shape of the probing filters an
 d adaptively distributes them in 3D space. The optimized probing points se
 nse the 3D space "intelligently"\, rather than operating blindly over the 
 entire domain. We show that field probing is significantly more efficient 
 than 3DCNNs\, while providing state-of-the-art performance\, on classifica
 tion tasks for 3D object recognition benchmark datasets.
LOCATION:Area 5+6+7+8 #81
END:VEVENT
BEGIN:VEVENT
SUMMARY:Causal meets Submodular: Subset Selection with Directed Informatio
 n | Yuxun Zhou \, Costas J Spanos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Causal meets Submodular: Subset Selection with Directed
  Information\nYuxun Zhou \, Costas J Spanos\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=6942\n\nWe study causal subset selection with Direct
 ed Information as the measure of prediction causality. Two typical tasks\,
  causal sensor placement and covariate selection\, are correspondingly for
 mulated into cardinality constrained directed information maximizations. T
 o attack the NP-hard problems\, we show that the first problem is submodul
 ar while not necessarily monotonic. And the second one is ``nearly'' submo
 dular.  To substantiate the idea of approximate submodularity\, we introdu
 ce a novel quantity\, namely submodularity index (SmI)\, for general set f
 unctions. Moreover\, we show that based on SmI\, greedy algorithm has perf
 ormance guarantee for the maximization of possibly non-monotonic and non-s
 ubmodular functions\, justifying its usage for a much broader class of pro
 blems. We evaluate the theoretical results with several case studies\, and
  also illustrate the application of the subset selection to causal structu
 re learning.
LOCATION:Area 5+6+7+8 #82
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improving Variational Autoencoders with Inverse Autoregressive Flo
 w | Diederik P Kingma \, Tim Salimans \, Rafal Jozefowicz \, Xi Chen \, Xi
  Chen \, Ilya Sutskever \, Max Welling
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Improving Variational Autoencoders with Inverse Autoreg
 ressive Flow\nDiederik P Kingma \, Tim Salimans \, Rafal Jozefowicz \, Xi 
 Chen \, Xi Chen \, Ilya Sutskever \, Max Welling\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=6943\n\nWe propose a simple and scalable method
  for improving the flexibility of variational inference through a transfor
 mation with autoregressive neural networks. Autoregressive neural networks
 \, such as RNNs or the PixelCNN\, are very powerful models and potentially
  interesting for use as variational posterior approximation. However\, anc
 estral sampling in such networks is a long sequential operation\, and ther
 efore typically very slow on modern parallel hardware\, such as GPUs. We s
 how that by inverting autoregressive neural networks we can obtain equally
  powerful posterior models from which we can sample efficiently on modern 
 hardware. We show that such data transformations\, inverse autoregressive 
 flows (IAF)\, can be used to transform a simple distribution over the late
 nt variables into a much more flexible distribution\, while still allowing
  us to compute the resulting variables' probability density function. The 
 method is simple to implement\, can be made arbitrarily flexible and\, in 
 contrast with previous work\, is well applicable to models with high-dimen
 sional latent spaces\, such as convolutional generative models.  The metho
 d is applied to a novel deep architecture of variational auto-encoders. In
  experiments with natural images\, we demonstrate that autoregressive flow
  leads to significant performance gains.
LOCATION:Area 5+6+7+8 #83
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Smoothed Online Multi-Task Learning | Keerthiram Murugesa
 n \, Hanxiao Liu \, Jaime Carbonell \, Yiming Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Adaptive Smoothed Online Multi-Task Learning\nKeerthira
 m Murugesan \, Hanxiao Liu \, Jaime Carbonell \, Yiming Yang\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=6944\n\nThis paper addresses the ch
 allenge of jointly learning both the per-task model parameters and the int
 er-task relationships in a multi-task online learning setting. The propose
 d algorithm features probabilistic interpretation\, efficient updating rul
 es and flexible modulation on whether learners focus on their specific tas
 k or on jointly address all tasks.  The paper also proves a sub-linear reg
 ret bound as compared to the best linear predictor in hindsight. Experimen
 ts over three multi-task learning benchmark datasets show advantageous per
 formance of the proposed approach over several state-of-the-art online mul
 ti-task learning baselines.
LOCATION:Area 5+6+7+8 #84
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Limits of Learning with Missing Data | Brian Bullins \, Elad H
 azan \, Tomer Koren
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:The Limits of Learning with Missing Data\nBrian Bullins
  \, Elad Hazan \, Tomer Koren\nhttp://nips.cc/Conferences/2016/Schedule?sh
 owEvent=6945\n\nWe study regression and classification in a setting where 
 the learning algorithm is allowed to access only a limited number of attri
 butes per example\, known as the limited attribute observation model. In t
 his well-studied model\, we provide the first lower bounds giving a limit 
 on the precision attainable by any algorithm for several variants of regre
 ssion\, notably linear regression with the absolute loss and the squared l
 oss\, as well as for classification with the hinge loss. We complement the
 se lower bounds with a general purpose algorithm that gives an upper bound
  on the achievable precision limit in the setting of learning with missing
  data.
LOCATION:Area 5+6+7+8 #85
END:VEVENT
BEGIN:VEVENT
SUMMARY:Safe Exploration in Finite Markov Decision Processes with Gaussian
  Processes | Matteo Turchetta \, Felix Berkenkamp \, Andreas Krause
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Safe Exploration in Finite Markov Decision Processes wi
 th Gaussian Processes\nMatteo Turchetta \, Felix Berkenkamp \, Andreas Kra
 use\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6946\n\nIn classic
 al reinforcement learning agents accept arbitrary short term loss for long
  term gain when exploring their environment. This is infeasible for safety
  critical applications such as robotics\, where even a single unsafe actio
 n may cause system failure or harm the environment. In this paper\, we add
 ress the problem of safely exploring finite Markov decision processes (MDP
 ). We define safety in terms of an a priori unknown safety constraint that
  depends on states and actions and satisfies certain regularity conditions
  expressed via a Gaussian process prior. We develop a novel algorithm\, SA
 FEMDP\, for this task and prove that it completely explores the safely rea
 chable part of the MDP without violating the safety constraint. To achieve
  this\, it cautiously explores safe states and actions in order to gain st
 atistical confidence about the safety of unvisited state-action pairs from
  noisy observations collected while navigating the environment. Moreover\,
  the algorithm explicitly considers reachability when exploring the MDP\, 
 ensuring that it does not get stuck in any state with no safe way out. We 
 demonstrate our method on digital terrain models for the task of exploring
  an unknown map with a rover.
LOCATION:Area 5+6+7+8 #86
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sparse Support Recovery with Non-smooth Loss Functions | Kévin De
 graux \, Gabriel Peyré \, Jalal Fadili \, Laurent Jacques
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Sparse Support Recovery with Non-smooth Loss Functions\
 nKévin Degraux \, Gabriel Peyré \, Jalal Fadili \, Laurent Jacques\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=6947\n\nIn this paper\, we 
 study the support recovery guarantees of underdetermined sparse regression
  using the $\\ell1$-norm as a regularizer and a non-smooth loss function f
 or data fidelity. More precisely\, we focus in detail on the cases of $\\e
 ll1$ and $\\ell\\infty$ losses\, and contrast them with the usual $\\ell2$
  loss.While these losses are routinely used to account for either sparse (
 $\\ell1$ loss) or uniform ($\\ell\\infty$ loss) noise models\, a theoretic
 al analysis of their performance is still lacking. In this article\, we ex
 tend the existing theory from the smooth $\\ell_2$ case to these non-smoot
 h cases. We derive a sharp condition which ensures that the support of the
  vector to recover is stable to small additive noise in the observations\,
  as long as the loss constraint size is tuned proportionally to the noise 
 level. A distinctive feature of our theory is that it also explains what h
 appens when the support is unstable. While the support is not stable anymo
 re\, we identify an "extended support" and show that this extended support
  is stable to small additive noise. To exemplify the usefulness of our the
 ory\, we give a detailed numerical analysis of the support stability/insta
 bility of compressed sensing recovery with these different losses. This hi
 ghlights different parameter regimes\, ranging from total support stabilit
 y to progressively increasing support instability.
LOCATION:Area 5+6+7+8 #87
END:VEVENT
BEGIN:VEVENT
SUMMARY:Crowdsourced Clustering: Querying Edges vs Triangles | Ramya Korla
 kai Vinayak \, Babak Hassibi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Crowdsourced Clustering: Querying Edges vs Triangles\nR
 amya Korlakai Vinayak \, Babak Hassibi\nhttp://nips.cc/Conferences/2016/Sc
 hedule?showEvent=6948\n\nWe consider the task of clustering items using an
 swers from non-expert crowd workers. In such cases\, the workers are often
  not able to label the items directly\, however\, it is reasonable to assu
 me that they can compare items and judge whether they are similar or not. 
 An important question is what queries to make\, and we compare two types: 
 random edge queries\, where a pair of items is revealed\, and random trian
 gles\, where a triple is. Since it is far too expensive to query all possi
 ble edges and/or triangles\, we need to work with partial observations sub
 ject to a fixed query budget constraint. When a generative model for the d
 ata is available (and we consider a few of these) we determine the cost of
  a query by its entropy\; when such models do not exist we use the average
  response time per query of the workers as a surrogate for the cost. In ad
 dition to theoretical justification\, through several simulations and expe
 riments on two real data sets on Amazon Mechanical Turk\, we empirically d
 emonstrate that\, for a fixed budget\, triangle queries uniformly outperfo
 rm edge queries. Even though\, in contrast to edge queries\, triangle quer
 ies reveal dependent edges\, they provide more reliable edges and\, for a 
 fixed budget\, many more of them. We also provide a sufficient condition o
 n the number of observations\, edge densities inside and outside the clust
 ers and the minimum cluster size required for the exact recovery of the tr
 ue adjacency matrix via triangle queries using a convex optimization-based
  clustering algorithm.
LOCATION:Area 5+6+7+8 #88
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dual Decomposed Learning with Factorwise Oracle for Structural SVM
  of Large Output Domain | Ian En-Hsu Yen \, Xiangru Huang \, Kai Zhong \, 
 Ruohan Zhang \, Pradeep K Ravikumar \, Inderjit S Dhillon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Dual Decomposed Learning with Factorwise Oracle for Str
 uctural SVM of Large Output Domain\nIan En-Hsu Yen \, Xiangru Huang \, Kai
  Zhong \, Ruohan Zhang \, Pradeep K Ravikumar \, Inderjit S Dhillon\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=6949\n\nMany applications of
  machine learning involve structured output with large domain\, where lear
 ning of structured predictor is prohibitive due to repetitive calls to exp
 ensive inference oracle. In this work\, we show that\, by decomposing trai
 ning of Structural Support Vector Machine (SVM) into a series of multiclas
 s SVM problems connected through messages\, one can replace expensive stru
 ctured oracle with Factorwise Maximization Oracle (FMO) that allows effici
 ent implementation of complexity sublinear to the factor domain. A Greedy 
 Direction Method of Multiplier (GDMM) algorithm is proposed to exploit spa
 rsity of messages which guarantees $\\epsilon$ sub-optimality after $O(log
 (1/\\epsilon))$ passes of FMO calls. We conduct experiments on chain-struc
 tured problems and fully-connected problems of large output domains. The p
 roposed approach is orders-of-magnitude faster than the state-of-the-art t
 raining algorithms for Structural SVM.
LOCATION:Area 5+6+7+8 #89
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sampling for Bayesian Program Learning | Kevin Ellis \, Armando So
 lar-Lezama \, Josh Tenenbaum
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Sampling for Bayesian Program Learning\nKevin Ellis \, 
 Armando Solar-Lezama \, Josh Tenenbaum\nhttp://nips.cc/Conferences/2016/Sc
 hedule?showEvent=6950\n\nTowards learning programs from data\, we introduc
 e the problem of   sampling programs from posterior distributions conditio
 ned on that   data. Within this setting\, we propose an algorithm that use
 s a   symbolic solver to efficiently sample programs.  The proposal   comb
 ines constraint-based program synthesis with sampling via random   parity 
 constraints.  We give theoretical guarantees on how well the   samples app
 roximate the true posterior\, and have empirical results   showing the alg
 orithm is efficient in practice\, evaluating our   approach on 22 program 
 learning problems in the domains of text   editing and computer-aided prog
 ramming.
LOCATION:Area 5+6+7+8 #90
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multiple-Play Bandits in the Position-Based Model | Paul Lagrée \
 , Claire Vernade \, Olivier Cappe
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Multiple-Play Bandits in the Position-Based Model\nPaul
  Lagrée \, Claire Vernade \, Olivier Cappe\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=6951\n\nSequentially learning to place items in mult
 i-position displays or lists is a task that can be cast into the multiple-
 play semi-bandit setting. However\, a major concern in this context is whe
 n the system cannot decide whether the user feedback for each item is actu
 ally exploitable. Indeed\, much of the content may have been simply ignore
 d by the user. The present work proposes to exploit available information 
 regarding the display position bias under the so-called Position-based cli
 ck model (PBM). We first discuss how this model differs from the Cascade m
 odel and its variants considered in several recent works on multiple-play 
 bandits. We then provide a novel regret lower bound for this model as well
  as computationally efficient algorithms that display good empirical and t
 heoretical performance.
LOCATION:Area 5+6+7+8 #91
END:VEVENT
BEGIN:VEVENT
SUMMARY:Image Restoration Using Very Deep Convolutional Encoder-Decoder Ne
 tworks with Symmetric Skip Connections | Xiaojiao Mao \, Chunhua Shen \, Y
 u-Bin Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Image Restoration Using Very Deep Convolutional Encoder
 -Decoder Networks with Symmetric Skip Connections\nXiaojiao Mao \, Chunhua
  Shen \, Yu-Bin Yang\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6
 952\n\nIn this paper\, we propose a very deep fully convolutional encoding
 -decoding framework for image restoration such as denoising and super-reso
 lution. The network is composed of multiple layers of convolution and deco
 nvolution operators\, learning end-to-end mappings from corrupted images t
 o the original ones. The convolutional layers act as the feature extractor
 \, which capture the abstraction of image contents while eliminating noise
 s/corruptions. Deconvolutional layers are then used to recover the image d
 etails. We propose to symmetrically link convolutional and deconvolutional
  layers with skip-layer connections\, with which the training converges mu
 ch faster and attains a higher-quality local optimum. First\, the skip con
 nections allow the signal to be back-propagated to bottom layers directly\
 , and thus tackles the problem of gradient vanishing\, making training dee
 p networks easier and achieving restoration performance gains consequently
 . Second\, these skip connections pass image details from convolutional la
 yers to deconvolutional layers\, which is beneficial in recovering the ori
 ginal image. Significantly\, with the large capacity\, we can handle diffe
 rent levels of noises using a single model. Experimental results show that
  our network achieves better performance than recent state-of-the-art meth
 ods.
LOCATION:Area 5+6+7+8 #92
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimistic Bandit Convex Optimization | Scott Yang \, Mehryar Mohr
 i
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Optimistic Bandit Convex Optimization\nScott Yang \, Me
 hryar Mohri\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6953\n\nWe
  introduce the general and powerful scheme of predicting information re-us
 e in optimization algorithms. This allows us to devise a computationally e
 fficient algorithm for bandit convex optimization with new state-of-the-ar
 t guarantees for both Lipschitz loss functions and loss functions with Lip
 schitz gradients. This is the first algorithm admitting both a polynomial 
 time complexity and a regret that is polynomial in the dimension of the ac
 tion space that improves upon the original regret bound for Lipschitz loss
  functions\, achieving a regret of $\\widetilde O(T^{11/16}d^{3/8})$. Our 
 algorithm further improves upon the best existing polynomial-in-dimension 
 bound (both computationally and in terms of regret) for loss functions wit
 h Lipschitz gradients\, achieving a regret of $\\widetilde O(T^{8/13} d^{5
 /3})$.
LOCATION:Area 5+6+7+8 #93
END:VEVENT
BEGIN:VEVENT
SUMMARY:Computing and maximizing influence in linear threshold and trigger
 ing models | Justin T Khim \, Varun Jog \, Po-Ling Loh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Computing and maximizing influence in linear threshold 
 and triggering models\nJustin T Khim \, Varun Jog \, Po-Ling Loh\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=6954\n\nWe establish upper and 
 lower bounds for the influence of a set of nodes in certain types of conta
 gion models. We derive two sets of bounds\, the first designed for linear 
 threshold models\, and the second more broadly applicable to a general cla
 ss of triggering models\, which subsumes the popular independent cascade m
 odels\, as well. We quantify the gap between our upper and lower bounds in
  the case of the linear threshold model and illustrate the gains of our up
 per bounds for independent cascade models in relation to existing results.
  Importantly\, our lower bounds are monotonic and submodular\, implying th
 at a greedy algorithm for influence maximization is guaranteed to produce 
 a maximizer within a (1 - 1/e)-factor of the truth. Although the problem o
 f exact influence computation is NP-hard in general\, our bounds may be ev
 aluated efficiently. This leads to an attractive\, highly scalable algorit
 hm for influence maximization with rigorous theoretical guarantees.
LOCATION:Area 5+6+7+8 #94
END:VEVENT
BEGIN:VEVENT
SUMMARY:Clustering with Bregman Divergences: an Asymptotic Analysis | Chao
 yue Liu \, Mikhail Belkin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Clustering with Bregman Divergences: an Asymptotic Anal
 ysis\nChaoyue Liu \, Mikhail Belkin\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=6955\n\nClustering\, in particular $k$-means clustering\, is
  a central topic in data analysis. Clustering with Bregman divergences is 
 a recently proposed generalization of $k$-means clustering which has alrea
 dy been widely used in applications.  In this paper we analyze theoretical
  properties of Bregman clustering when the number of the clusters $k$ is l
 arge. We establish quantization rates and describe the limiting distributi
 on of the centers as $k\\to \\infty$\, extending well-known results for  $
 k$-means clustering.
LOCATION:Area 5+6+7+8 #95
END:VEVENT
BEGIN:VEVENT
SUMMARY:Community Detection on Evolving Graphs | Stefano Leonardi \, Aris 
 Anagnostopoulos \, Jakub Łącki \, Silvio Lattanzi \, Mohammad Mahdian
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Community Detection on Evolving Graphs\nStefano Leonard
 i \, Aris Anagnostopoulos \, Jakub Łącki \, Silvio Lattanzi \, Mohammad 
 Mahdian\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6956\n\nCluste
 ring is a fundamental step in many information-retrieval and data-mining a
 pplications. Detecting clusters in graphs is also a key tool for finding t
 he community structure in social and behavioral networks. In many of these
  applications\, the input graph evolves over time in a continual and decen
 tralized manner\, and\, to maintain a good clustering\, the clustering alg
 orithm needs to repeatedly probe the graph. Furthermore\, there are often 
 limitations on the frequency of such probes\, either imposed explicitly by
  the online platform (e.g.\, in the case of crawling proprietary social ne
 tworks like twitter) or implicitly because of resource limitations (e.g.\,
  in the case of crawling the web).  In this paper\, we study a model of cl
 ustering on evolving graphs that captures this aspect of the problem. Our 
 model is based on the classical stochastic block model\, which has been us
 ed to assess rigorously the quality of various static clustering methods. 
 In our model\, the algorithm is supposed to reconstruct the planted cluste
 ring\, given the ability to query for small pieces of local information ab
 out the graph\, at a limited rate. We design and analyze clustering algori
 thms that work in this model\, and show asymptotically tight upper and low
 er bounds on their accuracy. Finally\, we perform simulations\, which demo
 nstrate that our main asymptotic results hold true also in practice.
LOCATION:Area 5+6+7+8 #96
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dueling Bandits: Beyond Condorcet Winners to General Tournament So
 lutions | Siddartha Y. Ramamohan \, Arun Rajkumar \, Shivani Agarwal \, Sh
 ivani Agarwal
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Dueling Bandits: Beyond Condorcet Winners to General To
 urnament Solutions\nSiddartha Y. Ramamohan \, Arun Rajkumar \, Shivani Aga
 rwal \, Shivani Agarwal\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=6957\n\nRecent work on deriving $O(\\log T)$ anytime regret bounds for s
 tochastic dueling bandit problems has considered mostly Condorcet winners\
 , which do not always exist\, and more recently\, winners defined by the C
 opeland set\, which do always exist. In this work\, we consider a broad no
 tion of winners defined by tournament solutions in social choice theory\, 
 which include the Copeland set as a special case but also include several 
 other notions of winners such as the top cycle\, uncovered set\, and Banks
  set\, and which\, like the Copeland set\, always exist. We develop a fami
 ly of UCB-style dueling bandit algorithms for such general tournament solu
 tions\, and show $O(\\log T)$ anytime regret bounds for them. Experiments 
 confirm the ability of our algorithms to achieve low regret relative to th
 e target winning set of interest.
LOCATION:Area 5+6+7+8 #97
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning a Metric Embedding  for Face Recognition using the Multib
 atch Method | Oren Tadmor \, Tal Rosenwein \, Shai Shalev-Shwartz \, Yonat
 an Wexler \, Amnon Shashua
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning a Metric Embedding  for Face Recognition using
  the Multibatch Method\nOren Tadmor \, Tal Rosenwein \, Shai Shalev-Shwart
 z \, Yonatan Wexler \, Amnon Shashua\nhttp://nips.cc/Conferences/2016/Sche
 dule?showEvent=6958\n\nThis work is motivated by the engineering task of a
 chieving a near state-of-the-art face recognition on a minimal computing b
 udget running on an embedded system.  Our main technical contribution cent
 ers around a novel training method\, called Multibatch\, for similarity le
 arning\, i.e.\, for the task of generating an invariantface signature'' th
 rough training pairs ofsame'' and ``not-same'' face images. The Multibatch
  method first generates signatures for a mini-batch of $k$ face images and
  then constructs an unbiased estimate of the full gradient by relying on a
 ll $k^2-k$ pairs from the mini-batch. We prove that the variance of the Mu
 ltibatch estimator is bounded by $O(1/k^2)$\, under some mild conditions. 
 In contrast\, the standard gradient estimator that relies on random $k/2$ 
 pairs has a variance of order $1/k$. The smaller variance of the Multibatc
 h estimator significantly speeds up the convergence rate of stochastic gra
 dient descent.  Using the Multibatch method we train a deep convolutional 
 neural network that achieves an accuracy of $98.2\\%$ on the LFW benchmark
 \, while its prediction runtime takes only $30$msec on a single ARM Cortex
  A9 core. Furthermore\, the entire training process took only 12 hours on 
 a single Titan X GPU.
LOCATION:Area 5+6+7+8 #98
END:VEVENT
BEGIN:VEVENT
SUMMARY:Convergence guarantees for kernel-based quadrature rules in misspe
 cified settings | Motonobu Kanagawa \, Bharath K. Sriperumbudur \, Kenji F
 ukumizu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Convergence guarantees for kernel-based quadrature rule
 s in misspecified settings\nMotonobu Kanagawa \, Bharath K. Sriperumbudur 
 \, Kenji Fukumizu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6959
 \n\nKernel-based quadrature rules are becoming important in machine learni
 ng and statistics\, as they achieve super-$¥sqrt{n}$ convergence rates in
  numerical integration\, and thus provide alternatives to Monte Carlo inte
 gration in challenging settings where integrands are expensive to evaluate
  or where integrands are high dimensional. These rules are based on the as
 sumption that the integrand has a certain degree of smoothness\, which is 
 expressed as that the integrand belongs to a certain reproducing kernel Hi
 lbert space (RKHS). However\, this assumption can be violated in practice 
 (e.g.\, when the integrand is a black box function)\, and no general theor
 y has been established for the convergence of kernel quadratures in such m
 isspecified settings. Our contribution is in proving that kernel quadratur
 es can be consistent even when the integrand does not belong to the assume
 d RKHS\, i.e.\,  when the integrand is less smooth than assumed. Specifica
 lly\, we derive convergence rates that depend on the (unknown) lesser smoo
 thness of the integrand\, where the degree of smoothness is expressed via 
 powers of RKHSs or via Sobolev spaces.
LOCATION:Area 5+6+7+8 #99
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Variational Deep Kernel Learning | Andrew G Wilson \, Z
 hiting Hu \, Ruslan Salakhutdinov \, Eric P Xing
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Stochastic Variational Deep Kernel Learning\nAndrew G W
 ilson \, Zhiting Hu \, Ruslan Salakhutdinov \, Eric P Xing\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=6960\n\nDeep kernel learning combines
  the non-parametric flexibility of kernel methods with the inductive biase
 s of deep learning architectures. We propose a novel deep kernel learning 
 model and stochastic variational inference procedure which generalizes dee
 p kernel learning approaches to enable classification\, multi-task learnin
 g\, additive covariance structures\, and stochastic gradient training. Spe
 cifically\, we apply additive base kernels to subsets of output features f
 rom deep neural architectures\, and jointly learn the parameters of the ba
 se kernels and deep network through a Gaussian process marginal likelihood
  objective.  Within this framework\, we derive an efficient form of stocha
 stic variational inference which leverages local kernel interpolation\, in
 ducing points\, and structure exploiting algebra.  We show improved perfor
 mance over stand alone deep networks\, SVMs\, and state of the art scalabl
 e Gaussian processes on several classification benchmarks\, including an a
 irline delay dataset containing 6 million training points\, CIFAR\, and Im
 ageNet.
LOCATION:Area 5+6+7+8 #100
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Submodular Functions: Definitions and Learning | Brian W Dolh
 ansky \, Jeff A Bilmes
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Deep Submodular Functions: Definitions and Learning\nBr
 ian W Dolhansky \, Jeff A Bilmes\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=6961\n\nWe propose and study a new class of submodular function
 s called deep submodular functions (DSFs). We define DSFs and situate them
  within the broader context of classes of submodular functions in relation
 ship both to various matroid ranks and sums of concave composed with modul
 ar functions (SCMs). Notably\, we find that DSFs constitute a strictly bro
 ader class than SCMs\, thus motivating their use\, but that they do not co
 mprise all submodular functions.  Interestingly\, some DSFs can be seen as
  special cases of certain deep neural networks (DNNs)\, hence the name.  F
 inally\, we provide a method to learn DSFs in a max-margin framework\, and
  offer preliminary results applying this both to synthetic and real-world 
 data instances.
LOCATION:Area 5+6+7+8 #101
END:VEVENT
BEGIN:VEVENT
SUMMARY:Scaled Least Squares Estimator for GLMs in Large-Scale Problems | 
 Murat A Erdogdu \, Lee H Dicker \, Mohsen Bayati
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Scaled Least Squares Estimator for GLMs in Large-Scale 
 Problems\nMurat A Erdogdu \, Lee H Dicker \, Mohsen Bayati\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=6962\n\nWe study the problem of effic
 iently estimating the coefficients of generalized linear models (GLMs) in 
 the large-scale setting where the number of observations $n$ is much large
 r than the number of predictors $p$\, i.e. $n\\gg p \\gg 1$. We show that 
 in GLMs with random (not necessarily Gaussian) design\, the GLM coefficien
 ts are approximately proportional to the corresponding ordinary least squa
 res (OLS) coefficients. Using this relation\, we design an algorithm that 
 achieves the same accuracy as the maximum likelihood estimator (MLE)  thro
 ugh iterations that  attain up to a cubic convergence rate\, and that are 
 cheaper than  any batch optimization algorithm by at least a factor of $\\
 mathcal{O}(p)$. We provide theoretical guarantees for our algorithm\, and 
 analyze the convergence behavior in terms of data dimensions. % Finally\, 
 we demonstrate the performance of  our algorithm through extensive numeric
 al studies  on large-scale real and synthetic datasets\, and show that it 
 achieves the highest performance compared to several other widely used opt
 imization algorithms.
LOCATION:Area 5+6+7+8 #102
END:VEVENT
BEGIN:VEVENT
SUMMARY:High-Rank Matrix Completion and Clustering under Self-Expressive M
 odels | Ehsan Elhamifar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:High-Rank Matrix Completion and Clustering under Self-E
 xpressive Models\nEhsan Elhamifar\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=6963\n\nWe propose efficient algorithms for simultaneous clust
 ering and completion of incomplete high-dimensional data that lie in a uni
 on of low-dimensional subspaces. We cast the problem as finding a completi
 on of the data matrix so that each point can be reconstructed as a linear 
 or affine combination of a few data points. Since the problem is NP-hard\,
  we propose a lifting framework and reformulate the problem as a group-spa
 rse recovery of each incomplete data point in a dictionary built using inc
 omplete data\, subject to rank-one constraints. To solve the problem effic
 iently\, we propose a rank pursuit algorithm and a convex relaxation. The 
 solution of our algorithms recover missing entries and provides a similari
 ty matrix for clustering. Our algorithms can deal with both low-rank and h
 igh-rank matrices\, does not suffer from initialization\, does not need to
  know dimensions of subspaces and can work with a small number of data poi
 nts. By extensive experiments on synthetic data and real problems of video
  motion segmentation and completion of motion capture data\, we show that 
 when the data matrix is low-rank\, our algorithm performs on par with or b
 etter than low-rank matrix completion methods\, while for high-rank data m
 atrices\, our method significantly outperforms existing algorithms.
LOCATION:Area 5+6+7+8 #103
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Three-Composite Convex Minimization | Alp Yurtsever \, 
 Bang Cong Vu \, Volkan Cevher
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Stochastic Three-Composite Convex Minimization\nAlp Yur
 tsever \, Bang Cong Vu \, Volkan Cevher\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=6964\n\nWe propose a stochastic optimization method for 
 the minimization of the sum of three convex functions\, one of which has L
 ipschitz continuous gradient as well as restricted strong convexity. Our a
 pproach is most suitable in the setting where it is computationally advant
 ageous to process smooth term in the decomposition with its stochastic gra
 dient estimate and the other two functions separately with their proximal 
 operators\, such as doubly regularized empirical risk minimization problem
 s. We prove the convergence characterization of the proposed algorithm in 
 expectation under the standard assumptions for the stochastic gradient est
 imate of the smooth term. Our method operates in the primal space and can 
 be considered as a stochastic extension of the three-operator splitting me
 thod. Finally\, numerical evidence supports the effectiveness of our metho
 d in real-world problems.
LOCATION:Area 5+6+7+8 #104
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tree-Structured Reinforcement Learning for Sequential Object Local
 ization | Zequn Jie \, Xiaodan Liang \, Jiashi Feng \, Xiaojie Jin \, Wen 
 Lu \, Shuicheng Yan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Tree-Structured Reinforcement Learning for Sequential O
 bject Localization\nZequn Jie \, Xiaodan Liang \, Jiashi Feng \, Xiaojie J
 in \, Wen Lu \, Shuicheng Yan\nhttp://nips.cc/Conferences/2016/Schedule?sh
 owEvent=6965\n\nExisting object proposal algorithms usually search for pos
 sible object regions over multiple locations and scales \\emph{ separately
 }\, which ignore the interdependency among different objects and deviate f
 rom the human perception procedure. To incorporate global interdependency 
 between objects into object localization\, we propose an effective Tree-st
 ructured Reinforcement Learning (Tree-RL) approach to sequentially search 
 for objects by fully exploiting both the current observation and  historic
 al search paths. The Tree-RL approach learns multiple  searching policies 
 through maximizing the long-term reward that reflects localization accurac
 ies over all the objects. Starting with taking the entire image as a propo
 sal\, the Tree-RL approach allows the agent to sequentially discover multi
 ple objects via a  tree-structured traversing scheme.  Allowing multiple n
 ear-optimal policies\, Tree-RL  offers more diversity in search paths and 
 is able to find multiple objects with a single feed-forward pass. Therefor
 e\, Tree-RL can better cover different objects with various scales which i
 s quite appealing in the context of object proposal. Experiments on PASCAL
  VOC 2007 and 2012 validate the effectiveness of the Tree-RL\, which can a
 chieve comparable recalls with current object proposal algorithms via much
  fewer candidate windows.
LOCATION:Area 5+6+7+8 #105
END:VEVENT
BEGIN:VEVENT
SUMMARY:The non-convex Burer-Monteiro approach works on smooth semidefinit
 e programs | Nicolas Boumal \, Vlad Voroninski \, Afonso Bandeira
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:The non-convex Burer-Monteiro approach works on smooth 
 semidefinite programs\nNicolas Boumal \, Vlad Voroninski \, Afonso Bandeir
 a\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6966\n\nSemidefinite
  programs (SDP's) can be solved in polynomial time by interior point metho
 ds\, but scalability can be an issue. To address this shortcoming\, over a
  decade ago\, Burer and Monteiro proposed to solve SDP's with few equality
  constraints via rank-restricted\, non-convex surrogates. Remarkably\, for
  some applications\, local optimization methods seem to converge to global
  optima of these non-convex surrogates reliably. Although some theory supp
 orts this empirical success\, a complete explanation of it remains an open
  question. In this paper\, we consider a class of SDP's which includes app
 lications such as max-cut\, community detection in the stochastic block mo
 del\, robust PCA\, phase retrieval and synchronization of rotations. We sh
 ow that the low-rank Burer-Monteiro formulation of SDP's in that class alm
 ost never has any spurious local optima.
LOCATION:Area 5+6+7+8 #106
END:VEVENT
BEGIN:VEVENT
SUMMARY:Neurons Equipped with Intrinsic Plasticity Learn Stimulus Intensit
 y Statistics | Travis Monk \, Cristina Savin \, Jörg Lücke
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Neurons Equipped with Intrinsic Plasticity Learn Stimul
 us Intensity Statistics\nTravis Monk \, Cristina Savin \, Jörg Lücke\nht
 tp://nips.cc/Conferences/2016/Schedule?showEvent=6967\n\nExperience consta
 ntly shapes neural circuits through a variety of plasticity mechanisms. Wh
 ile the functional roles of some plasticity mechanisms are well-understood
 \, it remains unclear how changes in neural excitability contribute to lea
 rning. Here\, we develop a normative interpretation of intrinsic plasticit
 y (IP) as a key component of unsupervised learning. We introduce a novel g
 enerative mixture model that accounts for the class-specific statistics of
  stimulus intensities\, and we derive a neural circuit that learns the inp
 ut classes and their intensities. We will analytically show that inference
  and learning for our generative model can be achieved by a neural circuit
  with intensity-sensitive neurons equipped with a specific form of IP. Num
 erical experiments verify our analytical derivations and show robust behav
 ior for artificial and natural stimuli. Our results link IP to non-trivial
  input statistics\, in particular the statistics of stimulus intensities f
 or classes to which a neuron is sensitive. More generally\, our work paves
  the way toward new classification algorithms that are robust to intensity
  variations.
LOCATION:Area 5+6+7+8 #107
END:VEVENT
BEGIN:VEVENT
SUMMARY:Greedy Feature Construction | Dino Oglic \, Thomas Gärtner
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Greedy Feature Construction\nDino Oglic \, Thomas Gärt
 ner\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6968\n\nWe present
  an effective method for supervised feature construction. The main goal of
  the approach is to construct a feature representation for which a set of 
 linear hypotheses is of sufficient capacity -- large enough to contain a s
 atisfactory solution to the considered problem and small enough to allow g
 ood generalization from a small number of training examples. We achieve th
 is goal with a greedy procedure that constructs features by empirically fi
 tting squared error residuals. The proposed constructive procedure is cons
 istent and can output a rich set of features. The effectiveness of the app
 roach is evaluated empirically by fitting a linear ridge regression model 
 in the constructed feature space and our empirical results indicate a supe
 rior performance of our approach over competing methods.
LOCATION:Area 5+6+7+8 #108
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dynamic Mode Decomposition with Reproducing Kernels for Koopman Sp
 ectral Analysis | Yoshinobu Kawahara
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Dynamic Mode Decomposition with Reproducing Kernels for
  Koopman Spectral Analysis\nYoshinobu Kawahara\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=6969\n\nA spectral analysis of the Koopman operat
 or\, which is an infinite dimensional linear operator on an observable\, g
 ives a (modal) description of the global behavior of a nonlinear dynamical
  system without any explicit prior knowledge of its governing equations. I
 n this paper\, we consider a spectral analysis of the Koopman operator in 
 a reproducing kernel Hilbert space (RKHS). We propose a modal decompositio
 n algorithm to perform the analysis using finite-length data sequences gen
 erated from a nonlinear system. The algorithm is in essence reduced to the
  calculation of a set of orthogonal bases for the Krylov matrix in RKHS an
 d the eigendecomposition of the projection of the Koopman operator onto th
 e subspace spanned by the bases. The algorithm returns a decomposition of 
 the dynamics into a finite number of modes\, and thus it can be thought of
  as a feature extraction procedure for a nonlinear dynamical system. There
 fore\, we further consider applications in machine learning using extracte
 d features with the presented analysis. We illustrate the method on the ap
 plications using synthetic and real-world data.
LOCATION:Area 5+6+7+8 #109
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning the Number of Neurons in Deep Networks | Jose M Alvarez \
 , Mathieu Salzmann
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning the Number of Neurons in Deep Networks\nJose M
  Alvarez \, Mathieu Salzmann\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=6970\n\nNowadays\, the number of layers and of neurons in each laye
 r of a deep network are typically set manually. While very deep and wide n
 etworks have proven effective in general\, they come at a high memory and 
 computation cost\, thus making them impractical for constrained platforms.
  These networks\, however\, are known to have many redundant parameters\, 
 and could thus\, in principle\, be replaced by more compact architectures.
  In this paper\, we introduce an approach to automatically determining the
  number of neurons in each layer of a deep network during learning. To thi
 s end\, we propose to make use of a group sparsity regularizer on the para
 meters of the network\, where each group is defined to act on a single neu
 ron. Starting from an overcomplete network\, we show that our approach can
  reduce the number of parameters by up to 80\\% while retaining or even im
 proving the network accuracy.
LOCATION:Area 5+6+7+8 #110
END:VEVENT
BEGIN:VEVENT
SUMMARY:Strategic Attentive Writer for Learning Macro-Actions | Alexander 
 Vezhnevets \, Volodymyr Mnih \, Simon Osindero \, Alex Graves \, Oriol Vin
 yals \, John Agapiou \, koray kavukcuoglu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Strategic Attentive Writer for Learning Macro-Actions\n
 Alexander Vezhnevets \, Volodymyr Mnih \, Simon Osindero \, Alex Graves \,
  Oriol Vinyals \, John Agapiou \, koray kavukcuoglu\nhttp://nips.cc/Confer
 ences/2016/Schedule?showEvent=6971\n\nWe present a novel deep recurrent ne
 ural network architecture that learns to build implicit plans in an end-to
 -end manner purely by interacting with an environment in reinforcement lea
 rning setting. The network builds an internal plan\, which is continuously
  updated upon observation of the next input from the environment. It can a
 lso partition this internal representation into contiguous sub-sequences b
 y learning for how long the plan can be committed to -- i.e. followed with
 out replaning. Combining these properties\, the proposed model\, dubbed ST
 Rategic Attentive Writer (STRAW) can learn high-level\, temporally abstrac
 ted macro-actions of varying lengths that are solely learnt from data with
 out any prior information.  These macro-actions enable both structured exp
 loration and economic computation. We experimentally demonstrate that STRA
 W delivers strong improvements on  several ATARI games by employing tempor
 ally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is a
 t the same time a general algorithm that can be applied on any sequence da
 ta. To that end\, we also show that when trained on text prediction task\,
  STRAW naturally predicts frequent n-grams (instead of macro-actions)\, de
 monstrating the generality of the approach.
LOCATION:Area 5+6+7+8 #111
END:VEVENT
BEGIN:VEVENT
SUMMARY:Active Learning from Imperfect Labelers | Songbai Yan \, Kamalika 
 Chaudhuri \, Tara Javidi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Active Learning from Imperfect Labelers\nSongbai Yan \,
  Kamalika Chaudhuri \, Tara Javidi\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=6972\n\nWe study active learning where the labeler can not on
 ly return incorrect labels but also abstain from labeling. We consider dif
 ferent noise and abstention conditions of the labeler. We propose an algor
 ithm which utilizes abstention responses\, and analyze its statistical con
 sistency and query complexity under fairly natural assumptions on the nois
 e and abstention rate of the labeler. This algorithm is adaptive in a sens
 e that it can automatically request less queries with a more informed or l
 ess noisy labeler. We couple our algorithm with lower bounds to show that 
 under some technical conditions\, it achieves nearly optimal query complex
 ity.
LOCATION:Area 5+6+7+8 #112
END:VEVENT
BEGIN:VEVENT
SUMMARY:Probabilistic Linear Multistep Methods | Onur Teymur \, Kostas Zyg
 alakis \, Ben Calderhead
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Probabilistic Linear Multistep Methods\nOnur Teymur \, 
 Kostas Zygalakis \, Ben Calderhead\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=6973\n\nWe present a derivation and theoretical investigation
  of the Adams-Bashforth and Adams-Moulton family of linear multistep metho
 ds for solving ordinary differential equations\, starting from a Gaussian 
 process (GP) framework. In the limit\, this formulation coincides with the
  classical deterministic methods\, which have been used as higher-order in
 itial value problem solvers for over a century. Furthermore\, the natural 
 probabilistic framework provided by the GP formulation allows us to derive
  probabilistic versions of these methods\, in the spirit of a number of ot
 her probabilistic ODE solvers presented in the recent literature. In contr
 ast to higher-order Runge-Kutta methods\, which require multiple intermedi
 ate function evaluations per step\, Adams family methods make use of previ
 ous function evaluations\, so that increased accuracy arising from a highe
 r-order multistep approach comes at very little additional computational c
 ost. We show that through a careful choice of covariance function for the 
 GP\, the posterior mean and standard deviation over the numerical solution
  can be made to exactly coincide with the value given by the deterministic
  method and its local truncation error respectively. We provide a rigorous
  proof of the convergence of these new methods\, as well as an empirical i
 nvestigation (up to fifth order) demonstrating their convergence rates in 
 practice.
LOCATION:Area 5+6+7+8 #113
END:VEVENT
BEGIN:VEVENT
SUMMARY:More Supervision\, Less Computation: Statistical-Computational Tra
 deoffs in Weakly Supervised Learning | Xinyang Yi \, Zhaoran Wang \, Zhuor
 an Yang \, Constantine Caramanis \, Han Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:More Supervision\, Less Computation: Statistical-Comput
 ational Tradeoffs in Weakly Supervised Learning\nXinyang Yi \, Zhaoran Wan
 g \, Zhuoran Yang \, Constantine Caramanis \, Han Liu\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=6974\n\nWe consider the weakly supervised 
 binary classification problem where the labels are randomly flipped with p
 robability $1-\\alpha$. Although there exist numerous algorithms for this 
 problem\, it remains theoretically unexplored how the statistical accuraci
 es and computational efficiency of these algorithms depend on the degree o
 f supervision\, which is quantified by $\\alpha$. In this paper\, we chara
 cterize the effect of $\\alpha$ by establishing the information-theoretic 
 and computational boundaries\, namely\, the minimax-optimal statistical ac
 curacy that can be achieved by all algorithms\, and polynomial-time algori
 thms under an oracle computational model. For small $\\alpha$\, our result
  shows a gap between these two boundaries\, which represents the computati
 onal price of achieving the information-theoretic boundary due to the lack
  of supervision. Interestingly\, we also show that this gap narrows as $\\
 alpha$ increases. In other words\, having more supervision\, i.e.\, more c
 orrect labels\, not only improves the optimal statistical accuracy as expe
 cted\, but also enhances the computational efficiency for achieving such a
 ccuracy.
LOCATION:Area 5+6+7+8 #114
END:VEVENT
BEGIN:VEVENT
SUMMARY:Mutual information for symmetric rank-one matrix estimation: A pro
 of of the replica formula | jean barbier \, Mohamad Dia \, Nicolas Macris 
 \, Florent Krzakala \, Thibault Lesieur \, Lenka Zdeborová
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Mutual information for symmetric rank-one matrix estima
 tion: A proof of the replica formula\njean barbier \, Mohamad Dia \, Nicol
 as Macris \, Florent Krzakala \, Thibault Lesieur \, Lenka Zdeborová\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=6975\n\nFactorizing low-ra
 nk matrices has many applications in machine learning and statistics. For 
 probabilistic models in the Bayes optimal setting\, a general expression f
 or the mutual information has been proposed using heuristic statistical ph
 ysics computations\, and proven in few specific cases. Here\, we show how 
 to rigorously prove the conjectured formula for the symmetric rank-one cas
 e. This allows to express the minimal mean-square-error and to characteriz
 e the detectability phase transitions in a large set of estimation problem
 s ranging from community detection to sparse PCA. We also show that for a 
 large set of parameters\, an iterative algorithm called approximate messag
 e-passing is Bayes optimal. There exists\, however\, a gap between what cu
 rrently known polynomial algorithms can do and what is expected informatio
 n theoretically. Additionally\, the proof technique has an interest of its
  own and exploits three essential ingredients: the interpolation method in
 troduced in statistical physics by Guerra\, the analysis of the approximat
 e message-passing algorithm and the theory of spatial coupling and thresho
 ld saturation in coding. Our approach is generic and applicable to other o
 pen problems in statistical estimation where heuristic statistical physics
  predictions are available.
LOCATION:Area 5+6+7+8 #115
END:VEVENT
BEGIN:VEVENT
SUMMARY:Coin Betting and Parameter-Free Online Learning | Francesco Orabon
 a \, David Pal
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Coin Betting and Parameter-Free Online Learning\nFrance
 sco Orabona \, David Pal\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=6976\n\nIn the recent years\, a number of parameter-free algorithms hav
 e been developed for online linear optimization over Hilbert spaces and fo
 r learning with expert advice.  These algorithms achieve optimal regret bo
 unds that depend on the unknown competitors\, without having to tune the l
 earning rates with oracle choices.  We present a new intuitive framework t
 o design parameter-free algorithms for both online linear optimization ove
 r Hilbert spaces and for learning with expert advice\, based on reductions
  to betting on outcomes of adversarial coins. We instantiate it using a be
 tting algorithm based on the Krichevsky-Trofimov estimator.  The resulting
  algorithms are simple\, with no parameters to be tuned\, and they improve
  or match previous results in terms of regret guarantee and per-round comp
 lexity.
LOCATION:Area 5+6+7+8 #116
END:VEVENT
BEGIN:VEVENT
SUMMARY:Normalized Spectral Map Synchronization | Yanyao Shen \, Qixing Hu
 ang \, Nati Srebro \, Sujay Sanghavi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Normalized Spectral Map Synchronization\nYanyao Shen \,
  Qixing Huang \, Nati Srebro \, Sujay Sanghavi\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=6977\n\nThe algorithmic advancement of synchroniz
 ing maps is important in order to solve a wide range of practice problems 
  with possible large-scale dataset. In this paper\, we provide theoretical
  justifications for spectral techniques for the map synchronization proble
 m\, i.e.\, it takes as input a collection of objects and noisy maps estima
 ted between pairs of objects\, and outputs clean maps between all pairs of
  objects. We show that a simple normalized spectral method that projects t
 he blocks of the top eigenvectors of a data matrix to the map space leads 
 to surprisingly good results. As the noise is modelled naturally as random
  permutation matrix\, this algorithm NormSpecSync leads to competing theor
 etical guarantees as state-of-the-art convex optimization techniques\, yet
  it is much more efficient. We demonstrate the usefulness of our algorithm
  in a couple of applications\, where it is optimal in both complexity and 
 exactness among existing methods.
LOCATION:Area 5+6+7+8 #117
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Explore-Then-Commit strategies | Aurelien Garivier \, Tor Latti
 more \, Emilie Kaufmann
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:On Explore-Then-Commit strategies\nAurelien Garivier \,
  Tor Lattimore \, Emilie Kaufmann\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=6978\n\nWe study the problem of minimising regret in two-armed
  bandit problems with Gaussian rewards. Our objective is to use this simpl
 e setting to illustrate that strategies based on an exploration phase (up 
 to a stopping time) followed by exploitation are necessarily suboptimal. T
 he results hold regardless of whether or not the difference in means betwe
 en the two arms is known. Besides the main message\, we also refine existi
 ng deviation inequalities\, which allow us to design fully sequential stra
 tegies with finite-time regret guarantees that are (a) asymptotically opti
 mal as the horizon grows and (b) order-optimal in the minimax sense. Furth
 ermore we provide empirical evidence that the theory also holds in practic
 e and discuss extensions to non-gaussian and multiple-armed case.
LOCATION:Area 5+6+7+8 #118
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Kernels with Random Features | Aman Sinha \, John C Duchi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning Kernels with Random Features\nAman Sinha \, Jo
 hn C Duchi\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6979\n\nRan
 domized features provide a computationally efficient way to approximate ke
 rnel machines in machine learning tasks. However\, such methods require a 
 user-defined kernel as input. We extend the randomized-feature approach to
  the task of learning a kernel (via its associated random features). Speci
 fically\, we present an efficient optimization problem that learns a kerne
 l in a supervised manner. We prove the consistency of the estimated kernel
  as well as generalization bounds for the class of estimators induced by t
 he optimized kernel\, and we experimentally evaluate our technique on seve
 ral datasets. Our approach is efficient and highly scalable\, and we attai
 n competitive results with a fraction of the training cost of other techni
 ques.
LOCATION:Area 5+6+7+8 #119
END:VEVENT
BEGIN:VEVENT
SUMMARY:Robustness of classifiers: from adversarial to random noise | Alhu
 ssein Fawzi \, Seyed-Mohsen Moosavi-Dezfooli \, Pascal Frossard
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Robustness of classifiers: from adversarial to random n
 oise\nAlhussein Fawzi \, Seyed-Mohsen Moosavi-Dezfooli \, Pascal Frossard\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6980\n\nSeveral recent
  works have shown that state-of-the-art classifiers are vulnerable to wors
 t-case (i.e.\, adversarial) perturbations of the datapoints. On the other 
 hand\, it has been empirically observed that these same classifiers are re
 latively robust to random noise. In this paper\, we propose to study a sem
 i-random noise regime that generalizes both the random and worst-case nois
 e regimes. We propose the first quantitative analysis of the robustness of
  nonlinear classifiers in this general noise regime. We establish precise 
 theoretical bounds on the robustness of classifiers in this general regime
 \, which depend on the curvature of the classifier's decision boundary. Ou
 r bounds confirm and quantify the empirical observations that classifiers 
 satisfying curvature constraints are robust to random noise. Moreover\, we
  quantify the robustness of classifiers in terms of the subspace dimension
  in the semi-random noise regime\, and show that our bounds remarkably int
 erpolate between the worst-case and random noise regimes. We perform exper
 iments and show that the derived bounds provide very accurate estimates wh
 en applied to various state-of-the-art deep neural networks and datasets. 
 This result suggests bounds on the curvature of the classifiers' decision 
 boundaries that we support experimentally\, and more generally offers impo
 rtant insights onto the geometry of high dimensional classification proble
 ms.
LOCATION:Area 5+6+7+8 #120
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Skills Adaptive Partitions (ASAP) | Daniel J Mankowitz \,
  Timothy A Mann \, Shie Mannor
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Adaptive Skills Adaptive Partitions (ASAP)\nDaniel J Ma
 nkowitz \, Timothy A Mann \, Shie Mannor\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=6981\n\nWe introduce the Adaptive Skills\, Adaptive Par
 titions (ASAP) framework that (1) learns skills (i.e.\, temporally extende
 d actions or options) as well as (2) where to apply them. We believe that 
 both (1) and (2) are necessary for a truly general skill learning framewor
 k\, which is a key building block needed to scale up to lifelong learning 
 agents. The ASAP framework is also able to  solve related new tasks simply
  by adapting where it applies its existing learned skills. We prove that A
 SAP converges to a local optimum under natural conditions. Finally\, our e
 xperimental results\, which include a RoboCup domain\, demonstrate the abi
 lity of ASAP to learn where to reuse skills as well as solve multiple task
 s with considerably less experience than solving each task from scratch.
LOCATION:Area 5+6+7+8 #121
END:VEVENT
BEGIN:VEVENT
SUMMARY:Gaussian Process Bandit Optimisation with Multi-fidelity Evaluatio
 ns | Kirthevasan Kandasamy \, Gautam Dasarathy \, Junier B Oliva \, Jeff S
 chneider \, Barnabas Poczos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Gaussian Process Bandit Optimisation with Multi-fidelit
 y Evaluations\nKirthevasan Kandasamy \, Gautam Dasarathy \, Junier B Oliva
  \, Jeff Schneider \, Barnabas Poczos\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=6982\n\nIn many scientific and engineering applications\, 
 we are tasked with the optimisation of an expensive to evaluate black box 
 function $\\func$. Traditional methods for this problem assume just the av
 ailability of this single function. However\, in many cases\, cheap approx
 imations to $\\func$ may be obtainable. For example\, the expensive real w
 orld behaviour of a robot can be approximated by a cheap computer simulati
 on. We can use these approximations to eliminate low function value region
 s cheaply and use the expensive evaluations of $\\func$ in a small but pro
 mising region and speedily identify the optimum. We formalise this task as
  a \\emph{multi-fidelity} bandit problem where the target function and its
  approximations are sampled from a Gaussian process. We develop \\mfgpucb\
 , a novel method based on upper confidence bound techniques. In our theore
 tical analysis we demonstrate that it exhibits precisely the above behavio
 ur\, and achieves better regret than strategies which ignore multi-fidelit
 y information. \\mfgpucbs outperforms such naive strategies and other mult
 i-fidelity methods  on several synthetic and real experiments.
LOCATION:Area 5+6+7+8 #122
END:VEVENT
BEGIN:VEVENT
SUMMARY:Flexible Models for Microclustering with Application to Entity Res
 olution | Brenda Betancourt \, Giacomo Zanella \, Jeffrey Miller \, Hanna 
 Wallach \, Abbas Zaidi \, Beka Steorts
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Flexible Models for Microclustering with Application to
  Entity Resolution\nBrenda Betancourt \, Giacomo Zanella \, Jeffrey Miller
  \, Hanna Wallach \, Abbas Zaidi \, Beka Steorts\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=6983\n\nMost generative models for clustering i
 mplicitly assume that the number of data points in each cluster grows line
 arly with the total number of data points. Finite mixture models\, Dirichl
 et process mixture models\, and Pitman--Yor process mixture models make th
 is assumption\, as do all other infinitely exchangeable clustering models.
  However\, for some applications\, this assumption is inappropriate. For e
 xample\, when performing entity resolution\, the size of each cluster shou
 ld be unrelated to the size of the data set\, and each cluster should cont
 ain a negligible fraction of the total number of data points. These applic
 ations require models that yield clusters whose sizes grow sublinearly wit
 h the size of the data set. We address this requirement by defining the mi
 croclustering property and introducing a new class of models that can exhi
 bit this property. We compare models within this class to two commonly use
 d clustering models using four entity-resolution data sets.
LOCATION:Area 5+6+7+8 #123
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo | 
 Alain Durmus \, Umut Simsekli \, Eric Moulines \, Roland Badeau \, Gaël R
 ICHARD
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Stochastic Gradient Richardson-Romberg Markov Chain Mon
 te Carlo\nAlain Durmus \, Umut Simsekli \, Eric Moulines \, Roland Badeau 
 \, Gaël RICHARD\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6984\
 n\nStochastic Gradient Markov Chain Monte Carlo (SG-MCMC) algorithms have 
 become increasingly popular for Bayesian inference in large-scale applicat
 ions. Even though these methods have proved useful in several scenarios\, 
 their performance is often limited by their bias. In this study\, we propo
 se a novel sampling algorithm that aims to reduce the bias of SG-MCMC whil
 e keeping the variance at a reasonable level. Our approach is based on a n
 umerical sequence acceleration method\, namely the Richardson-Romberg extr
 apolation\, which simply boils down   to running almost the same SG-MCMC a
 lgorithm twice in parallel with different step sizes. We illustrate our fr
 amework on the popular Stochastic Gradient Langevin Dynamics (SGLD) algori
 thm and propose a novel SG-MCMC algorithm referred to as Stochastic Gradie
 nt Richardson-Romberg Langevin Dynamics (SGRRLD). We provide formal theore
 tical analysis and show that SGRRLD is asymptotically consistent\, satisfi
 es a central limit theorem\, and its non-asymptotic bias and the mean squa
 red-error can be bounded. Our results show that SGRRLD attains higher rate
 s of convergence than SGLD in both finite-time and asymptotically\, and it
  achieves the theoretical   accuracy of the methods that are based on high
 er-order integrators. We support our findings using both synthetic and rea
 l data experiments.
LOCATION:Area 5+6+7+8 #124
END:VEVENT
BEGIN:VEVENT
SUMMARY:Online and Differentially-Private Tensor Decomposition | Yining Wa
 ng \, Anima Anandkumar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Online and Differentially-Private Tensor Decomposition\
 nYining Wang \, Anima Anandkumar\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=6985\n\nTensor decomposition is positioned to be a pervasive to
 ol in the era of big data. In this paper\, we resolve many of the key algo
 rithmic questions regarding robustness\, memory efficiency\, and different
 ial privacy of tensor decomposition. We propose simple variants of the ten
 sor power method which enjoy these strong properties. We propose the first
  streaming method with a linear memory requirement. Moreover\, we present 
 a noise calibrated tensor power method with efficient privacy guarantees. 
 At the heart of all these guarantees lies a careful perturbation analysis 
 derived in this paper which improves up on the existing results significan
 tly.
LOCATION:Area 5+6+7+8 #125
END:VEVENT
BEGIN:VEVENT
SUMMARY:Maximal Sparsity with Deep Networks? | Bo Xin \, Yizhou Wang \, We
 n Gao \, Baoyuan Wang \, David Wipf
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Maximal Sparsity with Deep Networks?\nBo Xin \, Yizhou 
 Wang \, Wen Gao \, Baoyuan Wang \, David Wipf\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=6986\n\nThe iterations of many sparse estimation a
 lgorithms are comprised of a fixed linear filter cascaded with a threshold
 ing nonlinearity\, which collectively resemble a typical neural network la
 yer.  Consequently\, a lengthy sequence of algorithm iterations can be vie
 wed as a deep network with shared\, hand-crafted layer weights.  It is the
 refore quite natural to examine the degree to which a learned network mode
 l might act as a viable surrogate for traditional sparse estimation in dom
 ains where ample training data is available.  While the possibility of a r
 educed computational budget is readily apparent when a ceiling is imposed 
 on the number of layers\, our work primarily focuses on estimation accurac
 y.  In particular\, it is well-known that when a signal dictionary has coh
 erent columns\, as quantified by a large RIP constant\, then most tractabl
 e iterative algorithms are unable to find maximally sparse representations
 .  In contrast\, we demonstrate both theoretically and empirically the pot
 ential for a trained deep network to recover minimal $\\ell_0$-norm repres
 entations in regimes where existing methods fail.  The resulting system\, 
 which can effectively learn novel iterative sparse estimation algorithms\,
  is deployed on a practical photometric stereo estimation problem\, where 
 the goal is to remove sparse outliers that can disrupt the estimation of s
 urface normals from a 3D scene.
LOCATION:Area 5+6+7+8 #126
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient High-Order Interaction-Aware Feature Selection Based on 
 Conditional Mutual Information | Alexander Shishkin \, Anastasia Bezzubtse
 va \, Alexey Drutsa \, Ilia Shishkov \, Ekaterina Gladkikh \, Gleb Gusev \
 , Pavel Serdyukov
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Efficient High-Order Interaction-Aware Feature Selectio
 n Based on Conditional Mutual Information\nAlexander Shishkin \, Anastasia
  Bezzubtseva \, Alexey Drutsa \, Ilia Shishkov \, Ekaterina Gladkikh \, Gl
 eb Gusev \, Pavel Serdyukov\nhttp://nips.cc/Conferences/2016/Schedule?show
 Event=6987\n\nThis study introduces a novel feature selection approach CMI
 COT\, which is a further evolution of filter methods with sequential  forw
 ard selection (SFS) whose scoring functions are based on conditional mutua
 l information (MI). We state and study a novel saddle point (max-min) opti
 mization problem to build a scoring function that is able to identify join
 t interactions between several  features. This method fills the gap of MI-
 based SFS techniques with high-order dependencies. In this high-dimensiona
 l case\, the estimation of MI has prohibitively high sample complexity. We
  mitigate this cost using a greedy approximation and binary representative
 s what makes our technique able to be effectively used. The superiority of
  our approach is demonstrated by comparison with recently proposed interac
 tion-aware filters and several interaction-agnostic state-of-the-art ones 
 on ten publicly available benchmark datasets.
LOCATION:Area 5+6+7+8 #127
END:VEVENT
BEGIN:VEVENT
SUMMARY:Geometric Dirichlet Means Algorithm for topic inference | Mikhail 
 Yurochkin \, XuanLong Nguyen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Geometric Dirichlet Means Algorithm for topic inference
 \nMikhail Yurochkin \, XuanLong Nguyen\nhttp://nips.cc/Conferences/2016/Sc
 hedule?showEvent=6988\n\nWe propose a geometric algorithm for topic learni
 ng and inference that is built on the convex geometry of topics arising fr
 om the Latent Dirichlet Allocation (LDA) model and its nonparametric exten
 sions. To this end we study the optimization of a geometric loss function\
 , which is a surrogate to the LDA's likelihood. Our method involves a fast
  optimization based weighted clustering procedure augmented with geometric
  corrections\, which overcomes the computational and statistical inefficie
 ncies encountered by other techniques based on Gibbs sampling and variatio
 nal inference\, while achieving the accuracy comparable to that of a Gibbs
  sampler. The topic estimates produced by our method are shown to be stati
 stically consistent under some conditions. The algorithm is evaluated with
  extensive experiments on simulated and real data.
LOCATION:Area 5+6+7+8 #128
END:VEVENT
BEGIN:VEVENT
SUMMARY:Interaction Screening: Efficient and Sample-Optimal Learning of Is
 ing Models | Marc Vuffray \, Sidhant Misra \, Andrey Lokhov \, Michael Che
 rtkov
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Interaction Screening: Efficient and Sample-Optimal Lea
 rning of Ising Models\nMarc Vuffray \, Sidhant Misra \, Andrey Lokhov \, M
 ichael Chertkov\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6989\n
 \nWe consider the problem of learning the underlying graph of an unknown I
 sing model on p spins from a collection of i.i.d. samples generated from t
 he model. We suggest a new estimator that is computationally efficient and
  requires a number of samples that is near-optimal with respect to previou
 sly established information theoretic lower-bound. Our statistical estimat
 or has a physical interpretation in terms of "interaction screening". The 
 estimator is consistent and is efficiently implemented using convex optimi
 zation. We prove that with appropriate regularization\, the estimator reco
 vers the underlying graph using a number of samples that is logarithmic in
  the system size p and exponential in the maximum coupling-intensity and m
 aximum node-degree.
LOCATION:Area 5+6+7+8 #129
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multi-armed Bandits: Competing with Optimal Sequences | Zohar Karn
 in \, Oren Anava
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Multi-armed Bandits: Competing with Optimal Sequences\n
 Zohar Karnin \, Oren Anava\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=6990\n\nWe consider sequential decision making problem in the adversa
 rial setting\, where regret is measured with respect to the optimal sequen
 ce of actions and the feedback adheres the bandit setting. It is well-know
 n that obtaining sublinear regret in this setting is impossible in general
 \, which arises the question of when can we do better than linear regret? 
 Previous works show that when the environment is guaranteed to vary slowly
  and furthermore we are given prior knowledge regarding its variation (i.e
 .\, a limit on the amount of changes suffered by the environment)\, then t
 his task is feasible. The caveat however is that such prior knowledge is n
 ot likely to be available in practice\, which causes the obtained regret b
 ounds to be somewhat irrelevant.   Our main result is a regret guarantee t
 hat scales with the variation parameter of the environment\, without requi
 ring any prior knowledge about it whatsoever. By that\, we also resolve an
  open problem posted by [Gur\, Zeevi and Besbes\, NIPS' 14]. An important 
 key component in our result is a statistical test for identifying non-stat
 ionarity in a sequence of independent random variables. This test either i
 dentifies non-stationarity or upper-bounds the absolute deviation of the c
 orresponding sequence of mean values in terms of its total variation. This
  test is interesting on its own right and has the potential to be found us
 eful in additional settings.
LOCATION:Area 5+6+7+8 #130
END:VEVENT
BEGIN:VEVENT
SUMMARY:Catching heuristics are optimal control policies | Boris Belousov 
 \, Gerhard Neumann \, Constantin A Rothkopf \, Jan R Peters
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Catching heuristics are optimal control policies\nBoris
  Belousov \, Gerhard Neumann \, Constantin A Rothkopf \, Jan R Peters\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=6991\n\nTwo seemingly cont
 radictory theories attempt to explain how humans move to intercept an airb
 orne ball. One theory posits that humans predict the ball trajectory to op
 timally plan future actions\; the other claims that\, instead of performin
 g such complicated computations\, humans employ heuristics to reactively c
 hoose appropriate actions based on immediate visual feedback. In this pape
 r\, we show that interception strategies appearing to be heuristics can be
  understood as computational solutions to the optimal control problem face
 d by a ball-catching agent acting under uncertainty. Modeling catching as 
 a continuous partially observable Markov decision process and employing st
 ochastic optimal control theory\, we discover that the four main heuristic
 s described in the literature are optimal solutions if the catcher has suf
 ficient time to continuously visually track the ball. Specifically\, by va
 rying model parameters such as noise\, time to ground contact\, and percep
 tual latency\, we show that different strategies arise under different cir
 cumstances. The catcher's policy switches between generating reactive and 
 predictive behavior based on the ratio of system to observation noise and 
 the ratio between reaction time and task duration. Thus\, we provide a rat
 ional account of human ball-catching behavior and a unifying explanation f
 or seemingly contradictory theories of target interception on the basis of
  stochastic optimal control.
LOCATION:Area 5+6+7+8 #131
END:VEVENT
BEGIN:VEVENT
SUMMARY:Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifo
 lds | Hongyi Zhang \, Sashank J. Reddi \, Suvrit Sra
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Riemannian SVRG: Fast Stochastic Optimization on Rieman
 nian Manifolds\nHongyi Zhang \, Sashank J. Reddi \, Suvrit Sra\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=6992\n\nWe study optimization of 
 finite sums of \\emph{geodesically} smooth functions on Riemannian manifol
 ds. Although variance reduction techniques for optimizing finite-sums have
  witnessed tremendous attention in the recent years\, existing work is lim
 ited to vector space problems. We introduce \\emph{Riemannian SVRG} (\\rsv
 rg)\, a new variance reduced Riemannian optimization method. We analyze \\
 rsvrg for both geodesically  \\emph{convex} and \\emph{nonconvex} (smooth)
  functions. Our analysis reveals that \\rsvrg inherits  advantages of the 
 usual SVRG method\, but with factors depending on curvature of the manifol
 d that influence its convergence. To our knowledge\, \\rsvrg is the first 
 \\emph{provably fast} stochastic Riemannian method. Moreover\, our paper p
 resents the first non-asymptotic complexity analysis (novel even for the b
 atch setting) for nonconvex Riemannian optimization. Our results have seve
 ral implications\; for instance\, they offer a Riemannian perspective on v
 ariance reduced PCA\, which promises a short\, transparent convergence ana
 lysis.
LOCATION:Area 5+6+7+8 #132
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Comprehensive Linear Speedup Analysis for Asynchronous Stochasti
 c Parallel Optimization from Zeroth-Order to First-Order | Xiangru Lian \,
  Huan Zhang \, Cho-Jui Hsieh \, Yijun Huang \, Ji Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Comprehensive Linear Speedup Analysis for Asynchronou
 s Stochastic Parallel Optimization from Zeroth-Order to First-Order\nXiang
 ru Lian \, Huan Zhang \, Cho-Jui Hsieh \, Yijun Huang \, Ji Liu\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=6993\n\nAsynchronous parallel op
 timization received substantial successes and extensive attention recently
 . One of core theoretical questions is how much speedup (or benefit) the a
 synchronous parallelization can bring to us. This paper provides a compreh
 ensive and generic analysis to study the speedup property for a broad rang
 e of asynchronous parallel stochastic algorithms from the zeroth order to 
 the first order methods. Our result recovers or improves existing analysis
  on special cases\, provides more insights for understanding the asynchron
 ous parallel behaviors\, and suggests a novel asynchronous parallel zeroth
  order method for the first time. Our experiments provide novel applicatio
 ns of the proposed asynchronous parallel zeroth order method on hyper para
 meter tuning and model blending problems.
LOCATION:Area 5+6+7+8 #133
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Gradient MCMC with Stale Gradients | Changyou Chen \, N
 an Ding \, Chunyuan Li \, Yizhe Zhang \, Lawrence Carin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Stochastic Gradient MCMC with Stale Gradients\nChangyou
  Chen \, Nan Ding \, Chunyuan Li \, Yizhe Zhang \, Lawrence Carin\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=6994\n\nStochastic gradient MC
 MC (SG-MCMC) has played an important role in large-scale Bayesian learning
 \, with well-developed theoretical convergence properties. In such applica
 tions of SG-MCMC\, it is becoming increasingly popular to employ distribut
 ed systems\, where stochastic gradients are computed based on some outdate
 d parameters\, yielding what are termed stale gradients. While stale gradi
 ents could be directly used in SG-MCMC\, their impact on convergence prope
 rties has not been well studied. In this paper we develop theory to show t
 hat while the bias and MSE of an SG-MCMC algorithm depend on the staleness
  of stochastic gradients\, its estimation variance (relative to the expect
 ed estimate\, based on a prescribed number of samples) is independent of i
 t. In a simple Bayesian distributed system with SG-MCMC\, where stale grad
 ients are computed asynchronously by a set of workers\, our theory indicat
 es a linear speedup on the decrease of estimation variance w.r.t. the numb
 er of workers. Experiments on synthetic data and deep neural networks vali
 date our theory\, demonstrating the effectiveness and scalability of SG-MC
 MC with stale gradients.
LOCATION:Area 5+6+7+8 #134
END:VEVENT
BEGIN:VEVENT
SUMMARY:Disentangling factors of variation in deep representation using ad
 versarial training | Michael F Mathieu \, Junbo Zhao \, Junbo Jake Zhao \,
  Aditya Ramesh \, Pablo Sprechmann \, Yann LeCun
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Disentangling factors of variation in deep representati
 on using adversarial training\nMichael F Mathieu \, Junbo Zhao \, Junbo Ja
 ke Zhao \, Aditya Ramesh \, Pablo Sprechmann \, Yann LeCun\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=6995\n\nWe propose a deep generative 
 model for learning to distill the hidden factors of variation within a set
  of labeled observations into two complementary codes. One code describes 
 the factors of variation relevant to solving a specified task. The other c
 ode describes the remaining factors of variation that are irrelevant to so
 lving this task. The only available source of supervision during the train
 ing process comes from our ability to distinguish among different observat
 ions belonging to the same category. Concrete examples include multiple im
 ages of the same object from different viewpoints\, or multiple speech sam
 ples from the same speaker. In both of these instances\, the factors of va
 riation irrelevant to classification are implicitly expressed by intra-cla
 ss variabilities\, such as the relative position of an object in an image\
 , or the linguistic content of an utterance. Most existing approaches for 
 solving this problem rely heavily on having access to pairs of observation
 s only sharing a single factor of variation\, e.g. different objects obser
 ved in the exact same conditions. This assumption is often not encountered
  in realistic settings where data acquisition is not controlled and labels
  for the uninformative components are not available. In this work\, we pro
 pose to overcome this limitation by augmenting deep convolutional autoenco
 ders with a form of adversarial training. Both factors of variation are im
 plicitly captured in the organization of the learned embedding space\, and
  can be used for solving single-image analogies.  Experimental results on 
 synthetic and real datasets show that the proposed method is capable of di
 sentangling the influences of style and content factors using a flexible r
 epresentation\, as well as generalizing to unseen styles or content classe
 s.
LOCATION:Area 5+6+7+8 #135
END:VEVENT
BEGIN:VEVENT
SUMMARY:Consistent Kernel Mean Estimation for Functions of Random Variable
 s | Adam Scibior \, Carl-Johann Simon-Gabriel \, Ilya Tolstikhin \, Prof. 
 Bernhard Schölkopf
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Consistent Kernel Mean Estimation for Functions of Rand
 om Variables\nAdam Scibior \, Carl-Johann Simon-Gabriel \, Ilya Tolstikhin
  \, Prof. Bernhard Schölkopf\nhttp://nips.cc/Conferences/2016/Schedule?sh
 owEvent=6996\n\nWe provide a theoretical foundation for non-parametric est
 imation of functions of random variables using kernel mean embeddings. We 
 show that for any continuous function f\, consistent estimators of the mea
 n embedding of a random variable X lead to consistent estimators of the me
 an embedding of f(X). For Matern kernels and sufficiently smooth functions
  we also provide rates of convergence.  Our results extend to functions of
  multiple random variables. If the variables are dependent\, we require an
  estimator of the mean embedding of their joint distribution as a starting
  point\; if they are independent\, it is sufficient to have separate estim
 ators of the mean embeddings of their marginal distributions. In either ca
 se\, our results cover both mean embeddings based on i.i.d. samples as wel
 l as "reduced set" expansions in terms of dependent expansion points. The 
 latter serves as a justification for using such expansions to limit memory
  resources when applying the approach as a basis for probabilistic program
 ming.
LOCATION:Area 5+6+7+8 #136
END:VEVENT
BEGIN:VEVENT
SUMMARY:DECOrrelated feature space partitioning for distributed sparse reg
 ression | Xiangyu Wang \, David B Dunson \, Chenlei Leng
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:DECOrrelated feature space partitioning for distributed
  sparse regression\nXiangyu Wang \, David B Dunson \, Chenlei Leng\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=6997\n\nFitting statistical m
 odels is computationally challenging when the sample size or the dimension
  of the dataset is huge. An attractive approach for down-scaling the probl
 em size is to first partition the dataset into subsets and then fit using 
 distributed algorithms. The dataset can be partitioned either horizontally
  (in the sample space) or vertically (in the feature space). While the maj
 ority of the literature focuses on sample space partitioning\, feature spa
 ce partitioning is more effective when p &gt\;&gt\; n. Existing methods fo
 r partitioning features\, however\, are either vulnerable to high correlat
 ions or inefficient in reducing the model dimension. In this paper\, we so
 lve these problems through a new embarrassingly parallel framework named D
 ECO for distributed variable selection and parameter estimation. In DECO\,
  variables are first partitioned and allocated to m distributed workers. T
 he decorrelated subset data within each worker are then fitted via any alg
 orithm designed for high-dimensional problems. We show that by incorporati
 ng the decorrelation step\, DECO can achieve consistent variable selection
  and parameter estimation on each subset with (almost) no assumptions. In 
 addition\, the convergence rate is nearly minimax optimal for both sparse 
 and weakly sparse models and does NOT depend on the partition number m. Ex
 tensive numerical experiments are provided to illustrate the performance o
 f the new framework.
LOCATION:Area 5+6+7+8 #137
END:VEVENT
BEGIN:VEVENT
SUMMARY:Coupled Generative Adversarial Networks | Ming-Yu Liu \, Oncel Tuz
 el
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Coupled Generative Adversarial Networks\nMing-Yu Liu \,
  Oncel Tuzel\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6998\n\nW
 e propose the coupled generative adversarial nets (CoGAN) framework for ge
 nerating pairs of corresponding images in two different domains. The frame
 work consists of a pair of generative adversarial nets\, each responsible 
 for generating images in one domain. We show that by enforcing a simple we
 ight-sharing constraint\, the CoGAN learns to generate pairs of correspond
 ing images without existence of any pairs of corresponding images in the t
 wo domains in the training set. In other words\, the CoGAN learns a joint 
 distribution of images in the two domains from images drawn separately fro
 m the marginal distributions of the individual domains. This is in contras
 t to the existing multi-modal generative models\, which require correspond
 ing images for training. We apply the CoGAN to several pair image generati
 on tasks. For each task\, the CoGAN learns to generate convincing pairs of
  corresponding images. We further demonstrate the applications of the CoGA
 N framework for the domain adaptation and cross-domain image generation ta
 sks.
LOCATION:Area 5+6+7+8 #138
END:VEVENT
BEGIN:VEVENT
SUMMARY:Matching Networks for One Shot Learning | Oriol Vinyals \, Charles
  Blundell \, Tim Lillicrap \, koray kavukcuoglu \, Daan Wierstra
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Matching Networks for One Shot Learning\nOriol Vinyals 
 \, Charles Blundell \, Tim Lillicrap \, koray kavukcuoglu \, Daan Wierstra
 \nhttp://nips.cc/Conferences/2016/Schedule?showEvent=6999\n\nLearning from
  a few examples remains a key challenge in machine learning. Despite recen
 t advances in important domains such as vision and language\, the standard
  supervised deep learning paradigm does not offer a satisfactory solution 
 for learning new concepts rapidly from little data. In this work\, we empl
 oy ideas from metric learning based on deep neural features and from recen
 t advances that augment neural networks with external memories. Our framew
 ork learns a network that maps a small labelled support set and an unlabel
 led example to its label\, obviating the need for fine-tuning to adapt to 
 new class types. We then define one-shot learning problems on vision (usin
 g Omniglot\, ImageNet) and language tasks. Our algorithm improves one-shot
  accuracy on ImageNet from 82.2% to 87.8% and from 88% accuracy to 95% acc
 uracy on Omniglot compared to competing approaches. We also demonstrate th
 e usefulness of the same model on language modeling by introducing a one-s
 hot task on the Penn Treebank.
LOCATION:Area 5+6+7+8 #139
END:VEVENT
BEGIN:VEVENT
SUMMARY:Distributed Flexible Nonlinear Tensor Factorization | Shandian Zhe
  \, Kai Zhang \, Pengyuan Wang \, Kuang-chih Lee \, Zenglin Xu \, Yuan Qi 
 \, Zoubin Ghahramani
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Distributed Flexible Nonlinear Tensor Factorization\nSh
 andian Zhe \, Kai Zhang \, Pengyuan Wang \, Kuang-chih Lee \, Zenglin Xu \
 , Yuan Qi \, Zoubin Ghahramani\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=7000\n\nTensor factorization is a powerful tool to analyse multi-
 way data. Recently proposed nonlinear factorization methods\, although cap
 able of capturing complex relationships\, are computationally quite expens
 ive and may suffer a severe learning bias in case of extreme data sparsity
 . Therefore\, we propose a distributed\, flexible nonlinear tensor factori
 zation model\, which avoids the expensive computations and structural rest
 rictions of the Kronecker-product in the existing TGP formulations\, allow
 ing an arbitrary subset of tensor entries to be selected for training. Mea
 nwhile\, we derive a tractable and tight variational evidence lower bound 
 (ELBO) that enables highly decoupled\, parallel computations and high-qual
 ity inference. Based on the new bound\, we develop a distributed\, key-val
 ue-free inference algorithm in the MapReduce framework\, which can fully e
 xploit the memory cache mechanism in fast MapReduce systems such as Spark.
  Experiments demonstrate the advantages of our method over several state-o
 f-the-art approaches\, in terms of both predictive performance and computa
 tional efficiency.
LOCATION:Area 5+6+7+8 #140
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tracking the Best Expert in Non-stationary Stochastic Environments
  | Chen-Yu Wei \, Yi-Te Hong \, Chi-Jen Lu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Tracking the Best Expert in Non-stationary Stochastic E
 nvironments\nChen-Yu Wei \, Yi-Te Hong \, Chi-Jen Lu\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7001\n\nWe study the dynamic regret of mult
 i-armed bandit and experts problem in non-stationary stochastic environmen
 ts. We introduce a new parameter $\\W$\, which measures the total statisti
 cal variance of the loss distributions over $T$ rounds of the process\, an
 d study how this amount affects the regret. We investigate the interaction
  between $\\W$ and $\\Gamma$\, which counts the number of times the distri
 butions change\, as well as $\\W$ and $V$\, which measures how far the dis
 tributions deviates over time. One striking result we find is that even wh
 en $\\Gamma$\, $V$\, and $\\Lambda$ are all restricted to constant\, the r
 egret lower bound in the bandit setting still grows with $T$. The other hi
 ghlight is that in the full-information setting\, a constant regret become
 s achievable with constant $\\Gamma$ and $\\Lambda$\, as it can be made in
 dependent of $T$\, while with constant $V$ and $\\Lambda$\, the regret sti
 ll has a $T^{1/3}$ dependency. We not only propose algorithms with upper b
 ound guarantee\, but prove their matching lower bounds as well.
LOCATION:Area 5+6+7+8 #141
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Alternative Neural Network: Exploring Contexts as Early as Po
 ssible for Action Recognition | Jinzhuo Wang \, Wenmin Wang \, xiongtao Ch
 en \, Ronggang Wang \, Wen Gao
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Deep Alternative Neural Network: Exploring Contexts as 
 Early as Possible for Action Recognition\nJinzhuo Wang \, Wenmin Wang \, x
 iongtao Chen \, Ronggang Wang \, Wen Gao\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7002\n\nContexts are crucial for action recognition in 
 video. Current methods often mine contexts after extracting hierarchical l
 ocal features and focus on their high-order encodings. This paper instead 
 explores contexts as early as possible and leverages their evolutions for 
 action recognition. In particular\, we introduce a novel architecture call
 ed deep alternative neural network (DANN) stacking alternative layers. Eac
 h alternative layer consists of a volumetric convolutional layer followed 
 by a recurrent layer. The former acts as local feature learner while the l
 atter is used to collect contexts. Compared with feed-forward neural netwo
 rks\, DANN learns contexts of local features from the very beginning. This
  setting helps to preserve hierarchical context evolutions which we show a
 re essential to recognize similar actions. Besides\, we present an adaptiv
 e method to determine the temporal size for network input based on optical
  flow energy\, and develop a volumetric pyramid pooling layer to deal with
  input clips of arbitrary sizes. We demonstrate the advantages of DANN on 
 two benchmarks HMDB51 and UCF101 and report competitive or superior result
 s to the state-of-the-art.
LOCATION:Area 5+6+7+8 #142
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Parametric Sparse Models for Image Super-Resolution | Yon
 gbo Li \, Weisheng Dong \, Xuemei Xie \, GUANGMING Shi \, Xin Li \, Dongla
 i Xu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning Parametric Sparse Models for Image Super-Resol
 ution\nYongbo Li \, Weisheng Dong \, Xuemei Xie \, GUANGMING Shi \, Xin Li
  \, Donglai Xu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7003\n\
 nLearning accurate prior knowledge of natural images is of great importanc
 e for single image super-resolution (SR). Existing SR methods either learn
  the prior from the low/high-resolution patch pairs or estimate the prior 
 models from the input low-resolution (LR) image. Specifically\, high-frequ
 ency details are learned in the former methods. Though effective\, they ar
 e heuristic and have limitations in dealing with blurred LR images\; while
  the latter suffers from the limitations of frequency aliasing. In this pa
 per\, we propose to combine those two lines of ideas for image super-resol
 ution. More specifically\, the parametric sparse prior of the desirable hi
 gh-resolution (HR) image patches are learned from both the input low-resol
 ution (LR) image and a training image dataset. With the learned sparse pri
 ors\, the sparse codes and thus the HR image patches can be accurately rec
 overed by solving a sparse coding problem. Experimental results show that 
 the proposed SR method outperforms existing state-of-the-art methods in te
 rms of both subjective and objective image qualities.
LOCATION:Area 5+6+7+8 #143
END:VEVENT
BEGIN:VEVENT
SUMMARY:Kernel Observers: Systems-Theoretic Modeling and Inference of Spat
 iotemporally Evolving Processes | Hassan A Kingravi \, Harshal R Maske \, 
 Girish Chowdhary
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Kernel Observers: Systems-Theoretic Modeling and Infere
 nce of Spatiotemporally Evolving Processes\nHassan A Kingravi \, Harshal R
  Maske \, Girish Chowdhary\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7004\n\nWe consider the problem of estimating the latent state of a s
 patiotemporally evolving continuous function using very few sensor measure
 ments. We show that layering a dynamical systems prior over temporal evolu
 tion of weights of a kernel model is a valid approach to spatiotemporal mo
 deling that does not necessarily require the design of complex nonstationa
 ry kernels. Furthermore\, we show that such a predictive model can be util
 ized to determine sensing locations that guarantee that the hidden state o
 f the phenomena can be recovered with very few measurements. We provide su
 fficient conditions on the number and spatial location of samples required
  to guarantee state recovery\, and provide a lower bound on the minimum nu
 mber of samples required to robustly infer the hidden states. Our approach
  outperforms existing methods in numerical experiments.
LOCATION:Area 5+6+7+8 #144
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning brain regions via large-scale online structured sparse di
 ctionary learning | Elvis DOHMATOB \, Arthur Mensch \, Gael Varoquaux \, B
 ertrand Thirion
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning brain regions via large-scale online structure
 d sparse dictionary learning\nElvis DOHMATOB \, Arthur Mensch \, Gael Varo
 quaux \, Bertrand Thirion\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=7005\n\nWe propose a multivariate online dictionary-learning method fo
 r obtaining decompositions of brain images with structured and sparse comp
 onents (aka atoms). Sparsity is to be understood in the usual sense: the d
 ictionary atoms are constrained to contain mostly zeros. This is imposed v
 ia an $\\ell_1$-norm constraint. By "structured"\, we mean that the atoms 
 are piece-wise smooth and compact\, thus making up blobs\, as opposed to s
 cattered patterns of activation. We propose to use a Sobolev (Laplacian) p
 enalty to impose this type of structure. Combining the two penalties\, we 
 obtain decompositions that properly delineate brain structures from functi
 onal images. This non-trivially extends the online dictionary-learning  wo
 rk of Mairal et al. (2010)\, at the price of only a factor of 2 or 3 on th
 e overall running time. Just like the Mairal et al. (2010) reference metho
 d\, the online nature of our proposed algorithm allows it to scale to arbi
 trarily sized datasets. Experiments on brain data show that our proposed m
 ethod extracts structured and denoised dictionaries that are more intepret
 able and better capture inter-subject variability in small medium\, and la
 rge-scale regimes alike\, compared to state-of-the-art models.
LOCATION:Area 5+6+7+8 #145
END:VEVENT
BEGIN:VEVENT
SUMMARY:Scaling Factorial Hidden Markov Models: Stochastic Variational Inf
 erence without Messages | Yin Cheng Ng \, Pawel M Chilinski \, Ricardo Sil
 va
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Scaling Factorial Hidden Markov Models: Stochastic Vari
 ational Inference without Messages\nYin Cheng Ng \, Pawel M Chilinski \, R
 icardo Silva\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7006\n\nF
 actorial Hidden Markov Models (FHMMs) are powerful models for sequential d
 ata but they do not scale well with long sequences. We propose a scalable 
 inference and learning algorithm for FHMMs that draws on ideas from the st
 ochastic variational inference\, neural network and copula literatures. Un
 like existing approaches\, the proposed algorithm requires no message pass
 ing procedure among latent variables and can be distributed to a network o
 f computers to speed up learning. Our experiments corroborate that the pro
 posed algorithm does not introduce further approximation bias compared to 
 the proven structured mean-field algorithm\, and achieves better performan
 ce with long sequences and large FHMMs.
LOCATION:Area 5+6+7+8 #146
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Bandit Framework for Strategic Regression | Yang Liu \, Yiling C
 hen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:A Bandit Framework for Strategic Regression\nYang Liu \
 , Yiling Chen\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7007\n\n
 We consider a learner's problem of acquiring data dynamically for training
  a regression model\, where the training data are collected from strategic
  data sources. A fundamental challenge is to incentivize data holders to e
 xert effort to improve the quality of their reported data\, despite that t
 he quality is not directly verifiable by the learner. In this work\, we st
 udy a dynamic data acquisition process where data holders can contribute m
 ultiple times. Using a bandit framework\, we leverage on the long-term inc
 entive of future job opportunities to incentivize high-quality contributio
 ns. We propose a Strategic Regression-Upper Confidence Bound (SR-UCB) fram
 ework\, an UCB-style index combined with a simple payment rule\, where the
  index of a worker approximates the quality of his past contributions and 
 is used by the learner to determine whether the worker receives future wor
 k. For linear regression and certain family of non-linear regression probl
 ems\, we show that SR-UCB enables a $O(\\sqrt{\\log T/T})$-Bayesian Nash E
 quilibrium (BNE) where each worker exerting a target effort level that the
  learner has chosen\, with $T$ being the number of data acquisition stages
 . The SR-UCB framework also has some other desirable properties: (1) The i
 ndexes can be updated in an online fashion (hence computationally light). 
 (2) A slight variant\, namely Private SR-UCB (PSR-UCB)\, is able to preser
 ve $(O(\\log^{-1} T)\, O(\\log^{-1} T))$-differential privacy for workers'
  data\, with only a small compromise on incentives (achieving $O(\\log^{6}
  T/\\sqrt{T})$-BNE).
LOCATION:Area 5+6+7+8 #147
END:VEVENT
BEGIN:VEVENT
SUMMARY:Convolutional Neural Networks on Graphs with Fast Localized Spectr
 al Filtering | Michaël Defferrard \, Xavier Bresson \, Pierre Vandergheyn
 st
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Convolutional Neural Networks on Graphs with Fast Local
 ized Spectral Filtering\nMichaël Defferrard \, Xavier Bresson \, Pierre V
 andergheynst\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7008\n\nI
 n this work\, we are interested in generalizing convolutional neural netwo
 rks (CNNs) from low-dimensional regular grids\, where image\, video and sp
 eech are represented\, to high-dimensional irregular domains\, such as soc
 ial networks\, brain connectomes or words’ embedding\, represented by gr
 aphs. We present a formulation of CNNs in the context of spectral graph th
 eory\, which provides the necessary mathematical background and efficient 
 numerical schemes to design fast localized convolutional filters on graphs
 . Importantly\, the proposed technique offers the same linear computationa
 l complexity and constant learning complexity as classical CNNs\, while be
 ing universal to any graph structure. Experiments on MNIST and 20NEWS demo
 nstrate the ability of this novel deep learning system to learn local\, st
 ationary\, and compositional features on graphs.
LOCATION:Area 5+6+7+8 #148
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stein Variational Gradient Descent: A General Purpose Bayesian Inf
 erence Algorithm | Qiang Liu \, Dilin Wang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Stein Variational Gradient Descent: A General Purpose B
 ayesian Inference Algorithm\nQiang Liu \, Dilin Wang\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7009\n\nWe propose a general purpose variat
 ional inference algorithm that forms a natural counterpart of gradient des
 cent for optimization. Our method iteratively transports a set of particle
 s to match the target distribution\, by applying a form of functional grad
 ient descent that minimizes the KL divergence. Empirical studies are perfo
 rmed on various real world models and datasets\, on which our method is co
 mpetitive with existing state-of-the-art methods. The derivation of our me
 thod is based on a new theoretical result that connects the derivative of 
 KL divergence under smooth transforms with Stein’s identity and a recent
 ly proposed kernelized Stein discrepancy\, which is of independent interes
 t.
LOCATION:Area 5+6+7+8 #149
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Learning Models of the Retinal Response to Natural Scenes | L
 ane McIntosh \, Niru Maheswaranathan \, Aran Nayebi \, Surya Ganguli \, St
 ephen Baccus
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Deep Learning Models of the Retinal Response to Natural
  Scenes\nLane McIntosh \, Niru Maheswaranathan \, Aran Nayebi \, Surya Gan
 guli \, Stephen Baccus\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7010\n\nA central challenge in sensory neuroscience is to understand neur
 al computations and circuit mechanisms that underlie the encoding of ethol
 ogically relevant\, natural stimuli. In multilayered neural circuits\, non
 linear processes such as synaptic transmission and spiking dynamics presen
 t a significant obstacle to the creation of accurate computational models 
 of responses to natural stimuli. Here we demonstrate that deep convolution
 al neural networks (CNNs) capture retinal responses to natural scenes near
 ly to within the variability of a cell's response\, and are markedly more 
 accurate than linear-nonlinear (LN) models and Generalized Linear Models (
 GLMs). Moreover\, we find two additional surprising properties of CNNs: th
 ey are less susceptible to overfitting than their LN counterparts when tra
 ined on small amounts of data\, and generalize better when tested on stimu
 li drawn from a different distribution (e.g. between natural scenes and wh
 ite noise). An examination of the learned CNNs reveals several properties.
   First\, a richer set of feature maps is necessary for predicting the res
 ponses to natural scenes compared to white noise.  Second\, temporally pre
 cise responses to slowly varying inputs originate from feedforward inhibit
 ion\, similar to known retinal mechanisms. Third\, the injection of latent
  noise sources in intermediate layers enables our model to capture the sub
 -Poisson spiking variability observed in retinal ganglion cells.  Fourth\,
  augmenting our CNNs with recurrent lateral connections enables them to ca
 pture contrast adaptation as an emergent property of accurately describing
  retinal responses to natural scenes.  These methods can be readily genera
 lized to other sensory modalities and stimulus ensembles. Overall\, this w
 ork demonstrates that CNNs not only accurately capture sensory circuit res
 ponses to natural scenes\, but also can yield information about the circui
 t's internal structure and function.
LOCATION:Area 5+6+7+8 #150
END:VEVENT
BEGIN:VEVENT
SUMMARY:Safe and Efficient Off-Policy Reinforcement Learning | Remi Munos 
 \, Tom Stepleton \, Anna Harutyunyan \, Marc Bellemare
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Safe and Efficient Off-Policy Reinforcement Learning\nR
 emi Munos \, Tom Stepleton \, Anna Harutyunyan \, Marc Bellemare\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7011\n\nIn this work\, we take 
 a fresh look at some old and new algorithms for off-policy\, return-based 
 reinforcement learning. Expressing these in a common form\, we derive a no
 vel algorithm\, Retrace(lambda)\, with three desired properties: (1) it ha
 s low variance\; (2) it safely uses samples collected from any behaviour p
 olicy\, whatever its degree of "off-policyness"\; and (3) it is efficient 
 as it makes the best use of samples collected from near on-policy behaviou
 r policies. We analyse the contractive nature of the related operator unde
 r both off-policy policy evaluation and control settings and derive online
  sample-based algorithms. We believe this is the first return-based off-po
 licy control algorithm converging a.s. to Q* without the GLIE assumption (
 Greedy in the Limit with Infinite Exploration). As a corollary\, we prove 
 the convergence of Watkins' Q(lambda)\, which was  an open problem since 1
 989. We illustrate the benefits of Retrace(lambda) on a standard suite of 
 Atari 2600 games.
LOCATION:Area 5+6+7+8 #151
END:VEVENT
BEGIN:VEVENT
SUMMARY:Yggdrasil: An Optimized System for Training Deep Decision Trees at
  Scale | Firas Abuzaid \, Joseph K Bradley \, Feynman T Liang \, Andrew Fe
 ng \, Lee Yang \, Matei Zaharia \, Ameet S Talwalkar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Yggdrasil: An Optimized System for Training Deep Decisi
 on Trees at Scale\nFiras Abuzaid \, Joseph K Bradley \, Feynman T Liang \,
  Andrew Feng \, Lee Yang \, Matei Zaharia \, Ameet S Talwalkar\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7012\n\nDeep distributed decision
  trees and tree ensembles have grown in importance due to the need to mode
 l increasingly large datasets.  However\, PLANET\, the standard distribute
 d tree learning algorithm implemented in systems such as \\xgboost and Spa
 rk MLlib\, scales poorly as data dimensionality and tree depths grow.  We 
 present Yggdrasil\, a new distributed tree learning method that outperform
 s existing methods by up to 24x.  Unlike PLANET\, Yggdrasil is based on ve
 rtical partitioning of the data (i.e.\, partitioning by feature)\, along w
 ith a set of optimized data structures to reduce the CPU and communication
  costs of training. Yggdrasil (1) trains directly on compressed data for c
 ompressible features and labels\; (2) introduces efficient data structures
  for training on uncompressed data\; and (3) minimizes communication betwe
 en nodes by using sparse bitvectors.  Moreover\, while PLANET approximates
  split points through feature binning\, Yggdrasil does not require binning
 \, and we analytically characterize the impact of this approximation. We e
 valuate Yggdrasil against the MNIST 8M dataset and a high-dimensional data
 set at Yahoo\; for both\, Yggdrasil is faster by up to an order of magnitu
 de.
LOCATION:Area 5+6+7+8 #152
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sample Complexity of Automated Mechanism Design | Maria-Florina Ba
 lcan \, Tuomas Sandholm \, Ellen Vitercik
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Sample Complexity of Automated Mechanism Design\nMaria-
 Florina Balcan \, Tuomas Sandholm \, Ellen Vitercik\nhttp://nips.cc/Confer
 ences/2016/Schedule?showEvent=7013\n\nThe design of revenue-maximizing com
 binatorial auctions\, i.e. multi item auctions over bundles of goods\, is 
 one of the most fundamental problems in computational economics\, unsolved
  even for two bidders and two items for sale. In the traditional economic 
 models\, it is assumed that the bidders' valuations are drawn from an unde
 rlying distribution and that the auction designer has perfect knowledge of
  this distribution. Despite this strong and oftentimes unrealistic assumpt
 ion\, it is remarkable that the revenue-maximizing combinatorial auction r
 emains unknown. In recent years\, automated mechanism design has emerged a
 s one of the most practical and promising approaches to designing high-rev
 enue combinatorial auctions. The most scalable automated mechanism design 
 algorithms take as input samples from the bidders' valuation distribution 
 and then search for a high-revenue auction in a rich auction class. In thi
 s work\, we provide the first sample complexity analysis for the standard 
 hierarchy of deterministic combinatorial auction classes used in automated
  mechanism design. In particular\, we provide tight sample complexity boun
 ds on the number of samples needed to guarantee that the empirical revenue
  of the designed mechanism on the samples is close to its expected revenue
  on the underlying\, unknown distribution over bidder valuations\, for eac
 h of the auction classes in the hierarchy. In addition to helping set auto
 mated mechanism design on firm foundations\, our results also push the bou
 ndaries of learning theory. In particular\, the hypothesis functions used 
 in our contexts are defined through multi stage combinatorial optimization
  procedures\, rather than simple decision boundaries\, as are common in ma
 chine learning.
LOCATION:Area 5+6+7+8 #153
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Exploration via Bootstrapped DQN | Ian Osband \, Charles Blun
 dell \, Alexander Pritzel \, Benjamin Van Roy
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Deep Exploration via Bootstrapped DQN\nIan Osband \, Ch
 arles Blundell \, Alexander Pritzel \, Benjamin Van Roy\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7014\n\nEfficient exploration remains a 
 major challenge for reinforcement learning (RL). Common dithering strategi
 es for exploration\, such as epsilon-greedy\, do not carry out temporally-
 extended (or deep) exploration\; this can lead to exponentially larger dat
 a requirements. However\, most algorithms for statistically efficient RL a
 re not computationally tractable in complex environments. Randomized value
  functions offer a promising approach to efficient exploration with genera
 lization\, but existing algorithms are not compatible with nonlinearly par
 ameterized value functions. As a first step towards addressing such contex
 ts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can c
 ombine deep exploration with deep neural networks for exponentially faster
  learning than any dithering strategy. In the Arcade Learning Environment 
 bootstrapped DQN substantially improves learning speed and cumulative perf
 ormance across most games.
LOCATION:Area 5+6+7+8 #154
END:VEVENT
BEGIN:VEVENT
SUMMARY:Search Improves Label for Active Learning | Alina Beygelzimer \, D
 aniel Hsu \, John Langford \, Chicheng Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Search Improves Label for Active Learning\nAlina Beygel
 zimer \, Daniel Hsu \, John Langford \, Chicheng Zhang\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7015\n\nWe investigate active learning wi
 th access to two distinct oracles: LABEL (which is standard) and SEARCH (w
 hich is not). The SEARCH oracle models the situation where a human searche
 s a database to seed or counterexample an existing solution. SEARCH is str
 onger than LABEL while being natural to implement in many situations. We s
 how that an algorithm using both oracles can provide exponentially large p
 roblem-dependent improvements over LABEL alone.
LOCATION:Area 5+6+7+8 #155
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient and Robust Spiking Neural Circuit for Navigation Inspire
 d by Echolocating Bats | Bipin Rajendran \, Pulkit Tandon \, Yash H Malviy
 a
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Efficient and Robust Spiking Neural Circuit for Navigat
 ion Inspired by Echolocating Bats\nBipin Rajendran \, Pulkit Tandon \, Yas
 h H Malviya\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7016\n\nWe
  demonstrate a spiking neural circuit for azimuth angle detection  inspire
 d by the  echolocation circuits of the Horseshoe bat  Rhinolophus ferrumeq
 uinum and utilize it to devise a  model for  navigation and target trackin
 g\, capturing several key aspects of information transmission in biology. 
 Our network\,  using only  a simple local-information based sensor impleme
 nting the cardioid angular gain function\, operates at biological  spike r
 ate of  10 Hz.  The network  tracks large angular targets (60 degrees) wit
 hin 1 sec with a 10%  RMS error. We study the navigational ability of our 
 model for foraging and target localization tasks in  a forest of obstacles
   and show that our network requires less than 200X   spike-triggered deci
 sions\, while suffering only a 1% loss in performance compared to a  propo
 rtional-integral-derivative controller\, in the presence of 50% additive n
 oise. Superior performance can be obtained at  a higher average spike rate
  of  100 Hz  and  1000 Hz\, but even the accelerated networks requires 20X
  and 10X lesser decisions respectively\, demonstrating the superior comput
 ational efficiency of bio-inspired information processing systems.
LOCATION:Area 5+6+7+8 #156
END:VEVENT
BEGIN:VEVENT
SUMMARY:Theoretical Comparisons of Positive-Unlabeled Learning against Pos
 itive-Negative Learning | Gang Niu \, Marthinus Christoffel du Plessis \, 
 Tomoya Sakai \, Yao Ma \, Masashi Sugiyama
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Theoretical Comparisons of Positive-Unlabeled Learning 
 against Positive-Negative Learning\nGang Niu \, Marthinus Christoffel du P
 lessis \, Tomoya Sakai \, Yao Ma \, Masashi Sugiyama\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7017\n\nIn PU learning\, a binary classifie
 r is trained from positive (P) and unlabeled (U) data without negative (N)
  data. Although N data is missing\, it sometimes outperforms PN learning (
 i.e.\, ordinary supervised learning). Hitherto\, neither theoretical nor e
 xperimental analysis has been given to explain this phenomenon. In this pa
 per\, we theoretically compare PU (and NU) learning against PN learning ba
 sed on the upper bounds on estimation errors. We find simple conditions wh
 en PU and NU learning are likely to outperform PN learning\, and we prove 
 that\, in terms of the upper bounds\, either PU or NU learning (depending 
 on the class-prior probability and the sizes of P and N data) given infini
 te U data will improve on PN learning. Our theoretical findings well agree
  with the experimental results on artificial and benchmark data even when 
 the experimental setup does not match the theoretical assumptions exactly.
LOCATION:Area 5+6+7+8 #157
END:VEVENT
BEGIN:VEVENT
SUMMARY:Quantized Random Projections and Non-Linear Estimation of Cosine S
 imilarity | Ping Li \, Michael Mitzenmacher \, Martin Slawski
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Quantized Random Projections and Non-Linear Estimation 
 of Cosine Similarity\nPing Li \, Michael Mitzenmacher \, Martin Slawski\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7018\n\nRandom projectio
 ns constitute a simple\, yet effective technique for dimensionality reduct
 ion with applications in learning and search problems. In the present pape
 r\, we consider the problem of estimating cosine similarities when the pro
 jected data undergo scalar quantization to $b$ bits. We here argue that th
 e maximum likelihood estimator (MLE) is a principled approach to deal with
  the non-linearity resulting from quantization\, and subsequently study it
 s computational and statistical properties. A specific focus is on the on 
 the trade-off between bit depth and the number of projections given a fixe
 d budget of bits for storage or transmission. Along the way\, we also touc
 h upon the existence of a qualitative counterpart to the Johnson-Lindenstr
 auss lemma in the presence of quantization.
LOCATION:Area 5+6+7+8 #158
END:VEVENT
BEGIN:VEVENT
SUMMARY:CNNpack: Packing Convolutional Neural Networks in the Frequency Do
 main | Yunhe Wang \, Chang Xu \, Shan You \, Dacheng Tao \, Chao Xu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:CNNpack: Packing Convolutional Neural Networks in the F
 requency Domain\nYunhe Wang \, Chang Xu \, Shan You \, Dacheng Tao \, Chao
  Xu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7019\n\nDeep convo
 lutional neural networks (CNNs) are successfully used in a number of appli
 cations. However\, their storage and computational requirements have large
 ly prevented their widespread use on mobile devices. Here we present an ef
 fective CNN compression approach in the frequency domain\, which focuses n
 ot only on smaller weights but on all the weights and their underlying con
 nections. By treating convolutional filters as images\, we decompose their
  representations in the frequency domain as common parts (i.e.\, cluster c
 enters) shared by other similar filters and their individual private parts
  (i.e.\, individual residuals). A large number of low-energy frequency coe
 fficients in both parts can be discarded to produce high compression witho
 ut significantly compromising accuracy. We relax the computational burden 
 of convolution operations in CNNs by linearly combining the convolution re
 sponses of discrete cosine transform (DCT) bases. The compression and spee
 d-up ratios of the proposed algorithm are thoroughly analyzed and evaluate
 d on benchmark image datasets to demonstrate its superiority over state-of
 -the-art methods.
LOCATION:Area 5+6+7+8 #159
END:VEVENT
BEGIN:VEVENT
SUMMARY:Verification Based Solution for Structured MAB Problems | Zohar Ka
 rnin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Verification Based Solution for Structured MAB Problems
 \nZohar Karnin\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7020\n\
 nWe consider the problem of finding the best arm in a stochastic Mutli-arm
 ed Bandit (MAB) game and propose a general framework based on verification
  that applies to multiple well-motivated generalizations of the classic MA
 B problem. In these generalizations\, additional structure is known in adv
 ance\, causing the task of verifying the optimality of a candidate to be  
 easier than discovering the best arm. Our results are focused on the scena
 rio where the failure probability $\\delta$ must be very low\; we essentia
 lly show that in this high confidence regime\, identifying the best arm is
  as easy as the task of verification.  We demonstrate the effectiveness of
  our framework by applying it\, and improving the state-of-the art results
  in the problems of: Linear bandits\, Dueling bandits with the Condorcet a
 ssumption\, Copeland dueling bandits\, Unimodal bandits and Graphical band
 its.
LOCATION:Area 5+6+7+8 #160
END:VEVENT
BEGIN:VEVENT
SUMMARY:Neurally-Guided Procedural Models: Amortized Inference for Procedu
 ral Graphics Programs using Neural Networks | Daniel Ritchie \, Anna Thoma
 s \, Pat Hanrahan \, Noah Goodman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Neurally-Guided Procedural Models: Amortized Inference 
 for Procedural Graphics Programs using Neural Networks\nDaniel Ritchie \, 
 Anna Thomas \, Pat Hanrahan \, Noah Goodman\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7021\n\nProbabilistic inference algorithms such as S
 equential Monte Carlo (SMC) provide powerful tools for constraining proced
 ural models in computer graphics\, but they require many samples to produc
 e desirable results. In this paper\, we show how to create procedural mode
 ls which learn how to satisfy constraints. We augment procedural models wi
 th neural networks which control how the model makes random choices based 
 on the output it has generated thus far. We call such models neurally-guid
 ed procedural models. As a pre-computation\, we train these models to maxi
 mize the likelihood of example outputs generated via SMC. They are then us
 ed as efficient SMC importance samplers\, generating high-quality results 
 with very few samples. We evaluate our method on L-system-like models with
  image-based constraints. Given a desired quality threshold\, neurally-gui
 ded models can generate satisfactory results up to 10x faster than unguide
 d models.
LOCATION:Area 5+6+7+8 #161
END:VEVENT
BEGIN:VEVENT
SUMMARY:Edge-exchangeable graphs and sparsity | Diana Cai \, Trevor Campbe
 ll \, Tamara Broderick
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Edge-exchangeable graphs and sparsity\nDiana Cai \, Tre
 vor Campbell \, Tamara Broderick\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7022\n\nMany popular network models rely on the assumption of (
 vertex) exchangeability\, in which the distribution of the graph is invari
 ant to relabelings of the vertices. However\, the Aldous-Hoover theorem gu
 arantees that these graphs are dense or empty with probability one\, where
 as many real-world graphs are sparse. We present an alternative notion of 
 exchangeability for random graphs\, which we call edge exchangeability\, i
 n which the distribution of a graph sequence is invariant to the order of 
 the edges. We demonstrate that edge-exchangeable models\, unlike models th
 at are traditionally vertex exchangeable\, can exhibit sparsity. To do so\
 , we outline a general framework for graph generative models\; by contrast
  to the pioneering work of Caron and Fox (2015)\, models within our framew
 ork are stationary across steps of the graph sequence. In particular\, our
  model grows the graph by instantiating more latent atoms of a single rand
 om measure as the dataset size increases\, rather than adding new atoms to
  the measure.
LOCATION:Area 5+6+7+8 #162
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning and Forecasting Opinion Dynamics in Social Networks | Abi
 r De \, Isabel Valera \, Niloy Ganguly \, Sourangshu Bhattacharya \, Manue
 l Gomez Rodriguez
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning and Forecasting Opinion Dynamics in Social Net
 works\nAbir De \, Isabel Valera \, Niloy Ganguly \, Sourangshu Bhattachary
 a \, Manuel Gomez Rodriguez\nhttp://nips.cc/Conferences/2016/Schedule?show
 Event=7023\n\nSocial media and social networking sites have become a globa
 l pinboard for exposition and discussion of news\, topics\, and ideas\, wh
 ere social media users often update their opinions about a particular topi
 c by learning from the opinions shared by their friends. In this context\,
  can we learn a data-driven model of opinion dynamics that is able to accu
 rately forecast users' opinions? In this paper\, we introduce SLANT\, a pr
 obabilistic modeling framework of opinion dynamics\, which represents user
 s' opinions over time by means of marked jump  diffusion stochastic differ
 ential equations\, and allows for efficient model simulation and parameter
  estimation from historical fine grained event data. We then leverage our 
 framework to derive a set of efficient predictive formulas for opinion for
 ecasting and identify conditions under which opinions converge to a steady
  state. Experiments on data gathered from Twitter show that our model prov
 ides a good fit to the data and our formulas achieve more accurate forecas
 ting than alternatives.
LOCATION:Area 5+6+7+8 #163
END:VEVENT
BEGIN:VEVENT
SUMMARY:Probing the Compositionality of Intuitive Functions | Eric Schulz 
 \, Josh Tenenbaum \, David Duvenaud \, Maarten Speekenbrink \, Samuel J Ge
 rshman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Probing the Compositionality of Intuitive Functions\nEr
 ic Schulz \, Josh Tenenbaum \, David Duvenaud \, Maarten Speekenbrink \, S
 amuel J Gershman\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7024\
 n\nHow do people learn about complex functional structure? Taking inspirat
 ion from other areas of cognitive science\, we propose that this is accomp
 lished by harnessing compositionality: complex structure is decomposed int
 o simpler building blocks. We formalize this idea within the framework of 
 Bayesian regression using a grammar over Gaussian process kernels. We show
  that participants prefer compositional over non-compositional function ex
 trapolations\, that samples from the human prior over functions are best d
 escribed by a compositional model\, and that people perceive compositional
  functions as more predictable than their non-compositional but otherwise 
 similar counterparts. We argue that the compositional nature of intuitive 
 functions is consistent with broad principles of human cognition.
LOCATION:Area 5+6+7+8 #164
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning shape correspondence with anisotropic convolutional neura
 l networks | Davide Boscaini \, Jonathan Masci \, Emanuele Rodolà \, Mich
 ael Bronstein
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Learning shape correspondence with anisotropic convolut
 ional neural networks\nDavide Boscaini \, Jonathan Masci \, Emanuele Rodol
 à \, Michael Bronstein\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7025\n\nConvolutional neural networks have achieved extraordinary result
 s in many computer vision and pattern recognition applications\; however\,
  their adoption in the computer graphics and geometry processing communiti
 es is limited due to the non-Euclidean structure of their data.  In this p
 aper\, we propose Anisotropic Convolutional Neural Network (ACNN)\, a gene
 ralization of classical CNNs to non-Euclidean domains\, where classical co
 nvolutions are replaced by projections over a set of oriented anisotropic 
 diffusion kernels. We use ACNNs to effectively learn intrinsic dense corre
 spondences between deformable shapes\, a fundamental problem in geometry p
 rocessing\, arising in a wide variety of applications. We tested ACNNs per
 formance in very challenging settings\, achieving state-of-the-art results
  on some of the most difficult recent correspondence benchmarks.
LOCATION:Area 5+6+7+8 #165
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improved Techniques for Training GANs | Tim Salimans \, Ian Goodfe
 llow \, Wojciech Zaremba \, Vicki Cheung \, Alec Radford \, Xi Chen \, Xi 
 Chen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Improved Techniques for Training GANs\nTim Salimans \, 
 Ian Goodfellow \, Wojciech Zaremba \, Vicki Cheung \, Alec Radford \, Xi C
 hen \, Xi Chen\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7026\n\
 nWe present a variety of new architectural features and training procedure
 s that we apply to the generative adversarial networks (GANs) framework. U
 sing our new techniques\, we achieve state-of-the-art results in semi-supe
 rvised classification on MNIST\, CIFAR-10 and SVHN. The generated images a
 re of high quality as confirmed by a visual Turing test: Our model generat
 es MNIST samples that humans cannot distinguish from real data\, and CIFAR
 -10 samples that yield a human error rate of 21.3%. We also present ImageN
 et samples with unprecedented resolution and show that our methods enable 
 the model to learn recognizable features of ImageNet classes.
LOCATION:Area 5+6+7+8 #166
END:VEVENT
BEGIN:VEVENT
SUMMARY:Automated scalable segmentation of neurons from multispectral imag
 es | Uygar Sümbül \, Douglas Roossien \, Fei Chen \, Dawen Cai \, Nichol
 as Barry \, John Cunningham \, Liam Paninski \, Edward Boyden
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Automated scalable segmentation of neurons from multisp
 ectral images\nUygar Sümbül \, Douglas Roossien \, Fei Chen \, Dawen Cai
  \, Nicholas Barry \, John Cunningham \, Liam Paninski \, Edward Boyden\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7027\n\nReconstruction o
 f neuroanatomy is a fundamental problem in neuroscience. Stochastic expres
 sion of colors in individual cells is a promising tool\, although its use 
 in the nervous system has been limited due to various sources of variabili
 ty in expression. Moreover\, the intermingled anatomy of neuronal trees is
  challenging for existing segmentation algorithms. Here\, we propose a met
 hod to automate the segmentation of neurons in such (potentially pseudo-co
 lored) images. The method uses spatio-color relations between the voxels\,
  generates supervoxels to reduce the problem size by four orders of magnit
 ude before the final segmentation\, and is parallelizable over the supervo
 xels. To quantify performance and gain insight\, we generate simulated ima
 ges\, where the noise level and characteristics\, the density of expressio
 n\, and the number of fluorophore types are variable. We also present segm
 entations of real Brainbow images of the mouse hippocampus\, which reveal 
 many of the dendritic segments.
LOCATION:Area 5+6+7+8 #167
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Cluster Recovery in the Labeled Stochastic Block Model | S
 e-Young Yun \, Alexandre Proutiere
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Optimal Cluster Recovery in the Labeled Stochastic Bloc
 k Model\nSe-Young Yun \, Alexandre Proutiere\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=7028\n\nWe consider the problem of community detect
 ion or clustering in the labeled Stochastic Block Model (LSBM) with a fini
 te number $K$ of clusters of sizes linearly growing with the global popula
 tion of items $n$. Every pair of items is labeled independently at random\
 , and label $\\ell$ appears with probability $p(i\,j\,\\ell)$ between two 
 items in clusters indexed by $i$ and $j$\, respectively. The objective is 
 to reconstruct the clusters from the observation of these random labels.  
  Clustering under the SBM and their extensions has attracted much attentio
 n recently. Most existing work aimed at characterizing the set of paramete
 rs such that it is possible to infer clusters either positively correlated
  with the true clusters\, or with a vanishing proportion of misclassified 
 items\, or exactly matching the true clusters. We find  the set of paramet
 ers such that there exists a clustering algorithm with at most $s$ misclas
 sified items in average under the general LSBM and for any $s=o(n)$\, whic
 h solves one open problem raised in \\cite{abbe2015community}. We further 
 develop an algorithm\, based on simple spectral methods\, that achieves th
 is fundamental performance limit within $O(n \\mbox{polylog}(n))$ computat
 ions and without the a-priori knowledge of the model parameters.
LOCATION:Area 5+6+7+8 #168
END:VEVENT
BEGIN:VEVENT
SUMMARY:Phased Exploration with Greedy Exploitation in Stochastic Combinat
 orial Partial Monitoring Games | Sougata Chaudhuri \, Ambuj Tewari
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Phased Exploration with Greedy Exploitation in Stochast
 ic Combinatorial Partial Monitoring Games\nSougata Chaudhuri \, Ambuj Tewa
 ri\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7029\n\nPartial mon
 itoring games are repeated games where the learner receives feedback that 
 might be different from adversary's move or even the reward gained by the 
 learner. Recently\, a general model of combinatorial partial monitoring (C
 PM) games was proposed \\cite{lincombinatorial2014}\, where the learner's 
 action space can be exponentially large and adversary samples its moves fr
 om a bounded\, continuous space\, according to a fixed distribution. The p
 aper gave a confidence bound based algorithm (GCB) that achieves $O(T^{2/3
 }\\log T)$ distribution independent and $O(\\log T)$ distribution dependen
 t regret bounds. The implementation of their algorithm depends on two sepa
 rate offline oracles and the distribution dependent regret additionally re
 quires existence of a unique optimal action for the learner. Adopting thei
 r CPM model\, our first contribution is a Phased Exploration with Greedy E
 xploitation (PEGE) algorithmic framework for the problem. Different algori
 thms within the framework achieve $O(T^{2/3}\\sqrt{\\log T})$ distribution
  independent and $O(\\log^2 T)$ distribution dependent regret respectively
 . Crucially\, our framework needs only the simpler ``argmax'' oracle from 
 GCB and the distribution dependent regret does not require existence of a 
 unique optimal action. Our second contribution is another algorithm\, PEGE
 2\, which combines gap estimation with a PEGE algorithm\, to achieve an $O
 (\\log T)$ regret bound\, matching the GCB guarantee but removing the depe
 ndence on size of  the learner's action space. However\, like GCB\, PEGE2 
 requires access to both offline oracles and the existence of a unique opti
 mal action. Finally\, we discuss how our algorithm can be efficiently appl
 ied to a CPM problem of practical interest: namely\, online ranking with f
 eedback at the top.
LOCATION:Area 5+6+7+8 #169
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dual Space Gradient Descent for Online Learning | Trung Le \, Tu N
 guyen \, Vu Nguyen \, Dinh Phung
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Dual Space Gradient Descent for Online Learning\nTrung 
 Le \, Tu Nguyen \, Vu Nguyen \, Dinh Phung\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7030\n\nOne crucial goal in kernel online learning is
  to bound the model size. Common approaches employ budget maintenance proc
 edures to restrict the model sizes using removal\, projection\, or merging
  strategies. Although projection and merging\, in the literature\, are kno
 wn to be the most effective strategies\, they demand extensive computation
  whilst removal strategy fails to retain information of the removed vector
 s. An alternative way to address the model size problem is to apply random
  features to approximate the kernel function. This allows the model to be 
 maintained directly in the random feature space\, hence effectively resolv
 e the curse of kernelization. However\, this approach still suffers from a
  serious shortcoming as it needs to use a high dimensional random feature 
 space to achieve a sufficiently accurate kernel approximation. Consequentl
 y\, it leads to a significant increase in the computational cost. To addre
 ss all of these aforementioned challenges\, we present in this paper the D
 ual Space Gradient Descent (DualSGD)\, a novel framework that utilizes ran
 dom features as an auxiliary space to maintain information from data point
 s removed during budget maintenance. Consequently\, our approach permits t
 he budget to be maintained in a simple\, direct and elegant way while simu
 ltaneously mitigating the impact of the dimensionality issue on learning p
 erformance. We further provide convergence analysis and extensively conduc
 t experiments on five real-world datasets to demonstrate the predictive pe
 rformance and scalability of our proposed method in comparison with the st
 ate-of-the-art baselines.
LOCATION:Area 5+6+7+8 #170
END:VEVENT
BEGIN:VEVENT
SUMMARY:Data Programming: Creating Large Training Sets\, Quickly | Alexand
 er J Ratner \, Christopher M De Sa \, Sen Wu \, Daniel Selsam \, Christoph
 er Ré
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Data Programming: Creating Large Training Sets\, Quickl
 y\nAlexander J Ratner \, Christopher M De Sa \, Sen Wu \, Daniel Selsam \,
  Christopher Ré\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7031\
 n\nLarge labeled training sets are the critical building blocks of supervi
 sed learning methods and are key enablers of deep learning techniques. For
  some applications\, creating labeled training sets is the most time-consu
 ming and expensive part of applying machine learning. We therefore propose
  a paradigm for the programmatic creation of training sets called data pro
 gramming in which users provide a set of labeling functions\, which are pr
 ograms that heuristically label subsets of the data\, but that are noisy a
 nd may conflict. By viewing these labeling functions as implicitly describ
 ing a generative model for this noise\, we show that we can recover the pa
 rameters of this model to "denoise" the generated training set\, and estab
 lish theoretically that we can recover the parameters of these generative 
 models in a handful of settings. We then show how to modify a discriminati
 ve loss function to make it noise-aware\, and demonstrate our method over 
 a range of discriminative models including logistic regression and LSTMs. 
 Experimentally\, on the 2014 TAC-KBP Slot Filling challenge\, we show that
  data programming would have led to a new winning score\, and also show th
 at applying data programming to an LSTM model leads to a TAC-KBP score alm
 ost 6 F1 points over a state-of-the-art LSTM baseline (and into second pla
 ce in the competition). Additionally\, in initial user studies we observed
  that data programming may be an easier way for non-experts to create mach
 ine learning models when training data is limited or unavailable.
LOCATION:Area 5+6+7+8 #171
END:VEVENT
BEGIN:VEVENT
SUMMARY:Near-Optimal Smoothing of Structured Conditional Probability Matri
 ces | Moein Falahatgar \, Mesrob I Ohannessian \, Alon Orlitsky
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:Near-Optimal Smoothing of Structured Conditional Probab
 ility Matrices\nMoein Falahatgar \, Mesrob I Ohannessian \, Alon Orlitsky\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7032\n\nUtilizing the 
 structure of a probabilistic model can significantly increase its learning
  speed. Motivated by several recent applications\, in particular bigram mo
 dels in language processing\, we consider learning low-rank conditional pr
 obability matrices under expected KL-risk. This choice makes smoothing\, t
 hat is the careful handling of low-probability elements\, paramount. We de
 rive an iterative algorithm that extends classical non-negative matrix fac
 torization to naturally incorporate additive smoothing and prove that it c
 onverges to the stationary points of a penalized empirical risk. We then d
 erive sample-complexity bounds for the global minimizer of the penalized r
 isk and show that it is within a small factor of the optimal sample comple
 xity. This framework generalizes to more sophisticated smoothing technique
 s\, including absolute-discounting.
LOCATION:Area 5+6+7+8 #172
END:VEVENT
BEGIN:VEVENT
SUMMARY:An urn model for majority voting in classification ensembles | Vic
 tor Soto \, Alberto Suárez \, Gonzalo Martinez-Muñoz
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161205T213000
DESCRIPTION:Poster:An urn model for majority voting in classification ense
 mbles\nVictor Soto \, Alberto Suárez \, Gonzalo Martinez-Muñoz\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7033\n\nIn this work we analyze
  the class prediction of parallel randomized ensembles by majority voting 
 as an urn model. For a given test instance\, the ensemble can be viewed as
  an urn of marbles of different colors. A marble represents an individual 
 classifier. Its color represents the class label prediction of the corresp
 onding classifier. The sequential querying of classifiers in the ensemble 
 can be seen as draws without replacement from the urn. An analysis of this
  classical urn model based on the hypergeometric distribution makes it pos
 sible to estimate the confidence on the outcome of majority voting when on
 ly a fraction of the individual predictions is known. These estimates can 
 be used to speed up the prediction by the ensemble. Specifically\, the agg
 regation of votes can be halted when the confidence in the final predictio
 n is sufficiently high. If one assumes a uniform prior for the distributio
 n of possible votes the analysis is shown to be equivalent to a previous o
 ne based on Dirichlet distributions. The advantage of the current approach
  is that prior knowledge on the possible vote outcomes can be readily inco
 rporated in a Bayesian framework. We show how incorporating this type of p
 roblem-specific knowledge into the statistical analysis of majority voting
  leads to faster classification by the ensemble and allows us to estimate 
 the expected average speed-up beforehand.
LOCATION:Area 5+6+7+8 #173
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Multi-fidelity Multi-armed Bandit | Kirthevasan Kandasamy \, G
 autam Dasarathy \, Barnabas Poczos \, Jeff Schneider
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:The Multi-fidelity Multi-armed Bandit\nKirthevasan Kand
 asamy \, Gautam Dasarathy \, Barnabas Poczos \, Jeff Schneider\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7034\n\nWe study a variant of the
  classical stochastic $K$-armed bandit where observing the outcome of each
  arm is expensive\, but cheap approximations to this outcome are available
 . For example\, in online advertising the performance of an ad can be appr
 oximated by displaying it for shorter time periods or to narrower audience
 s. We formalise this task as a \\emph{multi-fidelity} bandit\, where\, at 
 each time step\, the forecaster may choose to play an arm at any one of $M
 $ fidelities. The highest fidelity (desired outcome) expends cost $\\costM
 $. The $m$\\ssth fidelity (an approximation) expends $\\costm &lt\; \\cost
 M$ and returns a biased estimate of the highest fidelity. We develop \\mfu
 cb\, a novel upper confidence bound procedure for this setting and prove t
 hat it naturally adapts to the sequence of available approximations and co
 sts thus attaining better regret than naive strategies which ignore the ap
 proximations. For instance\, in the above online advertising example\, \\m
 fucbs would use the lower fidelities to quickly eliminate suboptimal ads a
 nd reserve the larger expensive experiments on a small set of promising ca
 ndidates. We complement this result with a lower bound and show that \\mfu
 cbs is nearly optimal under certain conditions.
LOCATION:Area 5+6+7+8 #1
END:VEVENT
BEGIN:VEVENT
SUMMARY:Probabilistic Inference with Generating Functions for Poisson Late
 nt Variable Models | Kevin Winner \, Daniel Sheldon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Probabilistic Inference with Generating Functions for P
 oisson Latent Variable Models\nKevin Winner \, Daniel Sheldon\nhttp://nips
 .cc/Conferences/2016/Schedule?showEvent=7035\n\nGraphical models with late
 nt count variables arise in a number of fields. Standard exact inference t
 echniques such as variable elimination and belief propagation do not apply
  to these models because the latent variables have countably infinite supp
 ort. As a result\, approximations such as truncation or MCMC are employed.
  We present the first exact inference algorithms for a class of models wit
 h latent count variables by developing a novel representation of countably
  infinite factors as probability generating functions\, and then performin
 g variable elimination with generating functions. Our approach is exact\, 
 runs in pseudo-polynomial time\, and is much faster than existing approxim
 ate techniques. It leads to better parameter estimates for problems in pop
 ulation ecology by avoiding error introduced by approximate likelihood com
 putations.
LOCATION:Area 5+6+7+8 #2
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Maximization of Pointwise Submodular Functions With Budge
 t Constraint | Nguyen Cuong \, Huan Xu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Adaptive Maximization of Pointwise Submodular Functions
  With Budget Constraint\nNguyen Cuong \, Huan Xu\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=7036\n\nWe study the worst-case adaptive optimi
 zation problem with budget constraint that is useful for modeling various 
 practical applications in artificial intelligence and machine learning. We
  investigate the near-optimality of greedy algorithms for this problem wit
 h both modular and non-modular cost functions. In both cases\, we prove th
 at two simple greedy algorithms are not near-optimal but the best between 
 them is near-optimal if the utility function satisfies pointwise submodula
 rity and pointwise cost-sensitive submodularity respectively. This implies
  a combined algorithm that is near-optimal with respect to the optimal alg
 orithm that uses half of the budget. We discuss applications of our theore
 tical results and also report experiments comparing the greedy algorithms 
 on the active learning problem.
LOCATION:Area 5+6+7+8 #3
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dual Learning for Machine Translation | Di He \, Yingce Xia \, Tao
  Qin \, Liwei Wang \, Nenghai Yu \, Tieyan Liu \, Wei-Ying Ma
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Dual Learning for Machine Translation\nDi He \, Yingce 
 Xia \, Tao Qin \, Liwei Wang \, Nenghai Yu \, Tieyan Liu \, Wei-Ying Ma\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7037\n\nWhile neural mac
 hine translation (NMT) is making good progress in the past two years\, ten
 s of millions of bilingual sentence pairs are needed for its training. How
 ever\, human labeling is very costly. To tackle this training data bottlen
 eck\, we develop a dual-learning mechanism\, which can enable an NMT syste
 m to automatically learn from unlabeled data through a dual-learning game.
  This mechanism is inspired by the following observation: any machine tran
 slation task has a dual task\, e.g.\, English-to-French translation (prima
 l) versus French-to-English translation (dual)\; the primal and dual tasks
  can form a closed loop\, and generate informative feedback signals to tra
 in the translation models\, even if without the involvement of a human lab
 eler. In the dual-learning mechanism\, we use one agent to represent the m
 odel for the primal task and the other agent to represent the model for th
 e dual task\, then ask them to teach each other through a reinforcement le
 arning process. Based on the feedback signals generated during this proces
 s (e.g.\, the language-model likelihood of the output of a model\, and the
  reconstruction error of the original sentence after the primal and dual t
 ranslations)\, we can iteratively update the two models until convergence 
 (e.g.\, using the policy gradient methods). We call the corresponding appr
 oach to neural machine translation \\emph{dual-NMT}. Experiments show that
  dual-NMT works very well on English$\\leftrightarrow$French translation\;
  especially\, by learning from monolingual data (with 10\\% bilingual data
  for warm start)\, it achieves a comparable accuracy to NMT trained from t
 he full bilingual data for the French-to-English translation task.
LOCATION:Area 5+6+7+8 #4
END:VEVENT
BEGIN:VEVENT
SUMMARY:Iterative Refinement of the Approximate Posterior for Directed Bel
 ief Networks | Devon Hjelm \, Ruslan Salakhutdinov \, Kyunghyun Cho \, Neb
 ojsa Jojic \, Vince Calhoun \, Junyoung Chung
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Iterative Refinement of the Approximate Posterior for D
 irected Belief Networks\nDevon Hjelm \, Ruslan Salakhutdinov \, Kyunghyun 
 Cho \, Nebojsa Jojic \, Vince Calhoun \, Junyoung Chung\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7038\n\nVariational methods that rely on
  a recognition network to approximate the posterior of directed graphical 
 models offer better inference and learning than previous methods. Recent a
 dvances that exploit the capacity and flexibility in this approach have ex
 panded what kinds of models can be trained. However\, as a proposal for th
 e posterior\, the capacity of the recognition network is limited\, which c
 an constrain the representational power of the generative model and increa
 se the variance of Monte Carlo estimates. To address these issues\, we int
 roduce an iterative refinement procedure for improving the approximate pos
 terior of the recognition network and show that training with the refined 
 posterior is competitive with state-of-the-art methods. The advantages of 
 refinement are further evident in an increased effective sample size\, whi
 ch implies a lower variance of gradient estimates.
LOCATION:Area 5+6+7+8 #5
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Risk Estimation Using Only Conditional Independence S
 tructure | Jacob Steinhardt \, Percy S Liang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Unsupervised Risk Estimation Using Only Conditional Ind
 ependence Structure\nJacob Steinhardt \, Percy S Liang\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7039\n\nWe show how to estimate a model
 ’s test error from unlabeled data\, on distributions very different from
  the training distribution\, while assuming only that certain conditional 
 independencies are preserved between train and test. We do not need to ass
 ume that the optimal predictor is the same between train and test\, or tha
 t the true distribution lies in any parametric family. We can also efficie
 ntly compute gradients of the estimated error and hence perform unsupervis
 ed discriminative learning. Our technical tool is the method of moments\, 
 which allows us to exploit conditional independencies in the absence of a 
 fully-specified model. Our framework encompasses a large family of losses 
 including the log and exponential loss\, and extends to structured output 
 settings such as conditional random fields.
LOCATION:Area 5+6+7+8 #6
END:VEVENT
BEGIN:VEVENT
SUMMARY:Hierarchical Question-Image Co-Attention for Visual Question Answe
 ring | Jiasen Lu \, Jianwei Yang \, Dhruv Batra \, Devi Parikh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Hierarchical Question-Image Co-Attention for Visual Que
 stion Answering\nJiasen Lu \, Jianwei Yang \, Dhruv Batra \, Devi Parikh\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7040\n\nA number of rec
 ent works have proposed attention models for Visual Question Answering (VQ
 A) that generate spatial maps highlighting image regions relevant to answe
 ring the question. In this paper\, we argue that in addition to modeling "
 where to look" or visual attention\, it is equally important to model "wha
 t words to listen to" or question attention. We present a novel co-attenti
 on model for VQA that jointly reasons about image and question attention. 
 In addition\, our model reasons about the question (and consequently the i
 mage via the co-attention mechanism) in a hierarchical fashion via a novel
  1-dimensional convolution neural networks (CNN). Our model improves the s
 tate-of-the-art on the VQA dataset from 60.3% to 60.5%\, and from 61.6% to
  63.3% on the COCO-QA dataset. By using ResNet\, the performance is furthe
 r improved to 62.1% for VQA and 65.4% for COCO-QA.
LOCATION:Area 5+6+7+8 #7
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian Optimization with a Finite Budget: An Approximate Dynamic
  Programming Approach | Remi Lam \, Karen Willcox \, David Wolpert
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Bayesian Optimization with a Finite Budget: An Approxim
 ate Dynamic Programming Approach\nRemi Lam \, Karen Willcox \, David Wolpe
 rt\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7041\n\nWe consider
  the problem of optimizing an expensive objective function when a finite b
 udget of total evaluations is prescribed. In that context\, the optimal so
 lution strategy for Bayesian optimization can be formulated as a dynamic p
 rogramming instance. This results in a complex problem with uncountable\, 
 dimension-increasing state space and an uncountable control space. We show
  how to approximate the solution of this dynamic programming problem  usin
 g rollout\, and propose rollout heuristics specifically designed for the B
 ayesian optimization setting. We present numerical experiments showing tha
 t the resulting algorithm for optimization with a finite budget outperform
 s several popular Bayesian optimization algorithms.
LOCATION:Area 5+6+7+8 #8
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning to learn by gradient descent by gradient descent | Marcin
  Andrychowicz \, Misha Denil \, Sergio Gómez \, Matthew W Hoffman \, Davi
 d Pfau \, Tom Schaul \, Nando de Freitas
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning to learn by gradient descent by gradient desce
 nt\nMarcin Andrychowicz \, Misha Denil \, Sergio Gómez \, Matthew W Hoffm
 an \, David Pfau \, Tom Schaul \, Nando de Freitas\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7042\n\nThe move from hand-designed features 
 to learned features in machine learning has been wildly successful. In spi
 te of this\, optimization algorithms are still designed by hand. In this p
 aper we show how the design of an optimization algorithm can be cast as a 
 learning problem\, allowing the algorithm to learn to exploit structure in
  the problems of interest in an automatic way. Our learned algorithms\, im
 plemented by LSTMs\, outperform generic\, hand-designed competitors on the
  tasks for which they are trained\, and also generalize well to new tasks 
 with similar structure. We demonstrate this on a number of tasks\, includi
 ng simple convex problems\, training neural networks\, and styling images 
 with neural art.
LOCATION:Area 5+6+7+8 #9
END:VEVENT
BEGIN:VEVENT
SUMMARY:Computational and Statistical Tradeoffs in Learning to Rank | Ashi
 sh Khetan \, Sewoong Oh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Computational and Statistical Tradeoffs in Learning to 
 Rank\nAshish Khetan \, Sewoong Oh\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7043\n\nFor massive and heterogeneous modern  data sets\, it i
 s of fundamental interest to provide guarantees on the accuracy of estimat
 ion when computational resources are limited. In the application of learni
 ng to rank\, we provide a hierarchy of rank-breaking mechanisms ordered by
  the complexity in thus generated sketch of the data. This allows the numb
 er of data points collected to be gracefully traded off against computatio
 nal resources available\, while guaranteeing the desired level of accuracy
 . Theoretical guarantees on the proposed generalized rank-breaking implici
 tly provide such trade-offs\, which can be explicitly characterized under 
 certain canonical scenarios on the structure of the data.
LOCATION:Area 5+6+7+8 #10
END:VEVENT
BEGIN:VEVENT
SUMMARY:Pairwise Choice Markov Chains | Stephen Ragain \, Johan Ugander
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Pairwise Choice Markov Chains\nStephen Ragain \, Johan 
 Ugander\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7044\n\nAs dat
 asets capturing human choices grow in richness and scale\, particularly in
  online domains\, there is an increasing need for choice models flexible e
 nough to handle data that violate traditional choice-theoretic axioms such
  as regularity\, stochastic transitivity\, or Luce's choice axiom. In this
  work we introduce the Pairwise Choice Markov Chain (PCMC) model of discre
 te choice\, an inferentially tractable model that does not assume these tr
 aditional axioms while still satisfying the foundational axiom of uniform 
 expansion\, which can be viewed as a weaker version of Luce's axiom. We sh
 ow that the PCMC model significantly outperforms the Multinomial Logit (MN
 L) model in prediction tasks on two empirical data sets known to exhibit v
 iolations of Luce's axiom. Our analysis also synthesizes several recent ob
 servations connecting the Multinomial Logit model and Markov chains\; the 
 PCMC model retains the Multinomial Logit model as a special case.
LOCATION:Area 5+6+7+8 #11
END:VEVENT
BEGIN:VEVENT
SUMMARY:Incremental Variational Sparse Gaussian Process Regression | Ching
 -An Cheng \, Byron Boots
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Incremental Variational Sparse Gaussian Process Regress
 ion\nChing-An Cheng \, Byron Boots\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=7045\n\nRecent work on scaling up Gaussian process regression
  (GPR) to large datasets has primarily focused on sparse GPR\, which lever
 ages a small set of basis functions to approximate the full Gaussian proce
 ss during inference.  However\, the majority of these approaches are batch
  methods that operate on the entire training dataset at once\, precluding 
 the use of datasets that are streaming or too large to fit into memory. Al
 though previous work has considered incrementally solving variational spar
 se GPR\, most algorithms fail to update the basis functions and therefore 
 perform suboptimally. We propose a novel incremental learning algorithm fo
 r variational sparse GPR based on stochastic mirror ascent of probability 
 densities in reproducing kernel Hilbert space. This new formulation allows
  our algorithm to update basis functions online in accordance with the man
 ifold structure of probability densities for fast convergence. We conduct 
 several experiments and show that our proposed approach achieves better em
 pirical performance in terms of prediction error than  the recent state-of
 -the-art incremental solutions to variational sparse GPR.
LOCATION:Area 5+6+7+8 #12
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combinatorial Multi-Armed Bandit with General Reward Functions | W
 ei Chen \, Wei Hu \, Fu Li \, Jian Li \, Yu Liu \, Pinyan Lu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Combinatorial Multi-Armed Bandit with General Reward Fu
 nctions\nWei Chen \, Wei Hu \, Fu Li \, Jian Li \, Yu Liu \, Pinyan Lu\nht
 tp://nips.cc/Conferences/2016/Schedule?showEvent=7046\n\nIn this paper\, w
 e study the stochastic combinatorial multi-armed bandit (CMAB) framework t
 hat allows a general nonlinear reward function\, whose expected value may 
 not depend only on the means of the input random variables but possibly on
  the entire distributions of these variables. Our framework enables a much
  larger class of reward functions such as the $\\max()$ function and nonli
 near utility functions. Existing techniques relying on accurate estimation
 s of the means of random variables\, such as the upper confidence bound (U
 CB) technique\, do not work directly on these functions. We propose a new 
 algorithm called stochastically dominant confidence bound (SDCB)\, which e
 stimates the distributions of underlying random variables and their stocha
 stically dominant confidence bounds. We prove that SDCB can achieve $O(\\l
 og T)$ distribution-dependent regret and $\\tilde{O}(\\sqrt{T})$ distribut
 ion-independent regret\, where $T$ is the time horizon. We apply our resul
 ts to the $K$-MAX problem and expected utility maximization problems. In p
 articular\, for $K$-MAX\, we provide the first polynomial-time approximati
 on scheme (PTAS) for its offline problem\, and give the first $\\tilde{O}(
 \\sqrt T)$ bound on the $(1-\\epsilon)$-approximation regret of its online
  problem\, for any $\\epsilon&gt\;0$.
LOCATION:Area 5+6+7+8 #13
END:VEVENT
BEGIN:VEVENT
SUMMARY:Observational-Interventional Priors for Dose-Response Learning | R
 icardo Silva
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Observational-Interventional Priors for Dose-Response L
 earning\nRicardo Silva\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7047\n\nControlled interventions provide the most direct source of inform
 ation for learning causal effects. In particular\, a dose-response curve c
 an be learned by varying the treatment level and observing the correspondi
 ng outcomes. However\, interventions can be expensive and time-consuming. 
 Observational data\, where the treatment is not controlled by a known mech
 anism\, is sometimes available. Under some strong assumptions\, observatio
 nal data allows for the estimation of dose-response curves. Estimating suc
 h curves nonparametrically is hard: sample sizes for controlled interventi
 ons may be small\, while in the observational case a large number of measu
 red confounders may need to be marginalized. In this paper\, we introduce 
 a hierarchical Gaussian process prior that constructs a distribution over 
 the dose-response curve by learning from observational data\, and reshapes
  the distribution with a nonparametric affine transform learned from contr
 olled interventions. This function composition from different sources is s
 hown to speed-up learning\, which we demonstrate with a thorough sensitivi
 ty analysis and an application to modeling the effect of therapy on cognit
 ive skills of premature infants.
LOCATION:Area 5+6+7+8 #14
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Graph Reconstruction via Empirical Risk Minimization: Fast Lear
 ning Rates and Scalability | Guillaume Papa \, Aurélien Bellet \, Stephan
  Clémençon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:On Graph Reconstruction via Empirical Risk Minimization
 : Fast Learning Rates and Scalability\nGuillaume Papa \, Aurélien Bellet 
 \, Stephan Clémençon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7048\n\nThe problem of predicting connections between a set of data point
 s finds many applications\, in systems biology and social network analysis
  among others. This paper focuses on the \\textit{graph reconstruction} pr
 oblem\, where the prediction rule is obtained by minimizing the average er
 ror over all n(n-1)/2 possible pairs of the n nodes of a training graph. O
 ur first contribution is to derive learning rates of order O(log n / n) fo
 r this problem\, significantly improving upon the slow rates of order O(1/
 √n) established in the seminal work of Biau &amp\; Bleakley (2006). Stri
 kingly\, these fast rates are universal\, in contrast to similar results k
 nown for other statistical learning problems (e.g.\, classification\, dens
 ity level set estimation\, ranking\, clustering) which require strong assu
 mptions on the distribution of the data. Motivated by applications to larg
 e graphs\, our second contribution deals with the computational complexity
  of graph reconstruction. Specifically\, we investigate to which extent th
 e learning rates can be preserved when replacing the empirical reconstruct
 ion risk by a computationally cheaper Monte-Carlo version\, obtained by sa
 mpling with replacement B &lt\;&lt\; n² pairs of nodes. Finally\, we illu
 strate our theoretical results by numerical experiments on synthetic and r
 eal graphs.
LOCATION:Area 5+6+7+8 #15
END:VEVENT
BEGIN:VEVENT
SUMMARY:DeepMath - Deep Sequence Models for Premise Selection | Geoffrey I
 rving \, Christian Szegedy \, Niklas Een \, Alexander A Alemi \, Francois 
 Chollet \, Josef Urban
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:DeepMath - Deep Sequence Models for Premise Selection\n
 Geoffrey Irving \, Christian Szegedy \, Niklas Een \, Alexander A Alemi \,
  Francois Chollet \, Josef Urban\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7049\n\nWe study the effectiveness of neural sequence models fo
 r premise selection in automated theorem proving\, a key bottleneck for pr
 ogress in formalized mathematics. We propose a two stage approach for this
  task that yields good results for the premise selection task on the Mizar
  corpus while avoiding the hand-engineered features of existing state-of-t
 he-art models. To our knowledge\, this is the first time deep learning has
  been applied  theorem proving on a large scale.
LOCATION:Area 5+6+7+8 #16
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient Second Order Online Learning by Sketching | Haipeng Luo 
 \, Alekh Agarwal \, Nicolò Cesa-Bianchi \, John Langford
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Efficient Second Order Online Learning by Sketching\nHa
 ipeng Luo \, Alekh Agarwal \, Nicolò Cesa-Bianchi \, John Langford\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=7050\n\nWe propose Sketched 
 Online Newton (SON)\, an online second order learning algorithm that enjoy
 s substantially improved regret guarantees for ill-conditioned data. SON i
 s an enhanced version of the Online Newton Step\, which\, via sketching te
 chniques enjoys a running time linear in the dimension and sketch size.  W
 e further develop sparse forms of the sketching methods (such as Oja's rul
 e)\, making the computation linear in the sparsity of features. Together\,
  the algorithm eliminates all computational obstacles in previous second o
 rder online learning approaches.
LOCATION:Area 5+6+7+8 #17
END:VEVENT
BEGIN:VEVENT
SUMMARY:Gaussian Processes for Survival Analysis | Tamara Fernandez \, Nic
 olas Rivera \, Yee Whye Teh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Gaussian Processes for Survival Analysis\nTamara Fernan
 dez \, Nicolas Rivera \, Yee Whye Teh\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=7051\n\nWe introduce a semi-parametric Bayesian model for 
 survival analysis. The model is centred on a parametric baseline hazard\, 
 and uses a Gaussian process to model variations away from it nonparametric
 ally\, as well as  dependence on covariates. As opposed to many other meth
 ods in survival analysis\, our framework does not impose unnecessary const
 raints in the hazard rate or in the survival function. Furthermore\, our m
 odel handles left\, right and interval censoring mechanisms common in surv
 ival analysis. We propose a MCMC algorithm to perform inference and an app
 roximation scheme based on random Fourier features to make computations fa
 ster. We report experimental results on synthetic and real data\, showing 
 that our model performs better than competing models such as Cox proportio
 nal hazards\, ANOVA-DDP and random survival forests.
LOCATION:Area 5+6+7+8 #18
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Power of Optimization from Samples | Eric Balkanski \, Aviad R
 ubinstein \, Yaron Singer
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:The Power of Optimization from Samples\nEric Balkanski 
 \, Aviad Rubinstein \, Yaron Singer\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7052\n\nWe consider the problem of optimization from samples
  of monotone submodular functions with bounded curvature. In numerous appl
 ications\, the function optimized is not known a priori\, but instead lear
 ned from data. What are the guarantees we have when optimizing functions f
 rom sampled data?  In this paper we show that for any monotone submodular 
 function with curvature c there is a (1 - c)/(1 + c - c^2) approximation a
 lgorithm for maximization under cardinality constraints when polynomially-
 many samples are drawn from the uniform distribution over feasible sets. M
 oreover\, we show that this algorithm is optimal. That is\, for any c &lt\
 ; 1\, there exists a submodular function with curvature c for which no alg
 orithm can achieve a better approximation. The curvature assumption is cru
 cial as for general monotone submodular functions no algorithm can obtain 
 a constant-factor approximation for maximization under a cardinality const
 raint when observing polynomially-many samples drawn from any distribution
  over feasible sets\, even when the function is statistically learnable.
LOCATION:Area 5+6+7+8 #19
END:VEVENT
BEGIN:VEVENT
SUMMARY:Global Optimality of Local Search for Low Rank Matrix Recovery | S
 rinadh Bhojanapalli \, Behnam Neyshabur \, Nati Srebro
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Global Optimality of Local Search for Low Rank Matrix R
 ecovery\nSrinadh Bhojanapalli \, Behnam Neyshabur \, Nati Srebro\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7053\n\nWe show that there are 
 no spurious local minima in the non-convex factorized parametrization of l
 ow-rank matrix recovery from incoherent linear measurements.  With noisy m
 easurements we show all local minima are very close to a global optimum.  
 Together with a curvature bound at saddle points\, this yields a polynomia
 l time global convergence guarantee for stochastic gradient descent {\\em 
  from random initialization}.
LOCATION:Area 5+6+7+8 #20
END:VEVENT
BEGIN:VEVENT
SUMMARY:A state-space model of cross-region dynamic connectivity in MEG/EE
 G | Ying Yang \, Elissa Aminoff \, Michael Tarr \, Kass E Robert
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A state-space model of cross-region dynamic connectivit
 y in MEG/EEG\nYing Yang \, Elissa Aminoff \, Michael Tarr \, Kass E Robert
 \nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7054\n\nCross-region 
 dynamic connectivity\, which describes spatio-temporal dependence of neura
 l activity among multiple brain regions of interest (ROIs)\, can provide i
 mportant information for understanding cognition. For estimating such conn
 ectivity\, magnetoencephalography (MEG) and electroencephalography (EEG) a
 re well-suited tools because of their millisecond temporal resolution. How
 ever\, localizing source activity in the brain requires solving an under-d
 etermined linear problem. In typical two-step approaches\, researchers fir
 st solve the linear problem with general priors assuming independence acro
 ss ROIs\, and secondly quantify cross-region connectivity. In this work\, 
 we propose a one-step state-space model to improve estimation of dynamic c
 onnectivity. The model treats the mean activity in individual ROIs as the 
 state variable\, and describes non-stationary dynamic dependence across RO
 Is using time-varying auto-regression. Compared with a two-step method\, w
 hich first obtains the commonly used minimum-norm estimates of source acti
 vity\, and then fits the auto-regressive model\, our state-space model yie
 lded smaller estimation errors on simulated data where the model assumptio
 ns held. When applied on empirical MEG data from one participant in a scen
 e-processing experiment\, our state-space model also demonstrated intrigui
 ng preliminary results\, indicating leading and lagged linear dependence b
 etween the early visual cortex and a higher-level scene-sensitive region\,
  which could reflect feed-forward and feedback information flow within the
  visual cortex during scene processing.
LOCATION:Area 5+6+7+8 #21
END:VEVENT
BEGIN:VEVENT
SUMMARY:Hypothesis Testing in Unsupervised Domain Adaptation with Applicat
 ions in Alzheimer's Disease | Hao Zhou \, Vamsi K Ithapu \, Sathya Narayan
 an Ravi \, Vikas Singh \, Grace Wahba \, Sterling C Johnson
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Hypothesis Testing in Unsupervised Domain Adaptation wi
 th Applications in Alzheimer's Disease\nHao Zhou \, Vamsi K Ithapu \, Sath
 ya Narayanan Ravi \, Vikas Singh \, Grace Wahba \, Sterling C Johnson\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7055\n\nConsider samples f
 rom two different data sources ${\\mathbf{xs^i}} \\sim P{\\rm source}$ and
  ${\\mathbf{xt^i}} \\sim P{\\rm target}$. We only observe their transforme
 d versions $h(\\mathbf{xs^i})$ and $g(\\mathbf{xt^i})$\, for some known fu
 nction class $h(\\cdot)$ and $g(\\cdot)$. Our goal is to perform a statist
 ical test checking if $P{\\rm source}$ = $P{\\rm target}$ while removing t
 he distortions induced by the transformations. This problem is closely rel
 ated to concepts underlying numerous domain adaptation algorithms\, and in
  our case\, is motivated by the need to combine clinical and imaging based
  biomarkers from multiple sites and/or batches\, where this problem is fai
 rly common and an impediment in the conduct of analyses with much larger s
 ample sizes. We develop a framework that addresses this problem using idea
 s from hypothesis testing on the transformed measurements\, where in the d
 istortions need to be estimated {\\it in tandem} with the testing. We deri
 ve a simple algorithm and study its convergence and consistency properties
  in detail\, and we also provide lower-bound strategies based on recent wo
 rk in continuous optimization. On a dataset of individuals at risk for neu
 rological disease\, our results are competitive with alternative procedure
 s that are twice as expensive and in some cases operationally infeasible t
 o implement.
LOCATION:Area 5+6+7+8 #22
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bi-Objective Online Matching and Submodular  Allocations | Hossein
  Esfandiari \, Nitish Korula \, Vahab Mirrokni
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Bi-Objective Online Matching and Submodular  Allocation
 s\nHossein Esfandiari \, Nitish Korula \, Vahab Mirrokni\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7056\n\nOnline allocation problems have
  been widely studied due to their numerous practical applications (particu
 larly to Internet advertising)\, as well as considerable theoretical inter
 est. The main challenge in such problems is making assignment decisions in
  the face of uncertainty about future input\; effective algorithms need to
  predict which constraints are most likely to bind\, and learn the balance
  between short-term gain and the value of long-term resource availability.
   In many important applications\, the algorithm designer is faced with mu
 ltiple objectives to optimize. In particular\, in online advertising it is
  fairly common to optimize multiple metrics\, such as clicks\, conversions
 \, and impressions\, as well as other metrics which may be largely uncorre
 lated such as ‘share of voice’\, and ‘buyer surplus’. While there 
 has been considerable work on multi-objective offline optimization (when t
 he entire input is known in advance)\, very little is known about the onli
 ne case\, particularly in the case of adversarial input. In this paper\, w
 e give the first results for bi-objective online submodular optimization\,
  providing almost matching upper and lower bounds for allocating items to 
 agents with two submodular value functions. We also study practically rele
 vant special cases of this problem related to Internet advertising\, and o
 btain improved results. All our algorithms are nearly best possible\, as w
 ell as being efficient and easy to implement in practice.
LOCATION:Area 5+6+7+8 #23
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Constant-Factor Bi-Criteria Approximation Guarantee for k-means+
 + | Dennis Wei
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Constant-Factor Bi-Criteria Approximation Guarantee f
 or k-means++\nDennis Wei\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7057\n\nThis paper studies the $k$-means++ algorithm for clustering as 
 well as the class of $D^\\ell$ sampling algorithms to which $k$-means++ be
 longs.  It is shown that for any constant factor $\\beta &gt\; 1$\, select
 ing $\\beta k$ cluster centers by $D^\\ell$ sampling yields a constant-fac
 tor approximation to the optimal clustering with $k$ centers\, in expectat
 ion and without conditions on the dataset.  This result extends the previo
 usly known $O(\\log k)$ guarantee for the case $\\beta = 1$ to the constan
 t-factor bi-criteria regime.  It also improves upon an existing constant-f
 actor bi-criteria result that holds only with constant probability.
LOCATION:Area 5+6+7+8 #24
END:VEVENT
BEGIN:VEVENT
SUMMARY:Causal Bandits: Learning Good Interventions via Causal Inference |
  Finnian Lattimore \, Tor Lattimore \, Mark Reid
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Causal Bandits: Learning Good Interventions via Causal 
 Inference\nFinnian Lattimore \, Tor Lattimore \, Mark Reid\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7058\n\nWe study the problem of using
  causal models to improve the rate at which good interventions can be lear
 ned online in a stochastic environment. Our formalism combines multi-arm b
 andits and causal inference to model a novel type of bandit feedback that 
 is not exploited by existing approaches. We propose a new algorithm that e
 xploits the causal feedback and prove a bound on its simple regret that is
  strictly better (in all quantities) than algorithms that do not use the a
 dditional causal information.
LOCATION:Area 5+6+7+8 #25
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Domain Adaptation with Residual Transfer Networks | M
 ingsheng Long \, Han Zhu \, Jianmin Wang \, Michael I Jordan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Unsupervised Domain Adaptation with Residual Transfer N
 etworks\nMingsheng Long \, Han Zhu \, Jianmin Wang \, Michael I Jordan\nht
 tp://nips.cc/Conferences/2016/Schedule?showEvent=7059\n\nThe recent succes
 s of deep neural networks relies on massive amounts of labeled data. For a
  target task where labeled data is unavailable\, domain adaptation can tra
 nsfer a learner from a different source domain. In this paper\, we propose
  a new approach to domain adaptation in deep networks that can jointly lea
 rn adaptive classifiers and transferable features from labeled data in the
  source domain and unlabeled data in the target domain. We relax a shared-
 classifier assumption made by previous methods and assume that the source 
 classifier and target classifier differ by a residual function. We enable 
 classifier adaptation by plugging several layers into deep network to expl
 icitly learn the residual function with reference to the target classifier
 . We fuse features of multiple layers with tensor product and embed them i
 nto reproducing kernel Hilbert spaces to match distributions for feature a
 daptation. The adaptation can be achieved in most feed-forward models by e
 xtending them with new residual layers and loss functions\, which can be t
 rained efficiently via back-propagation. Empirical evidence shows that the
  new approach outperforms state of the art methods on standard domain adap
 tation benchmarks.
LOCATION:Area 5+6+7+8 #26
END:VEVENT
BEGIN:VEVENT
SUMMARY:Data driven estimation of Laplace-Beltrami operator | Frederic Cha
 zal \, Ilaria Giulini \, Bertrand Michel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Data driven estimation of Laplace-Beltrami operator\nFr
 ederic Chazal \, Ilaria Giulini \, Bertrand Michel\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7060\n\nApproximations of Laplace-Beltrami op
 erators on manifolds through graph Laplacians have become popular tools in
  data analysis and machine learning. These discretized operators usually d
 epend on bandwidth parameters whose tuning remains a theoretical and pract
 ical problem. In this paper\, we address this problem for the unormalized 
 graph Laplacian by establishing an oracle inequality that opens the door t
 o a well-founded data-driven procedure for the bandwidth selection. Our ap
 proach relies on recent results by Lacour and Massart (2015) on the so-cal
 led Lepski's method.
LOCATION:Area 5+6+7+8 #27
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast Algorithms for Robust PCA via Gradient Descent | Xinyang Yi \
 , Dohyung Park \, Yudong Chen \, Constantine Caramanis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Fast Algorithms for Robust PCA via Gradient Descent\nXi
 nyang Yi \, Dohyung Park \, Yudong Chen \, Constantine Caramanis\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7061\n\nWe consider the problem
  of Robust PCA in the fully and partially observed settings. Without corru
 ptions\, this is the well-known matrix completion problem. From a statisti
 cal standpoint this problem has been recently well-studied\, and condition
 s on when recovery is possible (how many observations do we need\, how man
 y corruptions can we tolerate) via polynomial-time algorithms is by now un
 derstood. This paper presents and analyzes a non-convex optimization appro
 ach that greatly reduces the computational complexity of the above problem
 s\, compared to the best available algorithms. In particular\, in the full
 y observed case\, with $r$ denoting rank and $d$ dimension\, we reduce the
  complexity from $O(r^2d^2\\log(1/\\epsilon))$ to $O(rd^2\\log(1/\\epsilon
 ))$ -- a big savings when the rank is big. For the partially observed case
 \, we show the complexity of our algorithm is no more than $O(r^4d\\log(d)
 \\log(1/\\epsilon))$. Not only is this the best-known run-time for a prova
 ble algorithm under partial observation\, but in the setting where $r$ is 
 small compared to $d$\, it also allows for near-linear-in-$d$ run-time tha
 t can be exploited in the fully-observed case as well\, by simply running 
 our algorithm on a subset of the observations.
LOCATION:Area 5+6+7+8 #28
END:VEVENT
BEGIN:VEVENT
SUMMARY:NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed an
 d Stochastic Optimization | Davood Hajinezhad \, Mingyi Hong \, Tuo Zhao \
 , Zhaoran Wang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:NESTT: A Nonconvex Primal-Dual Splitting Method for Dis
 tributed and Stochastic Optimization\nDavood Hajinezhad \, Mingyi Hong \, 
 Tuo Zhao \, Zhaoran Wang\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7062\n\nWe study a stochastic and distributed algorithm for nonconvex  
 problems whose objective consists a sum $N$ nonconvex $Li/N$-smooth functi
 ons\, plus a  nonsmooth regularizer. The proposed NonconvEx primal-dual Sp
 liTTing (NESTT) algorithm splits the problem into $N$ subproblems\, and ut
 ilizes an augmented Lagrangian based primal-dual scheme to solve it in a d
 istributed and stochastic manner. With a special non-uniform sampling\, a 
 version of NESTT achieves $\\epsilon$-stationary solution  using $\\mathca
 l{O}((\\sum{i=1}^N\\sqrt{Li/N})^2/\\epsilon)$ gradient evaluations\, which
  can be up to $\\mathcal{O}(N)$ times better than the (proximal) gradient 
 descent methods. It also achieves Q-linear convergence rate for nonconvex 
 $\\ell1$ penalized quadratic problems with polyhedral constraints. Further
 \, we reveal  a fundamental connection between {\\it primal-dual} based me
 thods and a few {\\it primal only} methods such as IAG/SAG/SAGA.
LOCATION:Area 5+6+7+8 #29
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsour
 cing | Farshad Lahouti \, Babak Hassibi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Fundamental Limits of Budget-Fidelity Trade-off in Labe
 l Crowdsourcing\nFarshad Lahouti \, Babak Hassibi\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7063\n\nDigital crowdsourcing (CS) is a modern
  approach to perform certain large projects using small contributions of a
  large crowd. In CS\, a taskmaster typically breaks down the project into 
 small batches of tasks and assigns them to so-called workers with imperfec
 t skill levels. The crowdsourcer then collects and analyzes the results fo
 r inference and serving the purpose of the project. In this work\, the CS 
 problem\, as a human-in-the-loop computation problem\, is modeled and anal
 yzed in an information theoretic rate-distortion framework. The purpose is
  to identify the ultimate fidelity that one can achieve by any form of que
 ry from the crowd and any decoding (inference) algorithm with a given budg
 et. The results are established by a joint source channel (de)coding schem
 e\, which represent the query scheme and inference\, over parallel noisy c
 hannels\, which model workers with imperfect skill levels. We also present
  and analyze a query scheme dubbed k-ary incidence coding and study optimi
 zed query pricing in this setting.
LOCATION:Area 5+6+7+8 #30
END:VEVENT
BEGIN:VEVENT
SUMMARY:Supervised Learning with Tensor Networks | Edwin Stoudenmire \, Da
 vid J Schwab
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Supervised Learning with Tensor Networks\nEdwin Stouden
 mire \, David J Schwab\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7064\n\nTensor networks are approximations of high-order tensors which ar
 e efficient to work with and have been very successful for physics and mat
 hematics applications. We demonstrate how algorithms for optimizing tensor
  networks can be adapted to supervised learning tasks by using matrix prod
 uct states (tensor trains) to parameterize non-linear kernel learning mode
 ls. For the MNIST data set we obtain less than 1% test set classification 
 error. We discuss an interpretation of the additional structure imparted b
 y the tensor network to the learned model.
LOCATION:Area 5+6+7+8 #31
END:VEVENT
BEGIN:VEVENT
SUMMARY:Understanding Probabilistic Sparse Gaussian Process Approximations
  | Matthias Bauer \, Mark van der Wilk \, Carl Edward Rasmussen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Understanding Probabilistic Sparse Gaussian Process App
 roximations\nMatthias Bauer \, Mark van der Wilk \, Carl Edward Rasmussen\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7065\n\nGood sparse ap
 proximations are essential for practical inference in Gaussian Processes a
 s the computational cost of exact methods is prohibitive for large dataset
 s. The Fully Independent Training Conditional (FITC) and the Variational F
 ree Energy (VFE) approximations are two recent popular methods. Despite su
 perficial similarities\, these approximations have surprisingly different 
 theoretical properties and behave differently in practice. We thoroughly i
 nvestigate the two methods for regression both analytically and through il
 lustrative examples\, and draw conclusions to guide practical application.
LOCATION:Area 5+6+7+8 #32
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Locally Adaptive Normal Distribution | Georgios Arvanitidis \, L
 ars K Hansen \, Søren Hauberg
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Locally Adaptive Normal Distribution\nGeorgios Arvani
 tidis \, Lars K Hansen \, Søren Hauberg\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7066\n\nThe multivariate normal density is a monotonic 
 function of the distance to the mean\, and its ellipsoidal shape is due to
  the underlying Euclidean metric. We suggest to replace this metric with a
  locally adaptive\, smoothly changing (Riemannian) metric that favors regi
 ons of high local density. The resulting locally adaptive normal distribut
 ion (LAND) is a generalization of the normal distribution to the "manifold
 " setting\, where data is assumed to lie near a potentially low-dimensiona
 l manifold embedded in R^D. The LAND is parametric\, depending only on a m
 ean and a covariance\, and is the maximum entropy distribution under the g
 iven metric. The underlying metric is\, however\, non-parametric. We devel
 op a maximum likelihood algorithm to infer the distribution parameters tha
 t relies on a combination of gradient descent and Monte Carlo integration.
  We further extend the LAND to mixture models\, and provide the correspond
 ing EM algorithm. We demonstrate the efficiency of the LAND to fit non-tri
 vial probability distributions over both synthetic data\, and EEG measurem
 ents of human sleep.
LOCATION:Area 5+6+7+8 #33
END:VEVENT
BEGIN:VEVENT
SUMMARY:Anchor-Free Correlated Topic Modeling: Identifiability and Algorit
 hm | Kejun Huang \, Xiao Fu \, Nikolaos D. Sidiropoulos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Anchor-Free Correlated Topic Modeling: Identifiability 
 and Algorithm\nKejun Huang \, Xiao Fu \, Nikolaos D. Sidiropoulos\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=7067\n\nIn topic modeling\, ma
 ny algorithms that guarantee identifiability of the topics have been devel
 oped under the premise that there exist anchor words -- i.e.\, words that 
 only appear (with positive probability) in one topic. Follow-up work has r
 esorted to three or higher-order statistics of the data corpus to relax th
 e anchor word assumption. Reliable estimates of higher-order statistics ar
 e hard to obtain\, however\, and the identification of topics under those 
 models hinges on uncorrelatedness of the topics\, which can be unrealistic
 . This paper revisits topic modeling based on second-order moments\, and p
 roposes an anchor-free topic mining framework. The proposed approach guara
 ntees the identification of the topics under a much milder condition compa
 red to the anchor-word assumption\, thereby exhibiting much better robustn
 ess in practice. The associated algorithm only involves one eigen-decompos
 ition and a few small linear programs. This makes it easy to implement and
  scale up to very large problem instances. Experiments using the TDT2 and 
 Reuters-21578 corpus demonstrate that the proposed anchor-free approach ex
 hibits very favorable performance (measured using coherence\, similarity c
 ount\, and clustering accuracy metrics) compared to the prior art.
LOCATION:Area 5+6+7+8 #34
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Learning for Multi-pass Stochastic Gradient Methods | Junh
 ong Lin \, Lorenzo Rosasco
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimal Learning for Multi-pass Stochastic Gradient Met
 hods\nJunhong Lin \, Lorenzo Rosasco\nhttp://nips.cc/Conferences/2016/Sche
 dule?showEvent=7068\n\nWe analyze the learning  properties of the stochast
 ic gradient method when multiple passes over the data and mini-batches are
  allowed. In particular\, we consider the square loss and show that    for
   a universal step-size choice\, the number of passes acts as a regulariza
 tion parameter\, and optimal finite sample bounds  can be achieved by earl
 y-stopping. Moreover\, we show that larger step-sizes are allowed when con
 sidering mini-batches. Our analysis is based on  a unifying approach\, enc
 ompassing both batch and stochastic gradient methods as special cases.
LOCATION:Area 5+6+7+8 #35
END:VEVENT
BEGIN:VEVENT
SUMMARY:Contextual semibandits via supervised learning oracles | Akshay Kr
 ishnamurthy \, Alekh Agarwal \, Miro Dudik
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Contextual semibandits via supervised learning oracles\
 nAkshay Krishnamurthy \, Alekh Agarwal \, Miro Dudik\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7069\n\nWe study an online decision making 
 problem where on each round a learner chooses a list of items based on som
 e side information\, receives a scalar feedback value for each individual 
 item\, and a reward that is linearly related to this feedback. These probl
 ems\, known as contextual semibandits\, arise in crowdsourcing\, recommend
 ation\, and many other domains. This paper reduces contextual semibandits 
 to supervised learning\, allowing us to leverage powerful supervised learn
 ing methods in this partial-feedback setting. Our first reduction applies 
 when the mapping from feedback to reward is known and leads to a computati
 onally efficient algorithm with near-optimal regret. We show that this alg
 orithm outperforms state-of-the-art approaches on real-world learning-to-r
 ank datasets\, demonstrating the advantage of oracle-based algorithms. Our
  second reduction applies to the previously unstudied setting when the lin
 ear mapping from feedback to reward is unknown. Our regret guarantees are 
 superior to prior techniques that ignore the feedback.
LOCATION:Area 5+6+7+8 #36
END:VEVENT
BEGIN:VEVENT
SUMMARY:One-vs-Each Approximation to Softmax for Scalable Estimation of Pr
 obabilities | Michalis Titsias RC AUEB
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:One-vs-Each Approximation to Softmax for Scalable Estim
 ation of Probabilities\nMichalis Titsias RC AUEB\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=7070\n\nThe softmax representation of probabili
 ties for categorical variables plays a prominent role in modern machine le
 arning with numerous applications in areas such as large scale classificat
 ion\, neural language modeling and recommendation systems. However\, softm
 ax estimation is very expensive for large scale inference because of the h
 igh cost associated with computing the normalizing constant. Here\, we int
 roduce an efficient approximation to softmax probabilities which takes the
  form of a rigorous lower bound on the exact probability. This bound is ex
 pressed as a product over pairwise probabilities and it leads to scalable 
 estimation based on stochastic optimization. It allows us to perform doubl
 y stochastic estimation by subsampling both training instances and class l
 abels. We show that the new bound has interesting theoretical properties a
 nd we demonstrate its use in classification problems.
LOCATION:Area 5+6+7+8 #37
END:VEVENT
BEGIN:VEVENT
SUMMARY:Satisfying Real-world Goals with Dataset Constraints | Gabriel Goh
  \, Andrew Cotter \, Maya Gupta \, Michael P Friedlander
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Satisfying Real-world Goals with Dataset Constraints\nG
 abriel Goh \, Andrew Cotter \, Maya Gupta \, Michael P Friedlander\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7071\n\nThe goal of minimizin
 g misclassification error on a training set is often just one of several r
 eal-world goals that might be defined on different datasets. For example\,
  one may require a classifier to also make positive predictions at some sp
 ecified rate for some subpopulation (fairness)\, or to achieve a specified
  empirical recall. Other real-world goals include reducing churn with resp
 ect to a previously deployed model\, or stabilizing online training. In th
 is paper we propose handling multiple goals on multiple datasets by traini
 ng with dataset constraints\, using the ramp penalty to accurately quantif
 y costs\, and present an efficient algorithm to approximately optimize the
  resulting non-convex constrained optimization problem. Experiments on bot
 h benchmark and real-world industry datasets demonstrate the effectiveness
  of our approach.
LOCATION:Area 5+6+7+8 #38
END:VEVENT
BEGIN:VEVENT
SUMMARY:Blind Regression: Nonparametric Regression for Latent Variable Mod
 els via Collaborative Filtering | Dogyoon Song \, Christina E. Lee \, Yihu
 a Li \, Devavrat Shah
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Blind Regression: Nonparametric Regression for Latent V
 ariable Models via Collaborative Filtering\nDogyoon Song \, Christina E. L
 ee \, Yihua Li \, Devavrat Shah\nhttp://nips.cc/Conferences/2016/Schedule?
 showEvent=7072\n\nWe introduce the framework of {\\em blind regression} mo
 tivated by {\\em matrix completion} for recommendation systems: given $m$ 
 users\, $n$ movies\, and a subset of user-movie ratings\, the goal is to p
 redict the unobserved user-movie ratings given the data\, i.e.\, to comple
 te the partially observed matrix. Following the framework of non-parametri
 c statistics\, we posit that user $u$ and movie $i$ have features $x1(u)$ 
 and $x2(i)$ respectively\, and their corresponding rating $y(u\,i)$ is a n
 oisy measurement of $f(x1(u)\, x2(i))$ for some unknown function $f$. In c
 ontrast with classical regression\, the features $x = (x1(u)\, x2(i))$ are
  not observed\, making it challenging to apply standard regression methods
  to  predict the unobserved ratings.  Inspired by the classical Taylor's e
 xpansion for differentiable functions\, we provide a prediction algorithm 
 that is consistent for all Lipschitz functions. In fact\, the analysis thr
 ough our framework naturally leads to a variant of collaborative filtering
 \, shedding insight into the widespread success of collaborative filtering
  in practice. Assuming each entry is sampled independently with probabilit
 y at least $\\max(m^{-1+\\delta}\,n^{-1/2+\\delta})$ with $\\delta &gt\; 0
 $\, we prove that the expected fraction of our estimates with error greate
 r than $\\epsilon$ is less than $\\gamma^2 / \\epsilon^2$ plus a polynomia
 lly decaying term\, where $\\gamma^2$ is the variance of the additive entr
 y-wise noise term.  Experiments with the MovieLens and Netflix datasets su
 ggest that our algorithm provides principled improvements over basic colla
 borative filtering and is competitive with matrix factorization methods.
LOCATION:Area 5+6+7+8 #39
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generative Adversarial Imitation Learning | Jonathan Ho \, Stefano
  Ermon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Generative Adversarial Imitation Learning\nJonathan Ho 
 \, Stefano Ermon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7073\
 n\nConsider learning a policy from example expert behavior\, without inter
 action with the expert or access to a reinforcement signal. One approach i
 s to recover the expert's cost function with inverse reinforcement learnin
 g\, then extract a policy from that cost function with reinforcement learn
 ing. This approach is indirect and can be slow. We propose a new general f
 ramework for directly extracting a policy from data as if it were obtained
  by reinforcement learning following inverse reinforcement learning. We sh
 ow that a certain instantiation of our framework draws an analogy between 
 imitation learning and generative adversarial networks\, from which we der
 ive a model-free imitation learning algorithm that obtains significant per
 formance gains over existing model-free  methods in imitating complex beha
 viors in large\, high-dimensional environments.
LOCATION:Area 5+6+7+8 #40
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast Active Set Methods for Online Spike Inference from Calcium Im
 aging | Johannes Friedrich \, Liam Paninski
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Fast Active Set Methods for Online Spike Inference from
  Calcium Imaging\nJohannes Friedrich \, Liam Paninski\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=7074\n\nFluorescent calcium indicators are
  a popular means for observing the spiking activity of large neuronal popu
 lations. Unfortunately\, extracting the spike train of each neuron from ra
 w fluorescence calcium imaging data is a nontrivial problem. We present a 
 fast online active set method to solve this sparse nonnegative deconvoluti
 on problem. Importantly\, the algorithm progresses through each time serie
 s sequentially from beginning to end\, thus enabling real-time online spik
 e inference during the imaging session. Our algorithm is a generalization 
 of the pool adjacent violators algorithm (PAVA) for isotonic regression an
 d inherits its linear-time computational complexity. We gain remarkable in
 creases in processing speed:  more than one order of magnitude compared to
  currently employed state of the art convex solvers relying on interior po
 int methods. Our method can exploit warm starts\; therefore optimizing mod
 el hyperparameters only requires a handful of passes through the data. The
  algorithm enables real-time simultaneous deconvolution of $O(10^5)$ trace
 s of whole-brain zebrafish imaging data on a laptop.
LOCATION:Area 5+6+7+8 #41
END:VEVENT
BEGIN:VEVENT
SUMMARY:Path-Normalized Optimization of Recurrent Neural Networks with ReL
 U Activations | Behnam Neyshabur \, Yuhuai Wu \, Ruslan Salakhutdinov \, N
 ati Srebro
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Path-Normalized Optimization of Recurrent Neural Networ
 ks with ReLU Activations\nBehnam Neyshabur \, Yuhuai Wu \, Ruslan Salakhut
 dinov \, Nati Srebro\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7
 075\n\nWe investigate the parameter-space geometry of recurrent neural net
 works (RNNs)\, and develop an adaptation of path-SGD optimization method\,
  attuned to this geometry\, that can learn plain RNNs with ReLU activation
 s. On several datasets that require capturing long-term dependency structu
 re\, we show that path-SGD can significantly improve trainability of ReLU 
 RNNs compared to RNNs trained with SGD\, even with various recently sugges
 ted initialization schemes.
LOCATION:Area 5+6+7+8 #42
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improved Regret Bounds for Oracle-Based Adversarial Contextual Ban
 dits | Vasilis Syrgkanis \, Haipeng Luo \, Akshay Krishnamurthy \, Robert 
 Schapire
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Improved Regret Bounds for Oracle-Based Adversarial Con
 textual Bandits\nVasilis Syrgkanis \, Haipeng Luo \, Akshay Krishnamurthy 
 \, Robert Schapire\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=707
 6\n\nWe propose a new oracle-based algorithm\, BISTRO+\, for the adversari
 al contextual bandit problem\, where either contexts are drawn i.i.d. or t
 he sequence of contexts is known a priori\, but where the losses are picke
 d adversarially. Our algorithm is computationally efficient\, assuming acc
 ess to an offline optimization oracle\, and enjoys a regret of order $O((K
 T)^{\\frac{2}{3}}(\\log N)^{\\frac{1}{3}})$\, where $K$ is the number of a
 ctions\, $T$ is the number of iterations\, and $N$ is the number of baseli
 ne policies. Our result is the first to break the $O(T^{\\frac{3}{4}})$ ba
 rrier achieved by recent algorithms\, which was left as a major open probl
 em. Our analysis employs the recent relaxation framework of (Rakhlin and S
 ridharan\, ICML'16).
LOCATION:Area 5+6+7+8 #43
END:VEVENT
BEGIN:VEVENT
SUMMARY:Diffusion-Convolutional Neural Networks | James Atwood \, Don Tows
 ley
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Diffusion-Convolutional Neural Networks\nJames Atwood \
 , Don Towsley\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7077\n\n
 We present diffusion-convolutional neural networks (DCNNs)\, a new model f
 or graph-structured data.  Through the introduction of a diffusion-convolu
 tion operation\, we show how diffusion-based representations can be learne
 d from graph-structured data and used as an effective basis for node class
 ification. DCNNs have several attractive qualities\, including a latent re
 presentation for graphical data that is invariant under isomorphism\, as w
 ell as polynomial-time prediction and learning that can be represented as 
 tensor operations and efficiently implemented on a GPU.  Through several e
 xperiments with real structured datasets\, we demonstrate that DCNNs are a
 ble to  outperform probabilistic relational models and kernel-on-graph met
 hods at relational node classification tasks.
LOCATION:Area 5+6+7+8 #44
END:VEVENT
BEGIN:VEVENT
SUMMARY:Faster Projection-free Convex Optimization over the Spectrahedron 
 | Dan Garber \, Dan Garber
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Faster Projection-free Convex Optimization over the Spe
 ctrahedron\nDan Garber \, Dan Garber\nhttp://nips.cc/Conferences/2016/Sche
 dule?showEvent=7078\n\nMinimizing a convex function over the spectrahedron
 \, i.e.\, the set of all $d\\times d$ positive semidefinite matrices with 
 unit trace\, is an important optimization task with many applications in o
 ptimization\, machine learning\, and signal processing. It is also notorio
 usly difficult to solve in large-scale since standard techniques require t
 o compute expensive matrix decompositions. An alternative\, is the conditi
 onal gradient method (aka Frank-Wolfe algorithm) that regained much intere
 st in recent years\, mostly due to its application to this specific settin
 g. The key benefit of the CG method is that it avoids expensive matrix dec
 ompositions all together\, and simply requires a single eigenvector comput
 ation per iteration\, which is much more efficient. On the downside\, the 
 CG method\, in general\, converges with an inferior rate. The error for mi
 nimizing a $\\beta$-smooth function after $t$ iterations scales like $\\be
 ta/t$. This rate does not improve even if the function is also strongly co
 nvex. In this work we present a modification of the CG method tailored for
  the spectrahedron. The per-iteration complexity of the method is essentia
 lly identical to that of the standard CG method: only a single eigenvecor 
 computation is required. For minimizing an $\\alpha$-strongly convex and $
 \\beta$-smooth function\, the \\textit{expected} error of the method after
  $t$ iterations is: $O\\left({\\min{\\frac{\\beta{}}{t} \,\\left({\\frac{\
 \beta\\sqrt{\\rank(\\X^)}}{\\alpha^{1/4}t}}\\right)^{4/3}\, \\left({\\frac
 {\\beta}{\\sqrt{\\alpha}\\lambda_{\\min}(\\X^)t}}\\right)^{2}}}\\right)$. 
 Beyond the significant improvement in convergence rate\,  it also follows 
 that when the optimum is low-rank\, our method provides better accuracy-ra
 nk tradeoff than the standard CG method. To the best of our knowledge\, th
 is is the first result that attains provably faster convergence rates for 
 a CG variant for optimization over the spectrahedron. We also present enco
 uraging preliminary empirical results.
LOCATION:Area 5+6+7+8 #45
END:VEVENT
BEGIN:VEVENT
SUMMARY:Structured Matrix Recovery via the Generalized Dantzig Selector | 
 Sheng Chen \, Arindam Banerjee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Structured Matrix Recovery via the Generalized Dantzig 
 Selector\nSheng Chen \, Arindam Banerjee\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7079\n\nIn recent years\, structured matrix recovery pr
 oblems have gained considerable attention for its real world applications\
 , such as recommender systems and computer vision. Much of the existing wo
 rk has focused on matrices with low-rank structure\, and limited progress 
 has been made on matrices with other types of structure. In this paper we 
 present non-asymptotic analysis for estimation of generally structured mat
 rices via the generalized Dantzig selector based on sub-Gaussian measureme
 nts. We show that the estimation error can always be succinctly expressed 
 in terms of a few geometric measures such as Gaussian widths of suitable s
 ets associated with the structure of the underlying true matrix. Further\,
  we derive general bounds on these geometric measures for structures chara
 cterized by unitarily invariant norms\, a large family covering most matri
 x norms of practical interest. Examples are provided to illustrate the uti
 lity of our theoretical development.
LOCATION:Area 5+6+7+8 #46
END:VEVENT
BEGIN:VEVENT
SUMMARY:Convex Two-Layer Modeling with Latent Structure | Vignesh Ganapath
 iraman \, Xinhua Zhang \, Yaoliang Yu \, Junfeng Wen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Convex Two-Layer Modeling with Latent Structure\nVignes
 h Ganapathiraman \, Xinhua Zhang \, Yaoliang Yu \, Junfeng Wen\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7080\n\nUnsupervised learning of 
 structured predictors has been a long standing pursuit in machine learning
 .  Recently a conditional random field auto-encoder has been proposed in a
  two-layer setting\, allowing latent structured representation to be autom
 atically inferred.  Aside from being nonconvex\, it also requires the dema
 nding inference of normalization.  In this paper\, we develop a convex rel
 axation of two-layer conditional model which captures latent structure and
  estimates model parameters\, jointly and optimally.  We further expand it
 s applicability by resorting to a weaker form of inference---maximum a-pos
 teriori.  The flexibility of the model is demonstrated on two structures b
 ased on total unimodularity---graph matching and linear chain.  Experiment
 al results confirm the promise of the method.
LOCATION:Area 5+6+7+8 #47
END:VEVENT
BEGIN:VEVENT
SUMMARY:Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functio
 nal Estimators | Shashank Singh \, Barnabas Poczos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Finite-Sample Analysis of Fixed-k Nearest Neighbor Dens
 ity Functional Estimators\nShashank Singh \, Barnabas Poczos\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=7081\n\nWe provide finite-sample an
 alysis of a general framework for using k-nearest neighbor statistics to e
 stimate functionals of a nonparametric continuous probability density\, in
 cluding entropies and divergences. Rather than plugging a consistent densi
 ty estimate (which requires k → ∞ as the sample size n → ∞) into t
 he functional of interest\, the estimators we consider fix k and perform a
  bias correction. This can be more efficient computationally\, and\, as we
  show\, statistically\, leading to faster convergence rates. Our framework
  unifies several previous estimators\, for most of which ours are the firs
 t finite sample guarantees.
LOCATION:Area 5+6+7+8 #48
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Learning Games | Dale Schuurmans \, Martin A Zinkevich
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Deep Learning Games\nDale Schuurmans \, Martin A Zinkev
 ich\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7082\n\nWe investi
 gate a reduction of supervised learning to game playing that reveals new c
 onnections and learning methods.  For convex one-layer problems\, we demon
 strate an equivalence between global minimizers of the training problem an
 d Nash equilibria in a simple game.  We then show how the game can be exte
 nded to general acyclic neural networks with differentiable convex gates\,
  establishing a bijection between the Nash equilibria and critical (or KKT
 ) points of the deep learning problem.  Based on these connections we inve
 stigate alternative learning methods\, and find that regret matching can a
 chieve competitive training performance while producing sparser models tha
 n current deep learning approaches.
LOCATION:Area 5+6+7+8 #49
END:VEVENT
BEGIN:VEVENT
SUMMARY:“Congruent” and “Opposite” Neurons: Sisters for Multisenso
 ry Integration and Segregation | Wen-Hao Zhang \, He Wang \, K. Y. Michael
  Wong \, Si Wu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:“Congruent” and “Opposite” Neurons: Sisters for
  Multisensory Integration and Segregation\nWen-Hao Zhang \, He Wang \, K. 
 Y. Michael Wong \, Si Wu\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7083\n\nExperiments reveal that in the dorsal medial superior temporal 
 (MSTd) and the ventral intraparietal (VIP) areas\, where visual and vestib
 ular cues are integrated to infer heading direction\, there are two types 
 of neurons with roughly the same number. One is “congruent” cells\, wh
 ose preferred heading directions are similar in response to visual and ves
 tibular cues\; and the other is “opposite” cells\, whose preferred hea
 ding directions are nearly “opposite” (with an offset of 180 degree) i
 n response to visual vs. vestibular cues. Congruent neurons are known to b
 e responsible for cue integration\, but the computational role of opposite
  neurons remains largely unknown. Here\, we propose that opposite neurons 
 may serve to encode the disparity information between cues necessary for m
 ultisensory segregation. We build a computational model composed of two re
 ciprocally coupled modules\, MSTd and VIP\, and each module consists of gr
 oups of congruent and opposite neurons. In the model\, congruent neurons i
 n two modules are reciprocally connected with each other in the congruent 
 manner\, whereas opposite neurons are reciprocally connected in the opposi
 te manner. Mimicking the experimental protocol\, our model reproduces the 
 characteristics of congruent and opposite neurons\, and demonstrates that 
 in each module\, the sisters of congruent and opposite neurons can jointly
  achieve optimal multisensory information integration and segregation. Thi
 s study sheds light on our understanding of how the brain implements optim
 al multisensory integration and segregation concurrently in a distributed 
 manner.
LOCATION:Area 5+6+7+8 #50
END:VEVENT
BEGIN:VEVENT
SUMMARY:Statistical Inference for Cluster Trees | Jisu KIM \, Yen-Chi Chen
  \, Sivaraman Balakrishnan \, Alessandro Rinaldo \, Larry Wasserman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Statistical Inference for Cluster Trees\nJisu KIM \, Ye
 n-Chi Chen \, Sivaraman Balakrishnan \, Alessandro Rinaldo \, Larry Wasser
 man\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7084\n\nA cluster 
 tree provides an intuitive summary of a density function that reveals esse
 ntial structure about the high-density clusters. The true cluster tree is 
 estimated from a finite sample from an unknown true density. This paper ad
 dresses the basic question of quantifying our  uncertainty by assessing th
 e statistical significance of different features of an empirical cluster t
 ree. We first study a variety of metrics that can be used to compare diffe
 rent trees\, analyzing their properties and assessing their suitability fo
 r our inference task. We then propose methods to construct and summarize c
 onfidence sets for the unknown true cluster tree. We introduce a partial o
 rdering on cluster trees which we use to prune some of the statistically i
 nsignificant features of the empirical tree\, yielding interpretable and p
 arsimonious cluster trees. Finally\, we provide a variety of simulations t
 o illustrate our proposed methods and furthermore demonstrate their utilit
 y in the analysis of a Graft-versus-Host Disease (GvHD) data set.
LOCATION:Area 5+6+7+8 #51
END:VEVENT
BEGIN:VEVENT
SUMMARY:Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria i
 n Continuous Zero-Sum Games | Maximilian Balandat \, Walid Krichene \, Cla
 ire Tomlin \, Alexandre Bayen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Minimizing Regret on Reflexive Banach Spaces and Nash E
 quilibria in Continuous Zero-Sum Games\nMaximilian Balandat \, Walid Krich
 ene \, Claire Tomlin \, Alexandre Bayen\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7085\n\nWe study a general adversarial online learning p
 roblem\, in which we are given a decision set X' in a reflexive Banach spa
 ce X and a sequence of reward vectors in the dual space of X. At each iter
 ation\, we choose an action from X'\, based on the observed sequence of pr
 evious rewards. Our goal is to minimize regret\, defined as the gap betwee
 n the realized reward and the reward of the best fixed action in hindsight
 . Using results from infinite dimensional convex analysis\, we generalize 
 the method of Dual Averaging (or Follow the Regularized Leader) to our set
 ting and obtain upper bounds on the worst-case regret that generalize many
  previous results. Under the assumption of uniformly continuous rewards\, 
 we obtain explicit regret bounds in a setting where the decision set is th
 e set of probability distributions on a compact metric space S. Importantl
 y\, we make no convexity assumptions on either the set S or the reward fun
 ctions. We also prove a general lower bound on the worst-case regret for a
 ny online algorithm. We then apply these results to the problem of learnin
 g in repeated two-player zero-sum games on compact metric spaces. In doing
  so\, we first prove that if both players play a Hannan-consistent strateg
 y\, then with probability 1 the empirical distributions of play weakly con
 verge to the set of Nash equilibria of the game. We then show that\, under
  mild assumptions\, Dual Averaging on the (infinite-dimensional) space of 
 probability distributions indeed achieves Hannan-consistency.
LOCATION:Area 5+6+7+8 #52
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Neural Transducer | Navdeep Jaitly \, Quoc V Le \, Oriol Vinyals
  \, Ilya Sutskever \, David Sussillo \, Samy Bengio
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Neural Transducer\nNavdeep Jaitly \, Quoc V Le \, Ori
 ol Vinyals \, Ilya Sutskever \, David Sussillo \, Samy Bengio\nhttp://nips
 .cc/Conferences/2016/Schedule?showEvent=7086\n\nSequence-to-sequence model
 s have achieved impressive results on various tasks. However\, they are un
 suitable for tasks that require incremental predictions to be made as more
  data arrives or tasks that have long input sequences and output sequences
 . This is because they generate an output sequence conditioned on an entir
 e input sequence. In this paper\, we present a Neural Transducer that can 
 make incremental predictions as more input arrives\, without redoing the e
 ntire computation. Unlike sequence-to-sequence models\, the Neural Transdu
 cer computes the next-step distribution conditioned on the partially obser
 ved input sequence and the partially generated sequence. At each time step
 \, the transducer can decide to emit zero to many output symbols. The data
  can be processed using an encoder and presented as input to the transduce
 r. The discrete decision to emit a symbol at every time step makes it diff
 icult to learn with conventional backpropagation. It is however possible t
 o train the transducer by using a dynamic programming algorithm to generat
 e target discrete decisions. Our experiments show that the Neural Transduc
 er works well in settings where it is required to produce output predictio
 ns as data come in. We also find that the Neural Transducer performs well 
 for long sequences even when attention mechanisms are not used.
LOCATION:Area 5+6+7+8 #53
END:VEVENT
BEGIN:VEVENT
SUMMARY:Feature selection in functional data classification with recursive
  maxima hunting | José L. Torrecilla \, Alberto Suárez
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Feature selection in functional data classification wit
 h recursive maxima hunting\nJosé L. Torrecilla \, Alberto Suárez\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7087\n\nDimensionality reduct
 ion is one of the key issues in the design of effective machine learning m
 ethods for automatic induction.  In this work\, we introduce recursive max
 ima hunting (RMH) for variable selection in classification problems with f
 unctional data. In this context\, variable selection techniques are especi
 ally attractive because they reduce the dimensionality\, facilitate the in
 terpretation and can improve the accuracy of the predictive models. The me
 thod\, which is a recursive extension of maxima hunting (MH)\, performs va
 riable selection by identifying the maxima of a relevance function\, which
  measures the strength of the correlation of the predictor functional vari
 able with the class label. At each stage\, the information associated with
  the selected variable is removed by subtracting the conditional expectati
 on of the process. The results of an extensive empirical evaluation are us
 ed to illustrate that\, in the problems investigated\, RMH has comparable 
 or higher predictive accuracy than standard simensionality reduction techn
 iques\, such as PCA and PLS\, and state-of-the-art feature selection metho
 ds for functional data\, such as maxima hunting.
LOCATION:Area 5+6+7+8 #54
END:VEVENT
BEGIN:VEVENT
SUMMARY:Homotopy Smoothing for Non-Smooth Problems with Lower Complexity t
 han $O(1/\\epsilon)$ | Yi Xu \, Yan Yan \, Qihang Lin \, Tianbao Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Homotopy Smoothing for Non-Smooth Problems with Lower C
 omplexity than $O(1/\\epsilon)$\nYi Xu \, Yan Yan \, Qihang Lin \, Tianbao
  Yang\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7088\n\nIn this 
 paper\, we develop a novel {\\bf ho}moto{\\bf p}y  {\\bf s}moothing (HOPS)
  algorithm for solving a family of non-smooth problems that is composed of
  a non-smooth term with an explicit max-structure and  a smooth term or  a
  simple non-smooth term whose proximal mapping is easy to compute. The bes
 t known iteration complexity for solving such non-smooth optimization prob
 lems is $O(1/\\epsilon)$ without any assumption on the strong convexity. I
 n this work\, we will show that the proposed  HOPS achieved a lower iterat
 ion complexity of $\\tilde O(1/\\epsilon^{1-\\theta})$ with $\\theta\\in(0
 \,1]$ capturing the local sharpness of the objective function around the o
 ptimal solutions. To the best of our knowledge\, this is the lowest iterat
 ion complexity achieved so far for the considered non-smooth optimization 
 problems without strong convexity assumption.  The HOPS algorithm employs 
 Nesterov's smoothing technique and Nesterov's accelerated gradient method 
 and runs in stages\, which gradually decreases the smoothing parameter in 
 a stage-wise manner until it yields a sufficiently good approximation of t
 he original function. We show that HOPS enjoys a linear convergence for ma
 ny well-known non-smooth problems (e.g.\, empirical risk minimization with
  a piece-wise linear loss function and $\\ell_1$ norm regularizer\, findin
 g a point in a polyhedron\, cone programming\, etc). Experimental results 
 verify the effectiveness of HOPS in comparison with Nesterov's smoothing a
 lgorithm and the primal-dual style of first-order methods.
LOCATION:Area 5+6+7+8 #55
END:VEVENT
BEGIN:VEVENT
SUMMARY:Nested Mini-Batch K-Means | James Newling \, François Fleuret
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Nested Mini-Batch K-Means\nJames Newling \, François F
 leuret\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7089\n\nA new a
 lgorithm is proposed which accelerates the mini-batch k-means algorithm of
  Sculley (2010) by using the distance bounding approach of Elkan (2003). W
 e argue that\, when incorporating distance bounds into a mini-batch algori
 thm\, already used data should preferentially be reused. To this end we pr
 opose using nested mini-batches\, whereby data in a mini-batch at iteratio
 n t is automatically reused at iteration t+1.   Using nested mini-batches 
 presents two difficulties. The first is that unbalanced use of data can bi
 as estimates\, which we resolve by ensuring that each data sample contribu
 tes exactly once to centroids. The second is in choosing mini-batch sizes\
 , which we address by balancing premature fine-tuning of centroids with re
 dundancy induced slow-down. Experiments show that the resulting nmbatch al
 gorithm is very effective\, often arriving within 1\\% of the empirical mi
 nimum 100 times earlier than the standard mini-batch algorithm.
LOCATION:Area 5+6+7+8 #56
END:VEVENT
BEGIN:VEVENT
SUMMARY:Density Estimation via Discrepancy Based Adaptive Sequential Parti
 tion | Dangna Li \, Kun Yang \, Wing Hung Wong
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Density Estimation via Discrepancy Based Adaptive Seque
 ntial Partition\nDangna Li \, Kun Yang \, Wing Hung Wong\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7090\n\nGiven $iid$ observations from a
 n unknown continuous distribution defined on some domain $\\Omega$\, we pr
 opose a nonparametric method to learn a piecewise constant function to app
 roximate the underlying probability density function. Our density estimate
  is a piecewise constant function defined on a binary partition of $\\Omeg
 a$.  The key ingredient of the algorithm is to use discrepancy\, a concept
  originates from Quasi Monte Carlo analysis\, to control the partition pro
 cess. The resulting algorithm is simple\, efficient\, and has  provable co
 nvergence rate. We demonstrate empirically its efficiency as a density est
 imation method. We also show how it can be utilized to find good initializ
 ations for k-means.
LOCATION:Area 5+6+7+8 #57
END:VEVENT
BEGIN:VEVENT
SUMMARY:Budgeted stream-based active learning via adaptive submodular maxi
 mization | Kaito Fujii \, Hisashi Kashima
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Budgeted stream-based active learning via adaptive subm
 odular maximization\nKaito Fujii \, Hisashi Kashima\nhttp://nips.cc/Confer
 ences/2016/Schedule?showEvent=7091\n\nActive learning enables us to reduce
  the annotation cost by adaptively selecting unlabeled instances to be lab
 eled. For pool-based active learning\, several effective methods with theo
 retical guarantees have been developed through maximizing some utility fun
 ction satisfying adaptive submodularity. In contrast\, there have been few
  methods for stream-based active learning based on adaptive submodularity.
  In this paper\, we propose a new class of utility functions\, policy-adap
 tive submodular functions\, and prove this class includes many existing ad
 aptive submodular functions appearing in real world problems. We provide a
  general framework based on policy-adaptive submodularity that makes it po
 ssible to convert existing pool-based methods to stream-based methods and 
 give theoretical guarantees on their performance. In addition we empirical
 ly demonstrate their effectiveness comparing with existing heuristics on c
 ommon benchmark datasets.
LOCATION:Area 5+6+7+8 #58
END:VEVENT
BEGIN:VEVENT
SUMMARY:Lifelong Learning with Weighted Majority Votes | Anastasia Pentina
  \, Ruth Urner
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Lifelong Learning with Weighted Majority Votes\nAnastas
 ia Pentina \, Ruth Urner\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7092\n\nBetter understanding of the potential benefits of information t
 ransfer and representation learning is an important step towards the goal 
 of building intelligent systems that are able to persist in the world and 
 learn over time. In this work\, we consider a setting where the learner en
 counters a stream of tasks but is able to retain only limited information 
 from each encountered task\, such as a learned predictor. In contrast to m
 ost previous works analyzing this scenario\, we do not make any distributi
 onal assumptions on the task generating process. Instead\, we formulate a 
 complexity measure that captures the diversity of the observed tasks. We p
 rovide a lifelong learning algorithm with error guarantees for every obser
 ved task (rather than on average). We show sample complexity reductions in
  comparison to solving every task in isolation in terms of our task comple
 xity measure. Further\, our algorithmic framework can naturally be viewed 
 as learning a representation from encountered tasks with a neural network.
LOCATION:Area 5+6+7+8 #59
END:VEVENT
BEGIN:VEVENT
SUMMARY:How Deep is the Feature Analysis underlying Rapid Visual Categoriz
 ation? | Sven Eberhardt \, Jonah G Cader \, Thomas Serre
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:How Deep is the Feature Analysis underlying Rapid Visua
 l Categorization?\nSven Eberhardt \, Jonah G Cader \, Thomas Serre\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7093\n\nRapid categorization 
 paradigms have a long history in experimental psychology: Characterized by
  short presentation times and speeded behavioral responses\, these tasks h
 ighlight the efficiency with which our visual system processes natural obj
 ect categories. Previous studies have shown that feed-forward hierarchical
  models of the visual cortex provide a good fit to human visual decisions.
  At the same time\, recent work in computer vision has demonstrated signif
 icant gains in object recognition accuracy with increasingly deep hierarch
 ical architectures. But it is unclear how well these models account for hu
 man visual decisions and what they may reveal about the underlying brain p
 rocesses.      We have conducted a large-scale psychophysics study to asse
 ss the correlation between computational models and human behavioral respo
 nses on a rapid animal vs. non-animal categorization task. We considered v
 isual representations of varying complexity by analyzing the output of dif
 ferent stages of processing in three state-of-the-art deep networks. We fo
 und that recognition accuracy increases with higher stages of visual proce
 ssing (higher level stages indeed outperforming human participants on the 
 same task) but that human decisions agree best with predictions from inter
 mediate stages.       Overall\, these results suggest that human participa
 nts may rely on visual features of intermediate complexity and that the co
 mplexity of visual representations afforded by modern deep network models 
 may exceed the complexity of those used by human participants during rapid
  categorization.
LOCATION:Area 5+6+7+8 #60
END:VEVENT
BEGIN:VEVENT
SUMMARY:Incremental Boosting Convolutional Neural Network for Facial Actio
 n Unit Recognition | Shizhong Han \, Zibo Meng \, AHMED-SHEHAB KHAN \, Yan
  Tong
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Incremental Boosting Convolutional Neural Network for F
 acial Action Unit Recognition\nShizhong Han \, Zibo Meng \, AHMED-SHEHAB K
 HAN \, Yan Tong\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7094\n
 \nRecognizing facial action units (AUs) from spontaneous facial expression
 s is still a challenging problem. Most recently\, CNNs have shown promise 
 on facial AU recognition. However\, the learned CNNs are often overfitted 
 and do not generalize well to unseen subjects due to limited AU-coded trai
 ning images. We proposed a novel Incremental Boosting CNN (IB-CNN) to inte
 grate boosting into the CNN via an incremental boosting layer that selects
  discriminative neurons from the lower layer and is incrementally updated 
 on successive mini-batches. In addition\, a novel loss function that accou
 nts for errors from both the incremental boosted classifier and individual
  weak classifiers was proposed to fine-tune the IB-CNN. Experimental resul
 ts on four benchmark AU databases have demonstrated that the IB-CNN yields
  significant improvement over the traditional CNN and the boosting CNN wit
 hout incremental learning\, as well as outperforming the state-of-the-art 
 CNN-based methods in AU recognition. The improvement is more impressive fo
 r the AUs that have the lowest frequencies in the databases.
LOCATION:Area 5+6+7+8 #61
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multivariate tests of association based on univariate tests | Ruth
  Heller \, Yair Heller
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Multivariate tests of association based on univariate t
 ests\nRuth Heller \, Yair Heller\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7095\n\nFor testing two vector random variables for independenc
 e\, we  propose testing whether the distance of one vector from an arbitra
 ry  center point is independent from the distance of the other vector from
  another arbitrary center point by a univariate test. We prove that under 
 minimal assumptions\, it is enough to have a consistent univariate indepen
 dence test on the distances\, to guarantee that the power to detect depend
 ence between the random vectors increases to one with sample size.  If the
  univariate test is  distribution-free\, the multivariate test will also b
 e distribution-free. If we consider multiple center points and aggregate t
 he  center-specific univariate tests\, the power may be further improved\,
  and the resulting multivariate test may be distribution-free for specific
  aggregation methods (if the univariate test is distribution-free).  We sh
 ow that certain multivariate tests recently proposed in the literature can
  be viewed as instances of this general approach. Moreover\, we show in ex
 periments that novel tests constructed using our approach can have better 
 power and computational time than competing approaches.
LOCATION:Area 5+6+7+8 #62
END:VEVENT
BEGIN:VEVENT
SUMMARY:SURGE: Surface Regularized Geometry Estimation from a Single Image
  | Peng Wang \, Xiaohui Shen \, Bryan Russell \, Scott Cohen \, Brian Pric
 e \, Alan L Yuille
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:SURGE: Surface Regularized Geometry Estimation from a S
 ingle Image\nPeng Wang \, Xiaohui Shen \, Bryan Russell \, Scott Cohen \, 
 Brian Price \, Alan L Yuille\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7096\n\nThis paper introduces an approach to regularize 2.5D surfac
 e normal and depth predictions at each pixel given a single input image. T
 he approach infers and reasons about the underlying 3D planar surfaces dep
 icted in the image to snap predicted normals and depths to inferred planar
  surfaces\, all while maintaining fine detail within objects. Our approach
  comprises two components: (i) a fourstream convolutional neural network (
 CNN) where depths\, surface normals\, and likelihoods of planar region and
  planar boundary are predicted at each pixel\, followed by (ii) a dense co
 nditional random field (DCRF) that integrates the four predictions such th
 at the normals and depths are compatible with each other and regularized b
 y the planar region and planar boundary information. The DCRF is formulate
 d such that gradients can be passed to the surface normal and depth CNNs v
 ia backpropagation. In addition\, we propose new planar wise metrics to ev
 aluate geometry consistency within planar surfaces\, which are more tightl
 y related to dependent 3D editing applications. We show that our regulariz
 ation yields a 30% relative improvement in planar consistency on the NYU v
 2 dataset.
LOCATION:Area 5+6+7+8 #63
END:VEVENT
BEGIN:VEVENT
SUMMARY:Memory-Efficient Backpropagation Through Time | Audrunas Gruslys \
 , Remi Munos \, Ivo Danihelka \, Marc Lanctot \, Alex Graves
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Memory-Efficient Backpropagation Through Time\nAudrunas
  Gruslys \, Remi Munos \, Ivo Danihelka \, Marc Lanctot \, Alex Graves\nht
 tp://nips.cc/Conferences/2016/Schedule?showEvent=7097\n\nWe propose a nove
 l approach to reduce memory consumption of the backpropagation through tim
 e (BPTT) algorithm when training recurrent neural networks (RNNs). Our app
 roach uses dynamic programming to balance a trade-off between caching of i
 ntermediate results and recomputation. The algorithm is capable of tightly
  fitting within almost any user-set memory budget while finding an optimal
  execution policy minimizing the computational cost. Computational devices
  have limited memory capacity and maximizing a computational performance g
 iven a fixed memory budget is a practical use-case. We provide asymptotic 
 computational upper bounds for various regimes. The algorithm is particula
 rly effective for long sequences. For sequences of length 1000\, our algor
 ithm saves 95\\% of memory usage while using only one third more time per 
 iteration than the standard BPTT.
LOCATION:Area 5+6+7+8 #64
END:VEVENT
BEGIN:VEVENT
SUMMARY:Scan Order in Gibbs Sampling: Models in Which it Matters and Bound
 s on How Much | Bryan He \, Christopher M De Sa \, Ioannis Mitliagkas \, C
 hristopher Ré
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Scan Order in Gibbs Sampling: Models in Which it Matter
 s and Bounds on How Much\nBryan He \, Christopher M De Sa \, Ioannis Mitli
 agkas \, Christopher Ré\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7098\n\nGibbs sampling is a Markov Chain Monte Carlo sampling technique
  that iteratively samples variables from their conditional distributions. 
 There are two common scan orders for the variables: random scan and system
 atic scan. Due to the benefits of locality in hardware\, systematic scan i
 s commonly used\, even though most statistical guarantees are only for ran
 dom scan. While it has been conjectured that the mixing times of random sc
 an and systematic scan do not differ by more than a logarithmic factor\, w
 e show by counterexample that this is not the case\, and we prove that tha
 t the mixing times do not differ by more than a polynomial factor under mi
 ld conditions. To prove these relative bounds\, we introduce a method of a
 ugmenting the state space to study systematic scan using conductance.
LOCATION:Area 5+6+7+8 #65
END:VEVENT
BEGIN:VEVENT
SUMMARY:LightRNN: Memory and Computation-Efficient Recurrent Neural Networ
 ks | Xiang Li \, Tao Qin \, Jian Yang \, Xiaolin Hu \, Tieyan Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:LightRNN: Memory and Computation-Efficient Recurrent Ne
 ural Networks\nXiang Li \, Tao Qin \, Jian Yang \, Xiaolin Hu \, Tieyan Li
 u\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7099\n\nRecurrent ne
 ural networks (RNNs) have achieved state-of-the-art performances in many n
 atural language processing tasks\, such as language modeling and machine t
 ranslation. However\, when the vocabulary is large\, the RNN model will be
 come very big (e.g.\, possibly beyond the memory capacity of a GPU device)
  and its training will become very inefficient. In this work\, we propose 
 a novel technique to tackle this challenge. The key idea is to use 2-Compo
 nent (2C) shared embedding for word representations. We allocate every wor
 d in the vocabulary into a table\, each row of which is associated with a 
 vector\, and each column associated with another vector. Depending on its 
 position in the table\, a word is jointly represented by two components: a
  row vector and a column vector. Since the words in the same row share the
  row vector and the words in the same column share the column vector\, we 
 only need $2 \\sqrt{|V|}$ vectors to represent a vocabulary of $|V|$ uniqu
 e words\, which are far less than the $|V|$ vectors required by existing a
 pproaches. Based on the 2-Component shared embedding\, we design a new RNN
  algorithm and evaluate it using the language modeling task on several ben
 chmark datasets. The results show that our algorithm significantly reduces
  the model size and speeds up the training process\, without sacrifice of 
 accuracy (it achieves similar\, if not better\, perplexity as compared to 
 state-of-the-art language models). Remarkably\, on the One-Billion-Word be
 nchmark Dataset\, our algorithm achieves comparable perplexity to previous
  language models\, whilst reducing the model size by a factor of 40-100\, 
 and speeding up the training process by a factor of 2. We name our propose
 d algorithm \\emph{LightRNN} to reflect its very small model size and very
  high training speed.
LOCATION:Area 5+6+7+8 #66
END:VEVENT
BEGIN:VEVENT
SUMMARY:Direct Feedback Alignment Provides Learning in Deep Neural Network
 s | Arild Nøkland
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Direct Feedback Alignment Provides Learning in Deep Neu
 ral Networks\nArild Nøkland\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7100\n\nArtificial neural networks are most commonly trained with t
 he back-propagation algorithm\, where the gradient for learning is provide
 d by back-propagating the error\, layer by layer\, from the output layer t
 o the hidden layers. A recently discovered method called feedback-alignmen
 t shows that the weights used for propagating the error backward don't hav
 e to be symmetric with the weights used for propagation the activation for
 ward. In fact\, random feedback weights work evenly well\, because the net
 work learns how to make the feedback useful. In this work\, the feedback a
 lignment principle is used for training hidden layers more independently f
 rom the rest of the network\, and from a zero initial condition. The error
  is propagated through fixed random feedback connections directly from the
  output layer to each hidden layer. This simple method is able to achieve 
 zero training error even in convolutional networks and very deep networks\
 , completely without error back-propagation. The method is a step towards 
 biologically plausible machine learning because the error signal is almost
  local\, and no symmetric or reciprocal weights are required. Experiments 
 show that the test performance on MNIST and CIFAR is almost as good as tho
 se obtained with back-propagation for fully connected networks. If combine
 d with dropout\, the method achieves 1.45% error on the permutation invari
 ant MNIST task.
LOCATION:Area 5+6+7+8 #67
END:VEVENT
BEGIN:VEVENT
SUMMARY:Variational Bayes on Monte Carlo Steroids | Aditya Grover \, Stefa
 no Ermon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Variational Bayes on Monte Carlo Steroids\nAditya Grove
 r \, Stefano Ermon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=710
 1\n\nVariational approaches are often used to approximate intractable post
 eriors or normalization constants in hierarchical latent variable models. 
 While often effective in practice\, it is known that the approximation err
 or can be arbitrarily large.  We propose a new class of bounds on the marg
 inal log-likelihood of directed latent variable models. Our approach relie
 s on random projections to simplify the posterior. In contrast to standard
  variational methods\, our bounds are guaranteed to be tight with high pro
 bability. We provide a new approach for learning latent variable models ba
 sed on optimizing our new bounds on the log-likelihood. We demonstrate emp
 irical improvements on benchmark datasets in vision and language for sigmo
 id belief networks\, where a neural network is used to approximate the pos
 terior.
LOCATION:Area 5+6+7+8 #68
END:VEVENT
BEGIN:VEVENT
SUMMARY:Agnostic Estimation for Misspecified Phase Retrieval Models | Mate
 y Neykov \, Zhaoran Wang \, Han Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Agnostic Estimation for Misspecified Phase Retrieval Mo
 dels\nMatey Neykov \, Zhaoran Wang \, Han Liu\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=7102\n\nThe goal of noisy high-dimensional phase r
 etrieval is to estimate an $s$-sparse parameter $\\boldsymbol{\\beta}^\\in
  \\mathbb{R}^d$ from $n$ realizations of the model $Y = (\\boldsymbol{X}^{
 \\top} \\boldsymbol{\\beta}^)^2 + \\varepsilon$. Based on this model\, we 
 propose a significant semi-parametric generalization called misspecified p
 hase retrieval (MPR)\, in which $Y = f(\\boldsymbol{X}^{\\top}\\boldsymbol
 {\\beta}^\, \\varepsilon)$ with unknown $f$ and $\\operatorname{Cov}(Y\, (
 \\boldsymbol{X}^{\\top}\\boldsymbol{\\beta}^)^2) &gt\; 0$. For example\, M
 PR encompasses $Y = h(|\\boldsymbol{X}^{\\top} \\boldsymbol{\\beta}^|) + \
 \varepsilon$ with increasing $h$ as a special case. Despite the generality
  of the MPR model\, it eludes the reach of most existing semi-parametric e
 stimators. In this paper\, we propose an estimation procedure\, which cons
 ists of solving a cascade of two convex programs and provably recovers the
  direction of $\\boldsymbol{\\beta}^$. Our theory is backed up by thorough
  numerical results.
LOCATION:Area 5+6+7+8 #69
END:VEVENT
BEGIN:VEVENT
SUMMARY:Following the Leader and Fast Rates in Linear Prediction: Curved C
 onstraint Sets and Other Regularities | Ruitong Huang \, Tor Lattimore \, 
 András György \, Csaba Szepesvari
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Following the Leader and Fast Rates in Linear Predictio
 n: Curved Constraint Sets and Other Regularities\nRuitong Huang \, Tor Lat
 timore \, András György \, Csaba Szepesvari\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=7103\n\nThe follow the leader (FTL) algorithm\, pe
 rhaps the simplest of all online learning algorithms\, is known to perform
  well when the loss functions it is used on are positively curved. In this
  paper we ask whether there are other "lucky" settings when FTL achieves s
 ublinear\, "small" regret. In particular\, we study the fundamental proble
 m of linear prediction over a non-empty convex\, compact domain. Amongst o
 ther results\, we prove that the curvature of  the boundary of the domain 
 can act as if the losses were curved: In this case\, we prove that as long
  as the mean of the loss vectors have positive lengths bounded away from z
 ero\, FTL enjoys a logarithmic growth rate of regret\, while\, e.g.\, for 
 polyhedral domains and stochastic data it enjoys finite expected regret. B
 uilding on a previously known meta-algorithm\, we also get an algorithm th
 at simultaneously enjoys the worst-case guarantees and the bound available
  for FTL.
LOCATION:Area 5+6+7+8 #70
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combining Fully Convolutional and Recurrent Neural Networks for 3D
  Biomedical Image Segmentation | Jianxu Chen \, Lin Yang \, Yizhe Zhang \,
  Mark Alber \, Danny Z Chen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Combining Fully Convolutional and Recurrent Neural Netw
 orks for 3D Biomedical Image Segmentation\nJianxu Chen \, Lin Yang \, Yizh
 e Zhang \, Mark Alber \, Danny Z Chen\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=7104\n\nSegmentation of 3D images is a fundamental problem
  in biomedical image analysis. Deep learning (DL) approaches have achieved
  the state-of-the-art segmentation performance. To exploit the 3D contexts
  using neural networks\, known DL segmentation methods\, including 3D conv
 olution\, 2D convolution on the planes orthogonal to 2D slices\, and LSTM 
 in multiple directions\, all suffer incompatibility with the highly anisot
 ropic dimensions in common 3D biomedical images. In this paper\, we propos
 e a new DL framework for 3D image segmentation\, based on a combination of
  a fully convolutional network (FCN) and a recurrent neural network (RNN)\
 , which are responsible for exploiting the intra-slice and inter-slice con
 texts\, respectively. To our best knowledge\, this is the first DL framewo
 rk for 3D image segmentation that explicitly leverages 3D image anisotropi
 sm. Evaluating using a dataset from the ISBI Neuronal Structure Segmentati
 on Challenge and in-house image stacks for 3D fungus segmentation\, our ap
 proach achieves promising results\, comparing to the known DL-based 3D seg
 mentation approaches.
LOCATION:Area 5+6+7+8 #71
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Product Cut | Thomas Laurent \, James von Brecht \, Xavier Bre
 sson \, arthur szlam
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:The Product Cut\nThomas Laurent \, James von Brecht \, 
 Xavier Bresson \, arthur szlam\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=7105\n\nWe introduce a theoretical and algorithmic framework for 
 multi-way graph partitioning that relies on a multiplicative cut-based obj
 ective. We refer to this objective as the Product Cut. We provide a detail
 ed investigation of the mathematical properties of this objective and an e
 ffective algorithm for its optimization. The proposed model has strong mat
 hematical underpinnings\, and the corresponding algorithm achieves state-o
 f-the-art performance on benchmark data sets.
LOCATION:Area 5+6+7+8 #72
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Gradient Methods for Distributionally Robust Optimizati
 on with f-divergences | Hongseok Namkoong \, John C Duchi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Stochastic Gradient Methods for Distributionally Robust
  Optimization with f-divergences\nHongseok Namkoong \, John C Duchi\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=7106\n\nWe develop efficient
  solution methods for a robust empirical risk minimization problem designe
 d to give calibrated confidence intervals on performance and provide optim
 al tradeoffs between bias and variance. Our methods apply to distributiona
 lly robust optimization problems proposed by Ben-Tal et al.\, which put mo
 re weight on observations inducing high loss via a worst-case approach ove
 r a non-parametric uncertainty set on the underlying data distribution. Ou
 r algorithm solves the resulting minimax problems with nearly the same com
 putational cost of stochastic gradient descent through the use of several 
 carefully designed data structures. For a sample of size n\, the per-itera
 tion cost of our method scales as O(log n)\, which allows us to give optim
 ality certificates that distributionally robust optimization provides at l
 ittle extra cost compared to empirical risk minimization and stochastic gr
 adient methods.
LOCATION:Area 5+6+7+8 #73
END:VEVENT
BEGIN:VEVENT
SUMMARY:Man is to Computer Programmer as Woman is to Homemaker? Debiasing 
 Word Embeddings | Tolga Bolukbasi \, Kai-Wei Chang \, James Y Zou \, Venka
 tesh Saligrama \, Adam T Kalai
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Man is to Computer Programmer as Woman is to Homemaker?
  Debiasing Word Embeddings\nTolga Bolukbasi \, Kai-Wei Chang \, James Y Zo
 u \, Venkatesh Saligrama \, Adam T Kalai\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7107\n\nThe blind application of machine learning runs 
 the risk of amplifying biases present in data. Such a danger is facing us 
 with word embedding\, a popular framework to represent text data as vector
 s which has been used in many machine learning and natural language proces
 sing tasks. We show that even word embeddings trained on Google News artic
 les exhibit female/male gender stereotypes to a disturbing extent. This ra
 ises concerns because their widespread use\, as we describe\, often tends 
 to amplify these biases. Geometrically\, gender bias is first shown to be 
 captured by a direction in the word embedding. Second\, gender neutral wor
 ds are shown to be linearly separable from gender definition words in the 
 word embedding. Using these properties\, we provide a methodology for modi
 fying an embedding to remove gender stereotypes\, such as the association 
 between the words receptionist and female\, while maintaining desired asso
 ciations such as between the words queen and female.  Using crowd-worker e
 valuation as well as standard benchmarks\, we empirically demonstrate that
  our algorithms significantly reduce gender bias in embeddings while prese
 rving the its useful properties such as the ability to cluster related con
 cepts and to solve analogy tasks. The resulting embeddings can be used in 
 applications without amplifying gender bias.
LOCATION:Area 5+6+7+8 #74
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal spectral transportation with application to music transcri
 ption | Rémi Flamary \, Cédric Févotte \, Nicolas Courty \, Valentin Em
 iya
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimal spectral transportation with application to mus
 ic transcription\nRémi Flamary \, Cédric Févotte \, Nicolas Courty \, V
 alentin Emiya\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7108\n\n
 Many spectral unmixing methods rely on the non-negative decomposition of s
 pectral data onto a dictionary of spectral templates. In particular\, stat
 e-of-the-art music transcription systems decompose the spectrogram of the 
 input signal onto a dictionary of representative note spectra. The typical
  measures of fit used to quantify the adequacy of the decomposition compar
 e the data and template entries frequency-wise. As such\, small displaceme
 nts of energy from a frequency bin to another as well as variations of tim
 ber can disproportionally harm the fit. We address these issues by means o
 f optimal transportation and propose a new measure of fit that treats the 
 frequency distributions of energy holistically as opposed to frequency-wis
 e. Building on the harmonic nature of sound\, the new measure is invariant
  to shifts of energy to harmonically-related frequencies\, as well as to s
 mall and local displacements of energy. Equipped with this new measure of 
 fit\, the dictionary of note templates can be considerably simplified to a
  set of Dirac vectors located at the target fundamental frequencies (music
 al pitch values). This in turns gives ground to a very fast and simple dec
 omposition algorithm that achieves state-of-the-art performance on real mu
 sical data.
LOCATION:Area 5+6+7+8 #75
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combining Adversarial Guarantees and Stochastic Fast Rates in Onli
 ne Learning | Wouter M Koolen \, Peter Grünwald \, Tim van Erven
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Combining Adversarial Guarantees and Stochastic Fast Ra
 tes in Online Learning\nWouter M Koolen \, Peter Grünwald \, Tim van Erve
 n\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7109\n\nWe consider 
 online learning algorithms that guarantee worst-case regret rates in adver
 sarial environments (so they can be deployed safely and will perform robus
 tly)\, yet adapt optimally to favorable stochastic environments (so they w
 ill perform well in a variety of settings of practical importance). We qua
 ntify the friendliness of stochastic environments by means of the well-kno
 wn Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recen
 t algorithms (Squint for the Hedge setting and MetaGrad for online convex 
 optimization) we show that the particular form of their data-dependent ind
 ividual-sequence regret guarantees implies that they adapt automatically t
 o the Bernstein parameters of the stochastic environment. We prove that th
 ese algorithms attain fast rates in their respective settings both in expe
 ctation and with high probability.
LOCATION:Area 5+6+7+8 #76
END:VEVENT
BEGIN:VEVENT
SUMMARY:Towards Conceptual Compression | Karol Gregor \, Frederic Besse \,
  Danilo Jimenez Rezende \, Ivo Danihelka \, Daan Wierstra
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Towards Conceptual Compression\nKarol Gregor \, Frederi
 c Besse \, Danilo Jimenez Rezende \, Ivo Danihelka \, Daan Wierstra\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=7110\n\nWe introduce convolu
 tional DRAW\, a homogeneous deep generative model achieving state-of-the-a
 rt performance in latent variable image modeling. The algorithm naturally 
 stratifies information into higher and lower level details\, creating abst
 ract features and as such addressing one of the fundamentally desired prop
 erties of representation learning. Furthermore\, the hierarchical ordering
  of its latents creates the opportunity to selectively store global inform
 ation about an image\, yielding a high quality 'conceptual compression' fr
 amework.
LOCATION:Area 5+6+7+8 #77
END:VEVENT
BEGIN:VEVENT
SUMMARY:Can Peripheral Representations Improve Clutter Metrics on Complex 
 Scenes? | Arturo Deza \, Miguel Eckstein
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Can Peripheral Representations Improve Clutter Metrics 
 on Complex Scenes?\nArturo Deza \, Miguel Eckstein\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7111\n\nPrevious studies have proposed image-
 based clutter measures that correlate with human search times and/or eye m
 ovements. However\, most models do not take into account the fact that the
  effects of clutter interact with the foveated nature of the human visual 
 system: visual clutter further from the fovea has an increasing detrimenta
 l influence on perception. Here\, we introduce a new foveated clutter mode
 l to predict the detrimental effects in target search utilizing a forced f
 ixation search task. We use Feature Congestion (Rosenholtz et al.) as our 
 non foveated clutter model\, and we stack a peripheral architecture on top
  of Feature Congestion for our foveated model. We introduce the Peripheral
  Integration Feature Congestion (PIFC) coefficient\, as a fundamental ingr
 edient of our model that modulates clutter as a non-linear gain contingent
  on eccentricity. We finally show that Foveated Feature Congestion (FFC) c
 lutter scores (r(44) = −0.82 ± 0.04\, p &lt\; 0.0001) correlate better 
 with target detection (hit rate) than regular Feature Congestion (r(44) = 
 −0.19 ± 0.13\, p = 0.0774) in forced fixation search\; and we extend fo
 veation to other clutter models showing stronger correlations in all cases
 . Thus\, our model allows us to enrich clutter perception research by comp
 uting fixation specific clutter maps. Code for building peripheral represe
 ntations is  available.
LOCATION:Area 5+6+7+8 #78
END:VEVENT
BEGIN:VEVENT
SUMMARY:GAP Safe Screening Rules for Sparse-Group Lasso | Eugene Ndiaye \,
  Olivier Fercoq \, Alexandre Gramfort \, Joseph Salmon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:GAP Safe Screening Rules for Sparse-Group Lasso\nEugene
  Ndiaye \, Olivier Fercoq \, Alexandre Gramfort \, Joseph Salmon\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7112\n\nFor statistical learnin
 g in high dimension\, sparse regularizations have proven useful to boost b
 oth computational and statistical efficiency. In some contexts\, it is nat
 ural to handle more refined structures than pure sparsity\, such as for in
 stance group sparsity. Sparse-Group Lasso has recently been introduced in 
 the context of linear regression to enforce sparsity both at the feature a
 nd at the group level. We propose the first (provably) safe screening rule
 s for Sparse-Group Lasso\, i.e.\, rules that allow to discard early in the
  solver features/groups that are inactive at optimal solution. Thanks to e
 fficient dual gap computations relying on the geometric properties of $\\e
 psilon$-norm\, safe screening rules for Sparse-Group Lasso lead to signifi
 cant gains in term of computing time for our coordinate descent implementa
 tion.
LOCATION:Area 5+6+7+8 #79
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Treewidth-Bounded Bayesian Networks with Thousands of Var
 iables | Mauro Scanagatta \, Giorgio Corani \, Cassio P de Campos \, Marco
  Zaffalon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning Treewidth-Bounded Bayesian Networks with Thous
 ands of Variables\nMauro Scanagatta \, Giorgio Corani \, Cassio P de Campo
 s \, Marco Zaffalon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=71
 13\n\nWe present a method for learning treewidth-bounded Bayesian networks
  from data sets containing thousands of variables. Bounding the treewidth 
 of a Bayesian network greatly reduces the complexity of inferences.  Yet\,
  being a global property of the graph\, it considerably increases the diff
 iculty of the learning process. Our novel algorithm accomplishes this task
 \, scaling both to large domains and to large treewidths. Our novel approa
 ch consistently outperforms the state of the art on experiments with up to
  thousands of variables.
LOCATION:Area 5+6+7+8 #80
END:VEVENT
BEGIN:VEVENT
SUMMARY:Ancestral Causal Inference | Sara Magliacane \, Tom Claassen \, Jo
 ris M Mooij
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Ancestral Causal Inference\nSara Magliacane \, Tom Claa
 ssen \, Joris M Mooij\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=
 7114\n\nConstraint-based causal discovery from limited data is a notorious
 ly difficult challenge due to the many borderline independence test decisi
 ons.  Several approaches to improve the reliability of the predictions by 
 exploiting redundancy in the independence information have been proposed r
 ecently. Though promising\, existing approaches can still be greatly impro
 ved in terms of accuracy and scalability. We present a novel method that r
 educes the combinatorial explosion of the search space by using a more coa
 rse-grained representation of causal information\, drastically reducing co
 mputation time. Additionally\, we propose a method to score causal predict
 ions based on their confidence. Crucially\, our implementation also allows
  one to easily combine observational and interventional data and to incorp
 orate various types of available background knowledge.  We prove soundness
  and asymptotic consistency of our method and demonstrate that it can outp
 erform the state-of-the-art on synthetic data\, achieving a speedup of sev
 eral orders of magnitude. We illustrate its practical feasibility by apply
 ing it on a challenging protein data set.
LOCATION:Area 5+6+7+8 #81
END:VEVENT
BEGIN:VEVENT
SUMMARY:Visual Question Answering with Question Representation Update (QRU
 ) | Ruiyu Li \, Jiaya Jia
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Visual Question Answering with Question Representation 
 Update (QRU)\nRuiyu Li \, Jiaya Jia\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7115\n\nOur method aims at reasoning over natural language q
 uestions and visual images. Given a natural language question about an ima
 ge\, our model updates the question representation iteratively by selectin
 g image regions relevant to the query and learns to give the correct answe
 r. Our model contains several reasoning layers\, exploiting complex visual
  relations in the visual question answering (VQA) task. The proposed netwo
 rk is end-to-end trainable through back-propagation\, where its weights ar
 e initialized using pre-trained convolutional neural network (CNN) and gat
 ed recurrent unit (GRU). Our method is evaluated on challenging datasets o
 f COCO-QA and VQA and yields state-of-the-art performance.
LOCATION:Area 5+6+7+8 #82
END:VEVENT
BEGIN:VEVENT
SUMMARY:Identification and Overidentification of Linear Structural Equatio
 n Models | Bryant Chen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Identification and Overidentification of Linear Structu
 ral Equation Models\nBryant Chen\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7116\n\nIn this paper\, we address the problems of identifying 
 linear structural equation models and discovering the constraints they imp
 ly. We first extend the half-trek criterion to cover a broader class of mo
 dels and apply our extension to finding testable constraints implied by th
 e model. We then show that any semi-Markovian linear model can be recursiv
 ely decomposed into simpler sub-models\, resulting in improved identificat
 ion and constraint discovery power. Finally\, we show that\, unlike the ex
 isting methods developed for linear models\, the resulting method subsumes
  the identification and constraint discovery algorithms for non-parametric
  models.
LOCATION:Area 5+6+7+8 #83
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Valid Optimal Assignment Kernels and Applications to Graph Clas
 sification | Nils M. Kriege \, Pierre-Louis Giscard \, Richard Wilson
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:On Valid Optimal Assignment Kernels and Applications to
  Graph Classification\nNils M. Kriege \, Pierre-Louis Giscard \, Richard W
 ilson\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7117\n\nThe succ
 ess of kernel methods has initiated the design of novel positive semidefin
 ite functions\, in particular for structured data. A leading design paradi
 gm for this is the convolution kernel\, which decomposes structured object
 s into their parts and sums over all pairs of parts. Assignment kernels\, 
 in contrast\, are obtained from an optimal bijection between parts\, which
  can provide a more valid notion of similarity. In general however\, optim
 al assignments yield indefinite functions\, which complicates their use in
  kernel methods. We characterize a class of base kernels used to compare p
 arts that guarantees positive semidefinite optimal assignment kernels. The
 se base kernels give rise to hierarchies from which the optimal assignment
  kernels are computed in linear time by histogram intersection. We apply t
 hese results by developing the Weisfeiler-Lehman optimal assignment kernel
  for graphs. It provides high classification accuracy on widely-used bench
 mark data sets improving over the original Weisfeiler-Lehman kernel.
LOCATION:Area 5+6+7+8 #84
END:VEVENT
BEGIN:VEVENT
SUMMARY:Constraints Based Convex Belief Propagation | Yaniv Tenzer \, Ale
 x Schwing \, Kevin Gimpel \, Tamir Hazan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Constraints Based Convex Belief Propagation\nYaniv Ten
 zer \, Alex Schwing \, Kevin Gimpel \, Tamir Hazan\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7118\n\nInference in Markov random fields sub
 ject to consistency structure is a fundamental problem that arises in many
  real-life applications. In order to enforce consistency\, classical appro
 aches utilize consistency potentials or encode constraints over feasible i
 nstances. Unfortunately this comes at the price of a serious computational
  bottleneck. In this paper we suggest to tackle consistency by incorporati
 ng constraints on beliefs. This permits derivation of a closed-form messag
 e-passing algorithm which we refer to as the Constraints Based Convex Beli
 ef Propagation (CBCBP). Experiments show that CBCBP outperforms the standa
 rd approach while being at least an order of magnitude faster.
LOCATION:Area 5+6+7+8 #85
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combinatorial Energy Learning for Image Segmentation | Jeremy B Ma
 itin-Shepard \, Viren Jain \, Michal Januszewski \, Peter Li \, Pieter Abb
 eel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Combinatorial Energy Learning for Image Segmentation\nJ
 eremy B Maitin-Shepard \, Viren Jain \, Michal Januszewski \, Peter Li \, 
 Pieter Abbeel\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7119\n\n
 We introduce a new machine learning approach for image segmentation that u
 ses a neural network to model the conditional energy of a segmentation giv
 en an image.  Our approach\, combinatorial energy learning for image segme
 ntation (CELIS) places a particular emphasis on modeling the inherent comb
 inatorial nature of dense image segmentation problems.  We propose efficie
 nt algorithms for learning deep neural networks to model the energy functi
 on\, and for local optimization of this energy in the space of supervoxel 
 agglomerations.  We extensively evaluate our method on a publicly availabl
 e 3-D microscopy dataset with 25 billion voxels of ground truth data. On a
 n 11 billion voxel test set\, we find that our method improves volumetric 
 reconstruction accuracy by more than 20% as compared to two state-of-the-a
 rt baseline methods: graph-based segmentation of the output of a 3-D convo
 lutional neural network trained to predict boundaries\, as well as a rando
 m forest classifier trained to agglomerate supervoxels that were generated
  by a 3-D convolutional neural network.
LOCATION:Area 5+6+7+8 #86
END:VEVENT
BEGIN:VEVENT
SUMMARY:A scalable end-to-end Gaussian process adapter for irregularly sam
 pled time series classification | Steven Cheng-Xian Li \, Benjamin M Marli
 n
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A scalable end-to-end Gaussian process adapter for irre
 gularly sampled time series classification\nSteven Cheng-Xian Li \, Benjam
 in M Marlin\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7120\n\nWe
  present a general framework for classification of sparse and irregularly-
 sampled time series. The properties of such time series can result in subs
 tantial uncertainty about the values of the underlying temporal processes\
 , while making the data difficult to deal with using standard classificati
 on methods that assume fixed-dimensional feature spaces. To address these 
 challenges\, we propose an uncertainty-aware classification framework base
 d on a special computational layer we refer to as the Gaussian process ada
 pter that can connect irregularly sampled time series data to any black-bo
 x classifier learnable using gradient descent. We show how to scale up the
  required computations based on combining the structured kernel interpolat
 ion framework and the Lanczos approximation method\, and how to discrimina
 tively train the Gaussian process adapter in combination with a number of 
 classifiers end-to-end using backpropagation.
LOCATION:Area 5+6+7+8 #87
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Variance Reduction Methods for Saddle-Point Problems | 
 Balamurugan Palaniappan \, Francis Bach
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Stochastic Variance Reduction Methods for Saddle-Point 
 Problems\nBalamurugan Palaniappan \, Francis Bach\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7121\n\nWe consider convex-concave saddle-poin
 t problems where the objective functions may be split in many components\,
  and extend recent stochastic variance reduction methods (such as SVRG or 
 SAGA) to provide the first  large-scale linearly convergent algorithms for
  this class of problems which are common in machine learning. While the al
 gorithmic extension is straightforward\, it comes with challenges and oppo
 rtunities: (a) the convex minimization analysis does not apply and we use 
 the notion of monotone operators to prove convergence\, showing in particu
 lar that the same algorithm applies to a larger class of problems\, such a
 s variational inequalities\,  (b) there are two notions of splits\, in ter
 ms of functions\, or in terms of partial derivatives\, (c) the split does 
 need to be done with convex-concave terms\, (d) non-uniform sampling is ke
 y to an efficient algorithm\, both in theory and practice\, and (e)  these
  incremental algorithms can be easily accelerated using a simple extension
  of the "catalyst" framework\,  leading to an algorithm which is always su
 perior to accelerated batch algorithms.
LOCATION:Area 5+6+7+8 #88
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dimensionality Reduction of Massive Sparse Datasets Using Coresets
  | Dan Feldman \, Mikhail Volkov \, Daniela Rus
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Dimensionality Reduction of Massive Sparse Datasets Usi
 ng Coresets\nDan Feldman \, Mikhail Volkov \, Daniela Rus\nhttp://nips.cc/
 Conferences/2016/Schedule?showEvent=7122\n\nIn this paper we present a pra
 ctical solution with performance guarantees to the problem of dimensionali
 ty reduction for very large scale sparse matrices.  We show applications o
 f our approach to computing the Principle Component Analysis (PCA) of any 
 $n\\times d$ matrix\, using one pass over the stream of its rows. Our solu
 tion uses coresets: a scaled subset of the $n$ rows that approximates thei
 r sum of squared distances to \\emph{every} $k$-dimensional \\emph{affine}
  subspace. An open theoretical problem has been to compute such a coreset 
 that is independent of both $n$ and $d$. An open practical problem has bee
 n to compute a non-trivial approximation to the PCA of very large but spar
 se databases such as the Wikipedia document-term matrix in a reasonable ti
 me. We answer both of these questions affirmatively. Our main technical re
 sult is a new framework for deterministic coreset constructions based on a
  reduction to the problem of counting items in a stream.
LOCATION:Area 5+6+7+8 #89
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient state-space modularization for planning: theory\, behavi
 oral and neural signatures | Daniel McNamee \, Daniel M Wolpert \, Mate Le
 ngyel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Efficient state-space modularization for planning: theo
 ry\, behavioral and neural signatures\nDaniel McNamee \, Daniel M Wolpert 
 \, Mate Lengyel\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7123\n
 \nEven in state-spaces of modest size\, planning is plagued by the “curs
 e of dimensionality”. This problem is particularly acute in human and an
 imal cognition given the limited capacity of working memory\, and the time
  pressures under which planning often occurs in the natural environment. H
 ierarchically organized modular representations have long been suggested t
 o underlie the capacity of biological systems to efficiently and flexibly 
 plan in complex environments. However\, the principles underlying efficien
 t modularization remain obscure\, making it difficult to identify its beha
 vioral and neural signatures. Here\, we develop a normative theory of effi
 cient state-space representations which partitions an environment into dis
 tinct modules by minimizing the average (information theoretic) descriptio
 n length of planning within the environment\, thereby optimally trading of
 f the complexity of planning across and within modules. We show that such 
 optimal representations provide a unifying account for a diverse range of 
 hitherto unrelated phenomena at multiple levels of behavior and neural rep
 resentation.
LOCATION:Area 5+6+7+8 #90
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Newton Method for Empirical Risk Minimization to Statisti
 cal Accuracy | Aryan Mokhtari \, Hadi Daneshmand \, Aurelien Lucchi \, Tho
 mas Hofmann \, Alejandro Ribeiro
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Adaptive Newton Method for Empirical Risk Minimization 
 to Statistical Accuracy\nAryan Mokhtari \, Hadi Daneshmand \, Aurelien Luc
 chi \, Thomas Hofmann \, Alejandro Ribeiro\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7124\n\nWe consider empirical risk minimization for l
 arge-scale datasets. We introduce Ada Newton as an adaptive algorithm that
  uses Newton's method with adaptive sample sizes. The main idea of Ada New
 ton is to increase the size of the training set by a factor larger than on
 e in a way that the minimization variable for the current training set is 
 in the local neighborhood of the optimal argument of the next training set
 . This allows to exploit the quadratic convergence property of Newton's me
 thod and reach the statistical accuracy of each training set with only one
  iteration of Newton's method. We show theoretically that we can iterative
 ly increase the sample size while applying single Newton iterations withou
 t line search and staying within the statistical accuracy of the regulariz
 ed empirical risk. In particular\, we can double the size of the training 
 set in each iteration when the number of samples is sufficiently large. Nu
 merical experiments on various datasets confirm the possibility of increas
 ing the sample size by factor 2 at each iteration which implies that Ada N
 ewton achieves the statistical accuracy of the full training set with abou
 t two passes over the dataset.
LOCATION:Area 5+6+7+8 #91
END:VEVENT
BEGIN:VEVENT
SUMMARY:RETAIN: An Interpretable Predictive Model for Healthcare using Rev
 erse Time Attention Mechanism | Edward Choi \, Mohammad Taha Bahadori \, J
 oshua Kulas \, Jimeng Sun \, Andy Schuetz \, Walter Stewart
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:RETAIN: An Interpretable Predictive Model for Healthcar
 e using Reverse Time Attention Mechanism\nEdward Choi \, Mohammad Taha Bah
 adori \, Joshua Kulas \, Jimeng Sun \, Andy Schuetz \, Walter Stewart\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7125\n\nAccuracy and inter
 pretability are two dominant features of successful predictive models. Typ
 ically\, a choice must be made in favor of complex black box models such a
 s recurrent neural networks (RNN) for accuracy versus less accurate but mo
 re interpretable traditional models such as logistic regression. This trad
 eoff poses challenges in medicine where both accuracy and interpretability
  are important. We addressed this challenge by developing the REverse Time
  AttentIoN model (RETAIN) for application to Electronic Health Records (EH
 R) data. RETAIN achieves high accuracy while remaining clinically interpre
 table and is based on a two-level neural attention model that detects infl
 uential past visits and significant clinical variables within those visits
  (e.g. key diagnoses). RETAIN mimics physician practice by attending the E
 HR data in a reverse time order so that recent clinical visits are likely 
 to receive higher attention. RETAIN was tested on a large health system EH
 R dataset with 14 million visits completed by 263K patients over an 8 year
  period and demonstrated predictive accuracy and computational scalability
  comparable to state-of-the-art methods such as RNN\, and ease of interpre
 tability comparable to traditional models.
LOCATION:Area 5+6+7+8 #92
END:VEVENT
BEGIN:VEVENT
SUMMARY:Joint quantile regression in vector-valued RKHSs | Maxime Sangnier
  \, Olivier Fercoq \, Florence d'Alché-Buc
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Joint quantile regression in vector-valued RKHSs\nMaxim
 e Sangnier \, Olivier Fercoq \, Florence d'Alché-Buc\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=7126\n\nAddressing the will to give a more
  complete picture than an average relationship provided by standard regres
 sion\, a novel framework for estimating and predicting simultaneously seve
 ral conditional quantiles is introduced. The proposed methodology leverage
 s kernel-based multi-task learning to curb the embarrassing phenomenon of 
 quantile crossing\, with a one-step estimation procedure and no post-proce
 ssing. Moreover\, this framework comes along with theoretical guarantees a
 nd an efficient coordinate descent learning algorithm. Numerical experimen
 ts on benchmark and real datasets highlight the enhancements of our approa
 ch regarding the prediction error\, the crossing occurrences and the train
 ing time.
LOCATION:Area 5+6+7+8 #93
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learnable Visual Markers | Oleg Grinchuk \, Vadim Lebedev \, Victo
 r Lempitsky
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learnable Visual Markers\nOleg Grinchuk \, Vadim Lebede
 v \, Victor Lempitsky\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=
 7127\n\nWe propose a new approach to designing visual markers (analogous t
 o QR-codes\, markers for augmented reality\, and robotic fiducial tags) ba
 sed on the advances in deep generative networks. In our approach\, the mar
 kers are obtained as color images synthesized by a deep network from input
  bit strings\, whereas another deep network is trained to recover the bit 
 strings back from the photos of these markers. The two networks are traine
 d simultaneously in a joint backpropagation process that takes characteris
 tic photometric and geometric distortions associated with marker fabricati
 on and capture into account. Additionally\, a stylization loss based on st
 atistics of activations in a pretrained classification network can be inse
 rted into the learning in order to shift the marker appearance towards som
 e texture prototype. In the experiments\, we demonstrate that the markers 
 obtained using our approach are capable of retaining bit strings that are 
 long enough to be practical. The ability to automatically adapt markers ac
 cording to the usage scenario and the desired capacity as well as the abil
 ity to combine information encoding with artistic stylization are the uniq
 ue properties of our approach. As a byproduct\, our approach provides an i
 nsight on the structure of patterns that are most suitable for recognition
  by ConvNets and on their ability to distinguish composite patterns.
LOCATION:Area 5+6+7+8 #94
END:VEVENT
BEGIN:VEVENT
SUMMARY:Exponential expressivity in deep neural networks through transient
  chaos | Ben Poole \, Subhaneil Lahiri \, Maithreyi Raghu \, Jascha Sohl-D
 ickstein \, Surya Ganguli
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Exponential expressivity in deep neural networks throug
 h transient chaos\nBen Poole \, Subhaneil Lahiri \, Maithreyi Raghu \, Jas
 cha Sohl-Dickstein \, Surya Ganguli\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7128\n\nWe combine Riemannian geometry with the mean field t
 heory of high dimensional chaos to study the nature of signal propagation 
 in deep neural networks with random weights. Our results reveal a phase tr
 ansition in the expressivity of random deep networks\, with networks in th
 e chaotic phase computing nonlinear functions whose global curvature grows
  exponentially with depth\, but not with width. We prove that this generic
  class of random functions cannot be efficiently computed by any shallow n
 etwork\, going beyond prior work that restricts their analysis to single f
 unctions. Moreover\, we formally quantify and demonstrate the long conject
 ured idea that deep networks can disentangle exponentially curved manifold
 s in input space into flat manifolds in hidden space.  Our theoretical fra
 mework for analyzing the expressive power of deep networks is broadly appl
 icable and provides a basis for quantifying previously abstract notions ab
 out the geometry of deep functions.
LOCATION:Area 5+6+7+8 #95
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Multiplicative Integration with Recurrent Neural Networks | Yuh
 uai Wu \, Saizheng Zhang \, Ying Zhang \, Yoshua Bengio \, Ruslan Salakhut
 dinov
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:On Multiplicative Integration with Recurrent Neural Net
 works\nYuhuai Wu \, Saizheng Zhang \, Ying Zhang \, Yoshua Bengio \, Rusla
 n Salakhutdinov\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7129\n
 \nWe introduce a general simple structural design called “Multiplicative
  Integration” (MI) to improve recurrent neural networks (RNNs). MI chang
 es the way of how the information flow gets integrated in the computationa
 l building block of an RNN\, while introducing almost no extra parameters.
  The new structure can be easily embedded into many popular RNN models\, i
 ncluding LSTMs and GRUs. We empirically analyze its learning behaviour and
  conduct evaluations on several tasks using different RNN models. Our expe
 rimental results demonstrate that Multiplicative Integration can provide a
  substantial performance boost over many of the existing RNN models.
LOCATION:Area 5+6+7+8 #96
END:VEVENT
BEGIN:VEVENT
SUMMARY:Interpretable Nonlinear Dynamic Modeling of Neural Trajectories | 
 Yuan Zhao \, Il Memming Park
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Interpretable Nonlinear Dynamic Modeling of Neural Traj
 ectories\nYuan Zhao \, Il Memming Park\nhttp://nips.cc/Conferences/2016/Sc
 hedule?showEvent=7130\n\nA central challenge in neuroscience is understand
 ing how neural system implements computation through its dynamics. We prop
 ose a nonlinear time series model aimed at characterizing interpretable dy
 namics from neural trajectories. Our model assumes low-dimensional continu
 ous dynamics in a finite volume. It incorporates a prior assumption about 
 globally contractional dynamics to avoid overly enthusiastic extrapolation
  outside of the support of observed trajectories. We show that our model c
 an recover qualitative features of the phase portrait such as attractors\,
  slow points\, and bifurcations\, while also producing reliable long-term 
 future predictions in a variety of dynamical models and in real neural dat
 a.
LOCATION:Area 5+6+7+8 #97
END:VEVENT
BEGIN:VEVENT
SUMMARY:Globally Optimal Training of Generalized Polynomial Neural Network
 s with Nonlinear Spectral Methods | Antoine Gautier \, Quynh N Nguyen \, M
 atthias Hein
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Globally Optimal Training of Generalized Polynomial Neu
 ral Networks with Nonlinear Spectral Methods\nAntoine Gautier \, Quynh N N
 guyen \, Matthias Hein\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7131\n\nThe optimization problem behind neural networks is highly non-con
 vex. Training with stochastic gradient descent and variants requires caref
 ul parameter tuning and provides no guarantee to achieve the global optimu
 m. In contrast we show under quite weak assumptions on the data that a par
 ticular class of feedforward  neural networks can be trained globally opti
 mal with a linear convergence rate. Up to our knowledge this is the first 
 practically feasible method which achieves such a guarantee. While the met
 hod can in principle be applied to deep networks\, we restrict ourselves f
 or simplicity in this paper to one- and two hidden layer networks. Our exp
 eriments confirms that these models are already rich enough to achieve goo
 d performance on a series of real-world datasets.
LOCATION:Area 5+6+7+8 #98
END:VEVENT
BEGIN:VEVENT
SUMMARY:Linear Feature Encoding for Reinforcement Learning | Zhao Song \, 
 Ronald E Parr \, Xuejun Liao \, Lawrence Carin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Linear Feature Encoding for Reinforcement Learning\nZha
 o Song \, Ronald E Parr \, Xuejun Liao \, Lawrence Carin\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7132\n\nFeature construction is of vita
 l importance in reinforcement learning\, as the quality of a value functio
 n or policy is largely determined by the corresponding features. The recen
 t successes of deep reinforcement learning (RL) only increase the importan
 ce of understanding feature construction. Typical deep RL approaches use a
  linear output layer\, which means that deep RL can be interpreted as a fe
 ature construction/encoding network followed by linear value function appr
 oximation. This paper develops and evaluates a theory of linear feature en
 coding. We extend theoretical results on feature quality for linear value 
 function approximation from the uncontrolled case to the controlled case. 
 We then develop a supervised linear feature encoding method that is motiva
 ted by insights from linear value function approximation theory\, as well 
 as empirical successes from deep RL. The resulting encoder is a surprising
 ly effective method for linear value function approximation using raw imag
 es as inputs.
LOCATION:Area 5+6+7+8 #99
END:VEVENT
BEGIN:VEVENT
SUMMARY:Graphical Time Warping for Joint Alignment of Multiple Curves | Yi
 zhi Wang \, David J Miller \, Kira Poskanzer \, Yue Wang \, Lin Tian \, Gu
 oqiang Yu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Graphical Time Warping for Joint Alignment of Multiple 
 Curves\nYizhi Wang \, David J Miller \, Kira Poskanzer \, Yue Wang \, Lin 
 Tian \, Guoqiang Yu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=71
 33\n\nDynamic time warping (DTW) is a fundamental technique in time series
  analysis for comparing one curve to another using a flexible time-warping
  function. However\, it was designed to compare a single pair of curves. I
 n many applications\, such as in metabolomics and image series analysis\, 
 alignment is simultaneously needed for multiple pairs. Because the underly
 ing warping functions are often related\, independent application of DTW t
 o each pair is a sub-optimal solution. Yet\, it is largely unknown how to 
 efficiently conduct a joint alignment with all warping functions simultane
 ously considered\, since any given warping function is constrained by the 
 others and dynamic programming cannot be applied. In this paper\, we show 
 that the joint alignment problem can be transformed into a network flow pr
 oblem and thus can be exactly and efficiently solved by the max flow algor
 ithm\, with a guarantee of global optimality. We name the proposed approac
 h graphical time warping (GTW)\, emphasizing the graphical nature of the s
 olution and that the dependency structure of the warping functions can be 
 represented by a graph. Modifications of DTW\, such as windowing and weigh
 ting\, are readily derivable within GTW. We also discuss optimal tuning of
  parameters and hyperparameters in GTW. We illustrate the power of GTW usi
 ng both synthetic data and a real case study of an astrocyte calcium movie
 .
LOCATION:Area 5+6+7+8 #100
END:VEVENT
BEGIN:VEVENT
SUMMARY:Mixed Linear Regression with Multiple Components | Kai Zhong \, Pr
 ateek Jain \, Inderjit S Dhillon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Mixed Linear Regression with Multiple Components\nKai Z
 hong \, Prateek Jain \, Inderjit S Dhillon\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7134\n\nIn this paper\, we study the mixed linear reg
 ression (MLR) problem\, where the goal is to recover multiple underlying l
 inear models from their unlabeled linear measurements. We propose a non-co
 nvex objective function which we show is {\\em locally strongly convex} in
  the neighborhood of the ground truth. We use a tensor method for initiali
 zation so that the initial models are in the local strong convexity region
 . We then employ general convex optimization algorithms to minimize the ob
 jective function. To the best of our knowledge\, our approach provides fir
 st exact recovery guarantees for the MLR problem with $K \\geq 2$ componen
 ts. Moreover\,  our method has near-optimal computational complexity $\\ti
 lde O (Nd)$ as well as near-optimal sample complexity $\\tilde O (d)$ for 
 {\\em constant} $K$. Furthermore\, we show that our non-convex formulation
  can be extended to solving the {\\em subspace clustering} problem as well
 . In particular\, when initialized within a small constant distance to the
  true subspaces\, our method converges to the global optima (and recovers 
 true subspaces) in time {\\em linear} in the number of points. Furthermore
 \, our empirical results indicate that even with random initialization\, o
 ur approach converges to the global optima in linear time\, providing spee
 d-up of up to two orders of magnitude.
LOCATION:Area 5+6+7+8 #101
END:VEVENT
BEGIN:VEVENT
SUMMARY:Statistical Inference for Pairwise Graphical Models Using Score Ma
 tching | Ming Yu \, Mladen Kolar \, Varun Gupta
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Statistical Inference for Pairwise Graphical Models Usi
 ng Score Matching\nMing Yu \, Mladen Kolar \, Varun Gupta\nhttp://nips.cc/
 Conferences/2016/Schedule?showEvent=7135\n\nProbabilistic graphical models
  have been widely used to model complex systems and aid scientific discove
 ries. As a result\, there is a large body of literature focused on consist
 ent model selection. However\, scientists are often interested in understa
 nding uncertainty associated with the estimated parameters\, which current
  literature has not addressed thoroughly. In this paper\, we propose a nov
 el estimator for edge parameters for pairwise graphical models based on Hy
 v\\"arinen scoring rule. Hyv\\"arinen scoring rule is especially useful in
  cases where the normalizing constant cannot be  obtained efficiently in a
  closed form. We prove that the estimator is $\\sqrt{n}$-consistent and as
 ymptotically Normal. This result allows us to construct confidence interva
 ls for edge parameters\, as well as\, hypothesis tests.  We establish our 
 results under conditions that are typically assumed in the literature for 
 consistent estimation. However\, we do not require that the estimator cons
 istently recovers the graph structure. In particular\, we prove that the a
 symptotic distribution of the estimator is robust to model selection mista
 kes and uniformly valid for a large number of data-generating processes. W
 e illustrate validity of our estimator through extensive simulation studie
 s.
LOCATION:Area 5+6+7+8 #102
END:VEVENT
BEGIN:VEVENT
SUMMARY:Hardness of Online Sleeping Combinatorial Optimization Problems | 
 Satyen Kale \, Chansoo Lee \, David Pal
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Hardness of Online Sleeping Combinatorial Optimization 
 Problems\nSatyen Kale \, Chansoo Lee \, David Pal\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7136\n\nWe show that several online combinator
 ial optimization problems that admit efficient no-regret algorithms become
  computationally hard in the sleeping setting where a subset of actions be
 comes unavailable in each round. Specifically\, we show that the sleeping 
 versions of these problems are at least as hard as PAC learning DNF expres
 sions\, a long standing open problem. We show hardness for the sleeping ve
 rsions of Online Shortest Paths\, Online Minimum Spanning Tree\, Online k-
 Subsets\, Online k-Truncated Permutations\, Online Minimum Cut\, and Onlin
 e Bipartite Matching. The hardness result for the sleeping version of the 
 Online Shortest Paths problem resolves an open problem presented at COLT 2
 015 [Koolen et al.\, 2015].
LOCATION:Area 5+6+7+8 #103
END:VEVENT
BEGIN:VEVENT
SUMMARY:An algorithm for L1 nearest neighbor search via monotonic embeddin
 g | Xinan Wang \, Sanjoy Dasgupta
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:An algorithm for L1 nearest neighbor search via monoton
 ic embedding\nXinan Wang \, Sanjoy Dasgupta\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7137\n\nFast algorithms for nearest neighbor (NN) se
 arch have in large part focused on L2 distance. Here we develop an approac
 h for L1 distance that begins with an explicit and exact embedding of the 
 points into L2. We show how this embedding can efficiently be combined wit
 h random projection methods for L2 NN search\, such as locality-sensitive 
 hashing or random projection trees. We rigorously establish the correctnes
 s of the methodology and show by experimentation that it is competitive in
  practice with available alternatives.
LOCATION:Area 5+6+7+8 #104
END:VEVENT
BEGIN:VEVENT
SUMMARY:Local Maxima in the Likelihood of Gaussian Mixture Models: Structu
 ral Results and Algorithmic Consequences | Chi Jin \, Yuchen Zhang \, Siva
 raman Balakrishnan \, Martin J Wainwright \, Michael I Jordan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Local Maxima in the Likelihood of Gaussian Mixture Mode
 ls: Structural Results and Algorithmic Consequences\nChi Jin \, Yuchen Zha
 ng \, Sivaraman Balakrishnan \, Martin J Wainwright \, Michael I Jordan\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7138\n\nWe provide two f
 undamental results on the population (infinite-sample) likelihood function
  of Gaussian mixture models with $M \\geq 3$ components. Our first main re
 sult shows that the population likelihood function has bad local maxima ev
 en in the special case of equally-weighted mixtures of well-separated and 
 spherical Gaussians. We prove that the log-likelihood value of these bad l
 ocal maxima can be arbitrarily worse than that of any global optimum\, the
 reby resolving an open question of Srebro (2007). Our second main result s
 hows that the EM algorithm (or a first-order variant of it) with random in
 itialization will converge to bad critical points with probability at leas
 t $1-e^{-\\Omega(M)}$. We further establish that a first-order variant of 
 EM will not converge to strict saddle points almost surely\, indicating th
 at the poor performance of the first-order method can be attributed to the
  existence of bad local maxima rather than bad saddle points. Overall\, ou
 r results highlight the necessity of careful initialization when using the
  EM algorithm in practice\, even when applied in highly favorable settings
 .
LOCATION:Area 5+6+7+8 #105
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning User Perceived Clusters with Feature-Level Supervision | 
 Ting-Yu Cheng \, Guiguan Lin \, xinyang gong \, Kang-Jun Liu \, Shan-Hung 
 Wu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning User Perceived Clusters with Feature-Level Sup
 ervision\nTing-Yu Cheng \, Guiguan Lin \, xinyang gong \, Kang-Jun Liu \, 
 Shan-Hung Wu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7139\n\nS
 emi-supervised clustering algorithms have been proposed to identify data c
 lusters that align with user perceived ones via the aid of side informatio
 n such as seeds or pairwise constrains. However\, traditional side informa
 tion is mostly at the instance level and subject to the sampling bias\, wh
 ere non-randomly sampled instances in the supervision can mislead the algo
 rithms to wrong clusters. In this paper\, we propose learning from the fea
 ture-level supervision. We show that this kind of supervision can be easil
 y obtained in the form of perception vectors in many applications. Then we
  present novel algorithms\, called Perception Embedded (PE) clustering\, t
 hat exploit the perception vectors as well as traditional side information
  to find clusters perceived by the user. Extensive experiments are conduct
 ed on real datasets and the results demonstrate the effectiveness of PE em
 pirically.
LOCATION:Area 5+6+7+8 #106
END:VEVENT
BEGIN:VEVENT
SUMMARY:InfoGAN: Interpretable Representation Learning by Information Maxi
 mizing Generative Adversarial Nets | Xi Chen \, Xi Chen \, Yan Duan \, Rei
 n Houthooft \, John Schulman \, Ilya Sutskever \, Pieter Abbeel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:InfoGAN: Interpretable Representation Learning by Infor
 mation Maximizing Generative Adversarial Nets\nXi Chen \, Xi Chen \, Yan D
 uan \, Rein Houthooft \, John Schulman \, Ilya Sutskever \, Pieter Abbeel\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7140\n\nThis paper des
 cribes InfoGAN\, an information-theoretic extension to the Generative Adve
 rsarial Network that is able to learn disentangled representations in a co
 mpletely unsupervised manner. InfoGAN is a generative adversarial network 
 that also maximizes the mutual information between a small subset of the l
 atent variables and the observation. We derive a lower bound to the mutual
  information objective that can be optimized efficiently\, and show that o
 ur training procedure can be interpreted as a variation of the Wake-Sleep 
 algorithm. Specifically\, InfoGAN successfully disentangles writing styles
  from digit shapes on the MNIST dataset\, pose from lighting of 3D rendere
 d images\, and background digits from the central digit on the SVHN datase
 t. It also discovers visual concepts that include hair styles\, presence/a
 bsence of eyeglasses\, and emotions on the CelebA face dataset. Experiment
 s show that InfoGAN learns interpretable representations that are competit
 ive with representations learned by existing fully supervised methods.
LOCATION:Area 5+6+7+8 #107
END:VEVENT
BEGIN:VEVENT
SUMMARY:Neural Universal Discrete Denoiser | Taesup Moon \, Seonwoo Min \,
  Byunghan Lee \, Sungroh Yoon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Neural Universal Discrete Denoiser\nTaesup Moon \, Seon
 woo Min \, Byunghan Lee \, Sungroh Yoon\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7141\n\nWe present a new framework of applying deep neur
 al networks (DNN) to devise a universal discrete denoiser. Unlike other ap
 proaches that utilize supervised learning for denoising\, we do not requir
 e any additional training data. In such setting\, while the ground-truth l
 abel\, i.e.\, the clean data\, is not available\, we devise ``pseudo-label
 s'' and a novel objective function such that DNN can be trained in a same 
 way as supervised learning to become a discrete denoiser. We experimentall
 y show that our resulting algorithm\, dubbed as Neural DUDE\, significantl
 y outperforms the previous state-of-the-art in several applications with a
  systematic rule of choosing the hyperparameter\, which is an attractive f
 eature in practice.
LOCATION:Area 5+6+7+8 #108
END:VEVENT
BEGIN:VEVENT
SUMMARY:A primal-dual method for conic constrained distributed optimizatio
 n problems | Necdet Serhat Aybat \, Erfan Yazdandoost Hamedani
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A primal-dual method for conic constrained distributed 
 optimization problems\nNecdet Serhat Aybat \, Erfan Yazdandoost Hamedani\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7142\n\nWe consider coo
 perative multi-agent consensus optimization problems over an undirected ne
 twork of agents\, where only those agents connected by an edge can directl
 y communicate. The objective is to minimize the sum of agent-specific comp
 osite convex functions over agent-specific private conic constraint sets\;
  hence\, the optimal consensus decision should lie in the intersection of 
 these private sets. We provide convergence rates in sub-optimality\, infea
 sibility and consensus violation\; examine the effect of underlying networ
 k topology on the convergence rates of the proposed decentralized algorith
 ms\; and show how to extend these methods to handle time-varying communica
 tion networks.
LOCATION:Area 5+6+7+8 #109
END:VEVENT
BEGIN:VEVENT
SUMMARY:Simple and Efficient Weighted Minwise Hashing | Anshumali Shrivast
 ava
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Simple and Efficient Weighted Minwise Hashing\nAnshumal
 i Shrivastava\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7143\n\n
 Weighted minwise hashing (WMH) is one of the fundamental subroutine\, requ
 ired by many celebrated approximation algorithms\, commonly adopted in ind
 ustrial practice for large -scale search and learning. The resource bottle
 neck with WMH is the computation of multiple (typically a few hundreds to 
 thousands) independent hashes of the data.  We propose a simple rejection 
 type sampling scheme based on a carefully designed red-green map\, where w
 e show that the number of rejected sample has exactly the same distributio
 n as weighted minwise sampling. The running time of our method\,  for many
  practical datasets\, is an order of magnitude smaller than existing metho
 ds. Experimental evaluations\, on real datasets\, show that for computing 
 500 WMH\, our proposal can be 60000x faster than the Ioffe's method withou
 t losing any accuracy. Our method is also around 100x faster than approxim
 ate heuristics capitalizing on the efficient ``densified" one permutation 
 hashing schemes~\\cite{Proc:OneHashLSHICML14\,Proc:ShrivastavaUAI14}. Give
 n the simplicity of our approach and its significant advantages\, we hope 
 that it will replace existing implementations in practice.
LOCATION:Area 5+6+7+8 #110
END:VEVENT
BEGIN:VEVENT
SUMMARY:Eliciting Categorical Data for Optimal Aggregation | Chien-Ju Ho \
 , Rafael Frongillo \, Yiling Chen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Eliciting Categorical Data for Optimal Aggregation\nChi
 en-Ju Ho \, Rafael Frongillo \, Yiling Chen\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7144\n\nModels for collecting and aggregating catego
 rical data on crowdsourcing platforms typically fall into two broad catego
 ries: those assuming agents honest and consistent but with heterogeneous e
 rror rates\, and those assuming agents strategic and seek to maximize thei
 r expected reward. The former often leads to tractable aggregation of elic
 ited data\, while the latter usually focuses on optimal elicitation and do
 es not consider aggregation. In this paper\, we develop a Bayesian model\,
  wherein agents have differing quality of information\, but also respond t
 o incentives. Our model generalizes both categories and enables the joint 
 exploration of optimal elicitation and aggregation. This model enables our
  exploration\, both analytically and experimentally\, of optimal aggregati
 on of categorical data and optimal multiple-choice interface design.
LOCATION:Area 5+6+7+8 #111
END:VEVENT
BEGIN:VEVENT
SUMMARY:Depth from a Single Image by Harmonizing Overcomplete Local Networ
 k Predictions | Ayan Chakrabarti \, Jingyu Shao \, Greg Shakhnarovich
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Depth from a Single Image by Harmonizing Overcomplete L
 ocal Network Predictions\nAyan Chakrabarti \, Jingyu Shao \, Greg Shakhnar
 ovich\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7145\n\nA single
  color image can contain many cues informative towards different aspects o
 f local geometric structure. We approach the problem of monocular depth es
 timation by using a neural network to produce a mid-level representation t
 hat summarizes these cues. This network is trained to characterize local s
 cene geometry by predicting\, at every image location\, depth derivatives 
 of different orders\, orientations and scales. However\, instead of a sing
 le estimate for each derivative\, the network outputs probability distribu
 tions that allow it to express confidence about some coefficients\, and am
 biguity about others. Scene depth is then estimated by harmonizing this ov
 ercomplete set of network predictions\, using a globalization procedure th
 at finds a single consistent depth map that best matches all the local der
 ivative distributions. We demonstrate the efficacy of this approach throug
 h evaluation on the NYU v2 depth data set.
LOCATION:Area 5+6+7+8 #112
END:VEVENT
BEGIN:VEVENT
SUMMARY:SEBOOST - Boosting Stochastic Learning Using Subspace Optimization
  Techniques | Elad Richardson \, Rom Herskovitz \, Boris Ginsburg \, Micha
 el Zibulevsky
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:SEBOOST - Boosting Stochastic Learning Using Subspace O
 ptimization Techniques\nElad Richardson \, Rom Herskovitz \, Boris Ginsbur
 g \, Michael Zibulevsky\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7146\n\nWe present SEBOOST\, a technique for boosting the performance of
  existing stochastic optimization methods. SEBOOST applies a secondary opt
 imization process in the subspace spanned by the last steps and descent di
 rections. The method was inspired by the SESOP optimization method for lar
 ge-scale problems\, and has been adapted for the stochastic learning frame
 work. It can be applied on top of any existing optimization method with no
  need to tweak the internal algorithm. We show that the method is able to 
 boost the performance of different algorithms\, and make them more robust 
 to changes in their hyper-parameters. As the boosting steps of SEBOOST are
  applied between large sets of descent steps\, the additional subspace opt
 imization hardly increases the overall computational burden. We introduce 
 two hyper-parameters that control the balance between the baseline method 
 and the secondary optimization process. The method was evaluated on severa
 l deep learning tasks\, demonstrating promising results.
LOCATION:Area 5+6+7+8 #113
END:VEVENT
BEGIN:VEVENT
SUMMARY:Reshaped Wirtinger Flow for Solving Quadratic System of Equations 
 | Huishuai Zhang \, Yingbin Liang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Reshaped Wirtinger Flow for Solving Quadratic System of
  Equations\nHuishuai Zhang \, Yingbin Liang\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7147\n\nWe study the problem of recovering a vector 
 $\\bx\\in \\bbR^n$ from its magnitude measurements $yi=|\\langle \\bai\, \
 \bx\\rangle|\, i=1\,...\, m$. Our work is along the line of the Wirtinger 
 flow (WF) approach \\citet{candes2015phase}\, which solves the problem by 
 minimizing a nonconvex loss function via a gradient algorithm and can be s
 hown to converge to a global optimal point under good initialization. In c
 ontrast to the smooth loss function used in WF\, we adopt a nonsmooth but 
 lower-order loss function\, and design a gradient-like algorithm (referred
  to as reshaped-WF). We show that for random Gaussian measurements\, resha
 ped-WF enjoys geometric convergence to a global optimal point as long as t
 he number $m$ of measurements is at the order of $\\cO(n)$\, where $n$ is 
 the dimension of the unknown $\\bx$. This improves the sample complexity o
 f WF\, and achieves  the same sample complexity as truncated-WF \\citet{ch
 en2015solving} but without truncation at gradient step. Furthermore\, resh
 aped-WF costs less computationally than WF\, and runs faster numerically t
 han both WF and truncated-WF. Bypassing higher-order variables in the loss
  function and truncations in the gradient loop\, analysis of reshaped-WF i
 s simplified.
LOCATION:Area 5+6+7+8 #114
END:VEVENT
BEGIN:VEVENT
SUMMARY:Training and Evaluating Multimodal Word Embeddings with Large-scal
 e Web Annotated Images | Junhua Mao \, Jiajing Xu \, Kevin Jing \, Alan L 
 Yuille
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Training and Evaluating Multimodal Word Embeddings with
  Large-scale Web Annotated Images\nJunhua Mao \, Jiajing Xu \, Kevin Jing 
 \, Alan L Yuille\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7148\
 n\nIn this paper\, we focus on training and evaluating effective word embe
 ddings with both text and visual information. More specifically\, we intro
 duce a large-scale dataset with 300 million sentences describing over 40 m
 illion images crawled and downloaded from publicly available Pins (i.e. an
  image with sentence descriptions uploaded by users) on Pinterest. This da
 taset is more than 200 times larger than MS COCO\, the standard large-scal
 e image dataset with sentence descriptions. In addition\, we construct an 
 evaluation dataset to directly assess the effectiveness of word embeddings
  in terms of finding semantically similar or related words and phrases. Th
 e word/phrase pairs in this evaluation dataset are collected from the clic
 k data with millions of users in an image search system\, thus contain ric
 h semantic relationships. Based on these datasets\, we propose and compare
  several Recurrent Neural Networks (RNNs) based multimodal (text and image
 ) models. Experiments show that our model benefits from incorporating the 
 visual information into the word embeddings\, and a weight sharing strateg
 y is crucial for learning such multimodal embeddings. The project page is:
  http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html (The datas
 ets introduced in this work will be gradually released on the project page
 .).
LOCATION:Area 5+6+7+8 #115
END:VEVENT
BEGIN:VEVENT
SUMMARY:Online ICA: Understanding Global Dynamics of Nonconvex Optimizatio
 n via Diffusion Processes | Chris Junchi Li \, Zhaoran Wang \, Han Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Online ICA: Understanding Global Dynamics of Nonconvex 
 Optimization via Diffusion Processes\nChris Junchi Li \, Zhaoran Wang \, H
 an Liu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7149\n\nSolving
  statistical learning problems often involves nonconvex optimization. Desp
 ite the empirical success of nonconvex statistical optimization methods\, 
 their global dynamics\, especially convergence to the desirable local mini
 ma\, remain less well understood in theory. In this paper\, we propose a n
 ew analytic paradigm based on diffusion processes to characterize the glob
 al dynamics of nonconvex statistical optimization. As a concrete example\,
  we study stochastic gradient descent (SGD) for the tensor decomposition f
 ormulation of independent component analysis. In particular\, we cast diff
 erent phases of SGD into diffusion processes\, i.e.\, solutions to stochas
 tic differential equations. Initialized from an unstable equilibrium\, the
  global dynamics of SGD transit over three consecutive phases: (i) an unst
 able Ornstein-Uhlenbeck process slowly departing from the initialization\,
  (ii) the solution to an ordinary differential equation\, which quickly ev
 olves towards the desirable local minimum\, and (iii) a stable Ornstein-Uh
 lenbeck process oscillating around the desirable local minimum. Our proof 
 techniques are based upon Stroock and Varadhan’s weak convergence of Mar
 kov chains to diffusion processes\, which are of independent interest.
LOCATION:Area 5+6+7+8 #116
END:VEVENT
BEGIN:VEVENT
SUMMARY:VIME: Variational Information Maximizing Exploration | Rein Houtho
 oft \, Xi Chen \, Xi Chen \, Yan Duan \, John Schulman \, Filip De Turck \
 , Pieter Abbeel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:VIME: Variational Information Maximizing Exploration\nR
 ein Houthooft \, Xi Chen \, Xi Chen \, Yan Duan \, John Schulman \, Filip 
 De Turck \, Pieter Abbeel\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=7150\n\nScalable and effective exploration remains a key challenge in 
 reinforcement learning (RL). While there are methods with optimality guara
 ntees in the setting of discrete state and action spaces\, these methods c
 annot be applied in high-dimensional deep RL scenarios. As such\, most con
 temporary RL relies on simple heuristics such as epsilon-greedy exploratio
 n or adding Gaussian noise to the controls. This paper introduces Variatio
 nal Information Maximizing Exploration (VIME)\, an exploration strategy ba
 sed on maximization of information gain about the agent's belief of enviro
 nment dynamics. We propose a practical implementation\, using variational 
 inference in Bayesian neural networks which efficiently handles continuous
  state and action spaces. VIME modifies the MDP reward function\, and can 
 be applied with several different underlying RL algorithms. We demonstrate
  that VIME achieves significantly better performance compared to heuristic
  exploration methods across a variety of continuous control tasks and algo
 rithms\, including tasks with very sparse rewards.
LOCATION:Area 5+6+7+8 #117
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deconvolving Feedback Loops in Recommender Systems | Ayan Sinha \,
  David Gleich \, Karthik Ramani
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Deconvolving Feedback Loops in Recommender Systems\nAya
 n Sinha \, David Gleich \, Karthik Ramani\nhttp://nips.cc/Conferences/2016
 /Schedule?showEvent=7151\n\nCollaborative filtering is a popular technique
  to infer users' preferences on new content based on the collective inform
 ation of all users preferences. Recommender systems then use this informat
 ion to make personalized suggestions to users. When users accept these rec
 ommendations it creates a feedback loop in the recommender system\, and th
 ese loops iteratively influence the collaborative filtering algorithm's pr
 edictions over time. We investigate whether it is possible to identify ite
 ms affected by these feedback loops. We state sufficient assumptions to de
 convolve the feedback loops while keeping the inverse solution tractable. 
 We furthermore develop a metric to unravel the recommender system's influe
 nce on the entire user-item rating matrix. We use this metric on synthetic
  and real-world datasets to (1) identify the extent to which the recommend
 er system affects the final rating matrix\, (2) rank frequently recommende
 d items\, and (3) distinguish whether a user's rated item was recommended 
 or an intrinsic preference. Our results indicate that it is possible to re
 cover the ratings matrix of intrinsic user preferences using a single snap
 shot of the ratings matrix without any temporal information.
LOCATION:Area 5+6+7+8 #118
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Non-parametric Learning Method for Confidently Estimating Patien
 t's Clinical State and Dynamics | William Hoiles \, Mihaela Van Der Schaar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Non-parametric Learning Method for Confidently Estima
 ting Patient's Clinical State and Dynamics\nWilliam Hoiles \, Mihaela Van 
 Der Schaar\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7152\n\nEst
 imating patient's clinical state from multiple concurrent physiological st
 reams plays an important role in determining if a therapeutic intervention
  is necessary and for triaging patients in the hospital. In this paper we 
 construct a non-parametric learning algorithm to estimate the clinical sta
 te of a patient. The algorithm addresses several known challenges with cli
 nical state estimation such as eliminating bias introduced by therapeutic 
 intervention censoring\, increasing the timeliness of state estimation whi
 le ensuring a sufficient accuracy\, and the ability to detect anomalous cl
 inical states. These benefits are obtained by combining the tools of non-p
 arametric Bayesian inference\, permutation testing\, and generalizations o
 f the empirical Bernstein inequality. The algorithm is validated using rea
 l-world data from a cancer ward in a large academic hospital.
LOCATION:Area 5+6+7+8 #119
END:VEVENT
BEGIN:VEVENT
SUMMARY:Semiparametric Differential Graph Models | Pan Xu \, Quanquan Gu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Semiparametric Differential Graph Models\nPan Xu \, Qua
 nquan Gu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7153\n\nIn ma
 ny cases of network analysis\, it is more attractive to study how a networ
 k varies under  different conditions than an individual static network. We
  propose a novel graphical model\, namely Latent Differential Graph Model\
 , where the networks under two different conditions are represented by two
  semiparametric elliptical distributions respectively\, and the variation 
 of these two networks (i.e.\, differential graph) is characterized by the 
 difference between their latent precision matrices. We propose an estimato
 r for the differential graph based on quasi likelihood maximization with n
 onconvex regularization. We show that our estimator attains a faster stati
 stical rate in parameter estimation than the state-of-the-art methods\, an
 d enjoys oracle property under mild conditions. Thorough experiments on bo
 th synthetic and real world data support our theory.
LOCATION:Area 5+6+7+8 #120
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Non-convex One-Pass Framework for Generalized Factorization Mach
 ine and Rank-One Matrix Sensing | Ming Lin \, Jieping Ye
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Non-convex One-Pass Framework for Generalized Factori
 zation Machine and Rank-One Matrix Sensing\nMing Lin \, Jieping Ye\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7154\n\nWe develop an efficie
 nt alternating framework for learning a generalized version of Factorizati
 on Machine (gFM) on steaming data with provable guarantees. When the insta
 nces are sampled from $d$ dimensional random Gaussian vectors and the targ
 et second order coefficient matrix in gFM is of rank $k$\, our algorithm c
 onverges linearly\, achieves $O(\\epsilon)$ recovery error after retrievin
 g $O(k^{3}d\\log(1/\\epsilon))$ training instances\, consumes $O(kd)$ memo
 ry in one-pass of dataset and only requires matrix-vector product operatio
 ns in each iteration. The key ingredient of our framework is a constructio
 n of an estimation sequence  endowed with a so-called Conditionally Indepe
 ndent RIP condition (CI-RIP). As special cases of gFM\, our framework can 
 be applied to symmetric or asymmetric rank-one matrix sensing problems\, s
 uch as inductive matrix completion and phase retrieval.
LOCATION:Area 5+6+7+8 #121
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sublinear Time Orthogonal Tensor Decomposition | Zhao Song \, Davi
 d Woodruff \, Huan Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Sublinear Time Orthogonal Tensor Decomposition\nZhao So
 ng \, David Woodruff \, Huan Zhang\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=7155\n\nA recent work (Wang et. al.\, NIPS 2015) gives the fa
 stest known algorithms for orthogonal tensor decomposition with provable g
 uarantees. Their algorithm is based on computing sketches of the input ten
 sor\, which requires reading the entire input. We show in a number of case
 s one can achieve the same theoretical guarantees in sublinear time\, i.e.
 \, even without reading most of the input tensor. Instead of using sketche
 s to estimate inner products in tensor decomposition algorithms\, we use i
 mportance sampling. To achieve sublinear time\, we need to know the norms 
 of tensor slices\, and we show how to do this in a number of important cas
 es. For symmetric tensors $ T = \\sum{i=1}^k \\lambdai ui^{\\otimes p}$ wi
 th $\\lambdai &gt\; 0$ for all i\, we estimate such norms in sublinear tim
 e whenever p is even. For the important case of p = 3 and small values of 
 k\, we can also estimate such norms. For asymmetric tensors sublinear time
  is not possible in general\, but we show if the tensor slice norms are ju
 st slightly below $\\| T \\|_F$ then sublinear time is again possible. One
  of the main strengths of our work is empirical - in a number of cases our
  algorithm is orders of magnitude faster than existing methods with the sa
 me accuracy.
LOCATION:Area 5+6+7+8 #122
END:VEVENT
BEGIN:VEVENT
SUMMARY:Achieving budget-optimality with adaptive schemes in crowdsourcing
  | Ashish Khetan \, Sewoong Oh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Achieving budget-optimality with adaptive schemes in cr
 owdsourcing\nAshish Khetan \, Sewoong Oh\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7156\n\nAdaptive schemes\, where tasks are assigned bas
 ed on the data collected thus far\, are widely used in practical crowdsour
 cing systems to efficiently allocate the budget. However\, existing theore
 tical analyses of crowdsourcing systems suggest that the gain of adaptive 
 task assignments is minimal. To bridge this gap\, we investigate this ques
 tion under a strictly more general probabilistic model\, which has been re
 cently introduced to model practical crowdsourcing data sets. Under this g
 eneralized Dawid-Skene model\, we characterize the fundamental trade-off b
 etween budget and accuracy\, and introduce a novel adaptive scheme that ma
 tches this fundamental limit. We further quantify the gain of adaptivity\,
  by comparing the trade-off with the one for non-adaptive schemes\, and co
 nfirm that the gain is significant and can be made arbitrarily large depen
 ding on the distribution of the difficulty level of the tasks at hand.
LOCATION:Area 5+6+7+8 #123
END:VEVENT
BEGIN:VEVENT
SUMMARY:Joint Line Segmentation and Transcription for End-to-End Handwritt
 en Paragraph Recognition | Theodore Bluche
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Joint Line Segmentation and Transcription for End-to-En
 d Handwritten Paragraph Recognition\nTheodore Bluche\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7157\n\nOffline handwriting recognition sys
 tems require cropped text line images for both training and recognition. O
 n the one hand\, the annotation of position and transcript at line level i
 s costly to obtain. On the other hand\, automatic line segmentation algori
 thms are prone to errors\, compromising the subsequent recognition.  In th
 is paper\, we propose a modification of the popular and efficient Multi-Di
 mensional Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) t
 o enable end-to-end processing of handwritten paragraphs. More particularl
 y\, we replace the collapse layer transforming the two-dimensional represe
 ntation into a sequence of predictions by a recurrent version which can se
 lect one line at a time.  In the proposed model\, a neural network perform
 s a kind of implicit line segmentation by computing attention weights on t
 he image representation. The experiments on paragraphs of Rimes and IAM da
 tabases yield results that are competitive with those of networks trained 
 at line level\, and constitute a significant step towards end-to-end trans
 cription of full documents.
LOCATION:Area 5+6+7+8 #124
END:VEVENT
BEGIN:VEVENT
SUMMARY:Human Decision-Making under Limited Time | Pedro A Ortega \, Alan 
 A Stocker
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Human Decision-Making under Limited Time\nPedro A Orteg
 a \, Alan A Stocker\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=71
 58\n\nAbstract Subjective expected utility theory assumes that decision-ma
 kers possess unlimited computational resources to reason about their choic
 es\; however\, virtually all decisions in everyday life are made under res
 ource constraints---i.e. decision-makers are bounded in their rationality.
  Here we experimentally tested the predictions made by a formalization of 
 bounded rationality based on ideas from statistical mechanics and informat
 ion-theory. We systematically tested human subjects in their ability to so
 lve combinatorial puzzles under different time limitations. We found that 
 our bounded-rational model accounts well for the data. The decomposition o
 f the fitted model parameter into the subjects' expected utility function 
 and resource parameter provide interesting insight into the subjects' info
 rmation capacity limits. Our results confirm that humans gradually fall ba
 ck on their learned prior choice patterns when confronted with increasing 
 resource limitations.
LOCATION:Area 5+6+7+8 #125
END:VEVENT
BEGIN:VEVENT
SUMMARY:Joint M-Best-Diverse Labelings as a Parametric Submodular Minimiza
 tion | Alexander Kirillov \, Alexander Shekhovtsov \, Carsten Rother \, Bo
 gdan Savchynskyy
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Joint M-Best-Diverse Labelings as a Parametric Submodul
 ar Minimization\nAlexander Kirillov \, Alexander Shekhovtsov \, Carsten Ro
 ther \, Bogdan Savchynskyy\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7159\n\nWe consider the problem of jointly inferring the $M$-best div
 erse labelings for a binary (high-order) submodular energy of a graphical 
 model. Recently\, it was shown that this problem can be solved to a global
  optimum\, for many practically interesting diversity measures. It was not
 ed that the labelings are\, so-called\, nested.  This nestedness property 
 also holds for labelings of a class of parametric submodular minimization 
 problems\, where different values of the global parameter $\\gamma$ give r
 ise to different solutions. The popular example of the parametric submodul
 ar minimization is the monotonic parametric max-flow problem\, which is al
 so widely used for computing multiple labelings.  As the main contribution
  of this work we establish a close relationship between diversity with sub
 modular energies and the parametric submodular minimization. In particular
 \, the joint $M$-best diverse labelings can be obtained by running a non-p
 arametric submodular minimization (in the special case - max-flow) solver 
 for $M$ different values of $\\gamma$ in parallel\, for certain diversity 
 measures. Importantly\, the values for~$\\gamma$ can be computed in a clos
 ed form in advance\, prior to any optimization. These theoretical results 
 suggest two simple yet efficient algorithms for the joint $M$-best diverse
  problem\, which outperform competitors in terms of runtime and quality of
  results. In particular\, as we show in the paper\, the new methods comput
 e the exact $M$-best diverse labelings faster than a popular method of Bat
 ra et al.\, which in some sense only obtains approximate solutions.
LOCATION:Area 5+6+7+8 #126
END:VEVENT
BEGIN:VEVENT
SUMMARY:Even Faster SVD Decomposition Yet Without Agonizing Pain | Zeyuan 
 Allen-Zhu \, Yuanzhi Li
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Even Faster SVD Decomposition Yet Without Agonizing Pai
 n\nZeyuan Allen-Zhu \, Yuanzhi Li\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7160\n\nWe study k-SVD that is to obtain the first k singular 
 vectors of a matrix A approximately. Recently\, a few breakthroughs have b
 een discovered on $k$-SVD: Musco and Musco [1] provided the first gap-free
  theorem for the block Krylov method\, Shamir [2] discovered the first var
 iance-reduction stochastic method\, and Bhojanapalli et al. [3] provided t
 he fastest $O(nnz(A) + poly(1/eps))$-type of algorithm using alternating m
 inimization.  In this paper\, we improve the above breakthroughs by provid
 ing a new framework for solving k-SVD. In particular\, we obtain faster ga
 p-free convergence speed outperforming [1]\, we obtain the first accelerat
 ed AND stochastic method outperforming [3]. In the NNZ running-time regime
 \, we outperform [3] without even using alternating minimization for certa
 in parameter regimes.
LOCATION:Area 5+6+7+8 #127
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast and accurate spike sorting of high-channel count probes with 
 KiloSort | Marius Pachitariu \, Nicholas A Steinmetz \, Shabnam N Kadir \,
  Matteo Carandini \, Kenneth D Harris
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Fast and accurate spike sorting of high-channel count p
 robes with KiloSort\nMarius Pachitariu \, Nicholas A Steinmetz \, Shabnam 
 N Kadir \, Matteo Carandini \, Kenneth D Harris\nhttp://nips.cc/Conference
 s/2016/Schedule?showEvent=7161\n\nNew silicon technology is enabling large
 -scale electrophysiological recordings in vivo from hundreds to thousands 
 of channels. Interpreting these recordings requires scalable and accurate 
 automated methods for spike sorting\, which should minimize the time requi
 red for manual curation of the results. Here we introduce KiloSort\, a new
  integrated spike sorting framework that uses template matching both durin
 g spike detection and during spike clustering. KiloSort models the electri
 cal voltage as a sum of template waveforms triggered on the spike times\, 
 which allows overlapping spikes to be identified and resolved. Unlike prev
 ious algorithms that compress the data with PCA\, KiloSort operates on the
  raw data which allows it to construct a more accurate model of the wavefo
 rms. Processing times are faster than in previous algorithms thanks to bat
 ch-based optimization on GPUs. We compare KiloSort to an established algor
 ithm and show favorable performance\, at much reduced processing times. A 
 novel post-clustering merging step based on the continuity of the template
 s further reduced substantially the number of manual operations required o
 n this data\, for the neurons with near-zero error rates\, paving the way 
 for fully automated spike sorting of multichannel electrode recordings.
LOCATION:Area 5+6+7+8 #128
END:VEVENT
BEGIN:VEVENT
SUMMARY:Batched Gaussian Process Bandit Optimization via Determinantal Poi
 nt Processes | Tarun Kathuria \, Amit Deshpande \, Pushmeet Kohli
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Batched Gaussian Process Bandit Optimization via Determ
 inantal Point Processes\nTarun Kathuria \, Amit Deshpande \, Pushmeet Kohl
 i\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7162\n\nGaussian Pro
 cess bandit optimization has emerged as a powerful tool for optimizing noi
 sy black box functions. One example in machine learning is hyper-parameter
  optimization where each evaluation of the target function may require tra
 ining a model which may involve days or even weeks of computation. Most me
 thods for this so-called “Bayesian optimization” only allow sequential
  exploration of the parameter space. However\, it is often desirable to pr
 opose batches or sets of parameter values to explore simultaneously\, espe
 cially when there are large parallel processing facilities at our disposal
 . Batch methods require modeling the interaction between the different eva
 luations in the batch\, which can be expensive in complex scenarios. In th
 is paper\, we propose a new approach for parallelizing Bayesian optimizati
 on by modeling the diversity of a batch via Determinantal point processes 
 (DPPs) whose kernels are learned automatically. This allows us to generali
 ze a previous result as well as prove better regret bounds based on DPP sa
 mpling. Our experiments on a variety of synthetic and real-world robotics 
 and hyper-parameter optimization tasks indicate that our DPP-based methods
 \, especially those based on DPP sampling\, outperform state-of-the-art me
 thods.
LOCATION:Area 5+6+7+8 #129
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Multiple Choice Learning for Training Diverse Deep Ense
 mbles | Stefan Lee \, Senthil Purushwalkam Shiva Prakash \, Michael Cogswe
 ll \, Viresh Ranjan \, David Crandall \, Dhruv Batra
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Stochastic Multiple Choice Learning for Training Divers
 e Deep Ensembles\nStefan Lee \, Senthil Purushwalkam Shiva Prakash \, Mich
 ael Cogswell \, Viresh Ranjan \, David Crandall \, Dhruv Batra\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7163\n\nMany practical perception
  systems exist within larger processes which often include interactions wi
 th users or additional components that are capable of evaluating the quali
 ty of predicted solutions. In these contexts\, it is beneficial to provide
  these oracle mechanisms with multiple highly likely hypotheses rather tha
 n a single prediction. In this work\, we pose the task of producing multip
 le outputs as a learning problem over an ensemble of deep networks -- intr
 oducing a novel stochastic gradient descent based approach to minimize the
  loss with respect to an oracle. Our method is simple to implement\, agnos
 tic to both architecture and loss function\, and parameter-free. Our appro
 ach achieves lower oracle error compared to existing methods on a wide ran
 ge of tasks and deep architectures. We also show qualitatively that soluti
 ons produced from our approach often provide interpretable representations
  of task ambiguity.
LOCATION:Area 5+6+7+8 #130
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Sparse Linear Encoders and Sparse PCA | Malik Magdon-Ismai
 l \, Christos Boutsidis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimal Sparse Linear Encoders and Sparse PCA\nMalik Ma
 gdon-Ismail \, Christos Boutsidis\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7164\n\nPrincipal components analysis~(PCA) is the optimal lin
 ear  encoder of data. Sparse linear encoders (e.g.\, sparse PCA) produce m
 ore interpretable features that  can promote better generalization. (\\rn{
 1}) Given a level of sparsity\, what is the best approximation to PCA?  (\
 \rn{2}) Are there efficient algorithms which can achieve this optimal  com
 binatorial tradeoff? We answer both questions by  providing the first poly
 nomial-time algorithms to construct \\emph{optimal} sparse linear auto-enc
 oders\; additionally\, we demonstrate the performance of our algorithms on
  real data.
LOCATION:Area 5+6+7+8 #131
END:VEVENT
BEGIN:VEVENT
SUMMARY:Using Social Dynamics to Make Individual Predictions: Variational 
 Inference with a Stochastic Kinetic Model | Zhen Xu \, Wen Dong \, Sargur 
 N Srihari
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Using Social Dynamics to Make Individual Predictions: V
 ariational Inference with a Stochastic Kinetic Model\nZhen Xu \, Wen Dong 
 \, Sargur N Srihari\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=71
 65\n\nSocial dynamics is concerned primarily with interactions among indiv
 iduals and the resulting group behaviors\, modeling the temporal evolution
  of social systems via the interactions of individuals within these system
 s. In particular\, the availability of large-scale data from social networ
 ks and sensor networks offers an unprecedented opportunity to predict stat
 e-changing events at the individual level. Examples of such events include
  disease transmission\, opinion transition in elections\, and rumor propag
 ation. Unlike previous research focusing on the collective effects of soci
 al systems\, this study makes efficient inferences at the individual level
 . In order to cope with dynamic interactions among a large number of indiv
 iduals\, we introduce the stochastic kinetic model to capture adaptive tra
 nsition probabilities and propose an efficient variational inference algor
 ithm the complexity of which grows linearly — rather than exponentially
 — with the number of individuals. To validate this method\, we have perf
 ormed epidemic-dynamics experiments on wireless sensor network data collec
 ted from more than ten thousand people over three years. The proposed algo
 rithm was used to track disease transmission and predict the probability o
 f infection for each individual. Our results demonstrate that this method 
 is more efficient than sampling while nonetheless achieving high accuracy.
LOCATION:Area 5+6+7+8 #132
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Additive Exponential Family Graphical Models via $\\ell_{
 2\,1}$-norm Regularized M-Estimation | Xiaotong Yuan \, Ping Li \, Tong Zh
 ang \, Qingshan Liu \, Guangcan Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning Additive Exponential Family Graphical Models v
 ia $\\ell_{2\,1}$-norm Regularized M-Estimation\nXiaotong Yuan \, Ping Li 
 \, Tong Zhang \, Qingshan Liu \, Guangcan Liu\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=7166\n\nWe investigate a subclass of exponential f
 amily graphical models of which the sufficient statistics are defined by a
 rbitrary additive forms. We propose two $\\ell_{2\,1}$-norm regularized ma
 ximum likelihood estimators to learn the model parameters from i.i.d. samp
 les. The first one is a joint MLE estimator which estimates all the parame
 ters simultaneously. The second one is a node-wise conditional MLE estimat
 or which estimates the parameters for each node individually. For both est
 imators\, statistical analysis shows that under mild conditions the extra 
 flexibility gained by the additive exponential family models comes at almo
 st no cost of statistical efficiency. A Monte-Carlo approximation method i
 s developed to efficiently optimize the proposed estimators. The advantage
 s of our estimators over Gaussian graphical models and Nonparanormal estim
 ators are demonstrated on synthetic and real data sets.
LOCATION:Area 5+6+7+8 #133
END:VEVENT
BEGIN:VEVENT
SUMMARY:Residual Networks Behave Like Ensembles of Relatively Shallow Netw
 orks | Andreas Veit \, Michael J Wilber \, Serge Belongie
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Residual Networks Behave Like Ensembles of Relatively S
 hallow Networks\nAndreas Veit \, Michael J Wilber \, Serge Belongie\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=7167\n\nIn this work we prop
 ose a novel interpretation of residual networks showing that they can be s
 een as a collection of many paths of differing length. Moreover\, residual
  networks seem to enable very deep networks by leveraging only the short p
 aths during training. To support this observation\, we rewrite residual ne
 tworks as an explicit collection of paths. Unlike traditional models\, pat
 hs through residual networks vary in length. Further\, a lesion study reve
 als that these paths show ensemble-like behavior in the sense that they do
  not strongly depend on each other. Finally\, and most surprising\, most p
 aths are shorter than one might expect\, and only the short paths are need
 ed during training\, as longer paths do not contribute any gradient. For e
 xample\, most of the gradient in a residual network with 110 layers comes 
 from paths that are only 10-34 layers deep. Our results reveal one of the 
 key characteristics that seem to enable the training of very deep networks
 : Residual networks avoid the vanishing gradient problem by introducing sh
 ort paths which can carry gradient throughout the extent of very deep netw
 orks.
LOCATION:Area 5+6+7+8 #134
END:VEVENT
BEGIN:VEVENT
SUMMARY:Full-Capacity Unitary Recurrent Neural Networks | Scott Wisdom \, 
 Thomas Powers \, John Hershey \, Jonathan Le Roux \, Les Atlas
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Full-Capacity Unitary Recurrent Neural Networks\nScott 
 Wisdom \, Thomas Powers \, John Hershey \, Jonathan Le Roux \, Les Atlas\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7168\n\nRecurrent neura
 l networks are powerful models for processing sequential data\, but they a
 re generally plagued by vanishing and exploding gradient problems. Unitary
  recurrent neural networks (uRNNs)\, which use unitary recurrence matrices
 \, have recently been proposed as a means to avoid these issues. However\,
  in previous experiments\, the recurrence matrices were restricted to be a
  product of parameterized unitary matrices\, and an open question remains:
  when does such a parameterization fail to represent all unitary matrices\
 , and how does this restricted representational capacity limit what can be
  learned? To address this question\, we propose full-capacity uRNNs that o
 ptimize their recurrence matrix over all unitary matrices\, leading to sig
 nificantly improved performance over uRNNs that use a restricted-capacity 
 recurrence matrix. Our contribution consists of two main components. First
 \, we provide a theoretical argument to determine if a unitary parameteriz
 ation has restricted capacity. Using this argument\, we show that a recent
 ly proposed unitary parameterization has restricted capacity for hidden st
 ate dimension greater than 7. Second\,we show how a complete\, full-capaci
 ty unitary recurrence matrix can be optimized over the differentiable mani
 fold of unitary matrices. The resulting multiplicative gradient step is ve
 ry simple and does not require gradient clipping or learning rate adaptati
 on. We confirm the utility of our claims by empirically evaluating our new
  full-capacity uRNNs on both synthetic and natural data\, achieving superi
 or performance compared to both LSTMs and the original restricted-capacity
  uRNNs.
LOCATION:Area 5+6+7+8 #135
END:VEVENT
BEGIN:VEVENT
SUMMARY:Quantum Perceptron Models | Ashish Kapoor \, Nathan Wiebe \, Kryst
 a Svore
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Quantum Perceptron Models\nAshish Kapoor \, Nathan Wieb
 e \, Krysta Svore\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7169
 \n\nWe demonstrate how quantum computation can provide non-trivial improve
 ments in the computational and statistical complexity of the perceptron mo
 del. We develop two quantum algorithms for perceptron learning. The first 
 algorithm exploits quantum information processing to determine a separatin
 g hyperplane using a number of steps sublinear in the number of data point
 s $N$\, namely $O(\\sqrt{N})$. The second algorithm illustrates how the cl
 assical mistake bound of $O(\\frac{1}{\\gamma^2})$ can be further improved
  to $O(\\frac{1}{\\sqrt{\\gamma}})$ through quantum means\, where $\\gamma
 $ denotes the margin. Such improvements are achieved through the applicati
 on of quantum amplitude amplification to the version space interpretation 
 of the perceptron model.
LOCATION:Area 5+6+7+8 #136
END:VEVENT
BEGIN:VEVENT
SUMMARY:Mapping Estimation for Discrete Optimal Transport | Michaël Perro
 t \, Nicolas Courty \, Rémi Flamary \, Amaury Habrard
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Mapping Estimation for Discrete Optimal Transport\nMich
 aël Perrot \, Nicolas Courty \, Rémi Flamary \, Amaury Habrard\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7170\n\nWe are interested in th
 e computation of the transport map of an Optimal Transport problem. Most o
 f the computational approaches of Optimal Transport use the Kantorovich re
 laxation of the problem to learn a probabilistic coupling $\\mgamma$ but d
 o not address the problem of learning the underlying transport map $\\func
 T$ linked to the original Monge problem. Consequently\, it lowers the pote
 ntial usage of such methods in contexts where out-of-samples computations 
 are mandatory. In this paper we propose a new way to jointly learn the cou
 pling and an approximation of the transport map. We use a jointly convex f
 ormulation which can be efficiently optimized. Additionally\, jointly lear
 ning the coupling and the transport map allows to smooth the result of the
  Optimal Transport and generalize it to out-of-samples examples. Empirical
 ly\, we show the interest and the relevance of our method in two tasks: do
 main adaptation and image editing.
LOCATION:Area 5+6+7+8 #137
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Gradient Geodesic MCMC Methods | Chang Liu \, Jun Zhu \
 , Yang Song
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Stochastic Gradient Geodesic MCMC Methods\nChang Liu \,
  Jun Zhu \, Yang Song\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=
 7171\n\nWe propose two stochastic gradient MCMC methods for sampling from 
 Bayesian posterior distributions defined on Riemann manifolds with a known
  geodesic flow\, e.g. hyperspheres. Our methods are the first scalable sam
 pling methods on these manifolds\, with the aid of stochastic gradients. N
 ovel dynamics are conceived and 2nd-order integrators are developed. By ad
 opting embedding techniques and the geodesic integrator\, the methods do n
 ot require a global coordinate system of the manifold and do not involve i
 nner iterations. Synthetic experiments show the validity of the method\, a
 nd its application to the challenging inference for spherical topic models
  indicate practical usability and efficiency.
LOCATION:Area 5+6+7+8 #138
END:VEVENT
BEGIN:VEVENT
SUMMARY:Variational Information Maximization for Feature Selection | Shuya
 ng Gao \, Greg Ver Steeg \, Aram Galstyan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Variational Information Maximization for Feature Select
 ion\nShuyang Gao \, Greg Ver Steeg \, Aram Galstyan\nhttp://nips.cc/Confer
 ences/2016/Schedule?showEvent=7172\n\nFeature selection is one of the most
  fundamental problems in machine learning. An extensive body of work on in
 formation-theoretic feature selection exists which is based on maximizing 
 mutual information between subsets of features and class labels. Practical
  methods are forced to rely on approximations due to the difficulty of est
 imating mutual information. We demonstrate that approximations made by exi
 sting methods are based on unrealistic assumptions. We formulate a more fl
 exible and general class of assumptions based on variational distributions
  and use them to tractably generate lower bounds for mutual information. T
 hese bounds define a novel information-theoretic framework for feature sel
 ection\, which we prove to be optimal under tree graphical models with pro
 per choice of variational distributions. Our experiments demonstrate that 
 the proposed method strongly outperforms existing information-theoretic fe
 ature selection approaches.
LOCATION:Area 5+6+7+8 #139
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Minimax Approach to Supervised Learning | Farzan Farnia \, David
  Tse
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Minimax Approach to Supervised Learning\nFarzan Farni
 a \, David Tse\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7173\n\
 nGiven a task of predicting Y from X\, a loss function L\, and a set of pr
 obability distributions Gamma on (X\,Y)\, what is the optimal decision rul
 e minimizing the worst-case expected loss over Gamma? In this paper\, we a
 ddress this question by introducing a generalization of the maximum entrop
 y principle. Applying this principle to sets of distributions with margina
 l on X constrained to be the empirical marginal\, we provide a minimax int
 erpretation of the maximum likelihood problem over generalized linear mode
 ls\, which connects the minimax problem for each loss function to a genera
 lized linear model. While in some cases such as quadratic and logarithmic 
 loss functions we revisit well-known linear and logistic regression models
 \, our approach reveals novel models for other loss functions. In particul
 ar\, for the 0-1 loss we derive a classification approach which we call th
 e minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss 
 over the proposed Gamma by solving a tractable optimization problem. We pe
 rform several numerical experiments in all of which the minimax SVM outper
 forms the SVM.
LOCATION:Area 5+6+7+8 #140
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast Distributed Submodular Cover: Public-Private Data Summarizati
 on | Baharan Mirzasoleiman \, Morteza Zadimoghaddam \, Amin Karbasi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Fast Distributed Submodular Cover: Public-Private Data 
 Summarization\nBaharan Mirzasoleiman \, Morteza Zadimoghaddam \, Amin Karb
 asi\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7174\n\nIn this pa
 per\, we introduce the public-private framework of data summarization moti
 vated by privacy concerns in personalized recommender systems and online s
 ocial services. Such systems have usually access to massive data generated
  by a large pool of users. A major fraction of the data is public and is v
 isible to (and can be used for) all users. However\, each user can also co
 ntribute some private data that should not be shared with other users to e
 nsure her privacy. The goal is to provide a succinct summary of massive da
 taset\, ideally as small as possible\, from which customized summaries can
  be built for each user\, i.e. it can contain elements from the public dat
 a (for diversity) and users' private data (for personalization). To formal
 ize the above challenge\, we assume that the scoring function according to
  which a user evaluates the utility of her summary satisfies submodularity
 \, a widely used notion in data summarization applications. Thus\, we mode
 l the data summarization targeted to each user as an instance of a submodu
 lar cover problem. However\, when the data is massive it is infeasible to 
 use the centralized greedy algorithm to find a customized summary even for
  a single user. Moreover\, for a large pool of users\, it is too time cons
 uming to find such summaries separately. Instead\, we develop a fast distr
 ibuted algorithm for submodular cover\, FASTCOVER\, that provides a succin
 ct summary in one shot and for all users. We show that the solution provid
 ed by FASTCOVER is competitive with that of the centralized algorithm with
  the number of rounds that is exponentially smaller than state of the art 
 results. Moreover\, we have implemented FASTCOVER with Spark to demonstrat
 e its practical performance on a number of concrete applications\, includi
 ng personalized location recommendation\, personalized movie recommendatio
 n\, and dominating set on tens of millions of data points and varying numb
 er of users.
LOCATION:Area 5+6+7+8 #141
END:VEVENT
BEGIN:VEVENT
SUMMARY:Domain Separation Networks | Konstantinos Bousmalis \, George Trig
 eorgis \, Nathan Silberman \, Dilip Krishnan \, Dumitru Erhan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Domain Separation Networks\nKonstantinos Bousmalis \, G
 eorge Trigeorgis \, Nathan Silberman \, Dilip Krishnan \, Dumitru Erhan\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7175\n\nThe cost of larg
 e scale data collection and annotation often makes the application of mach
 ine learning algorithms to new tasks or datasets prohibitively expensive. 
 One approach circumventing this cost is training models on synthetic data 
 where annotations are provided automatically. Despite their appeal\, such 
 models often fail to generalize from synthetic to real images\, necessitat
 ing domain adaptation algorithms to manipulate these models before they ca
 n be successfully applied. Existing approaches focus either on mapping rep
 resentations from one domain to the other\, or on learning to extract feat
 ures that are invariant to the domain from which they were extracted. Howe
 ver\, by focusing only on creating a mapping or shared representation betw
 een the two domains\, they ignore the individual characteristics of each d
 omain. We hypothesize that explicitly modeling what is unique to each doma
 in can improve a model's ability to extract domain-invariant features. Ins
 pired by work on private-shared component analysis\, we explicitly learn t
 o extract image representations that are partitioned into two subspaces: o
 ne component which is private to each domain and one which is shared acros
 s domains. Our model is trained to not only perform the task we care about
  in the source domain\, but also to use the partitioned representation to 
 reconstruct the images from both domains. Our novel architecture results i
 n a model that outperforms the state-of-the-art on a range of unsupervised
  domain adaptation scenarios and additionally produces  visualizations of 
 the private and shared representations enabling interpretation of the doma
 in adaptation process.
LOCATION:Area 5+6+7+8 #142
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multimodal Residual Learning for Visual QA | Jin-Hwa Kim \, Sang-W
 oo Lee \, Donghyun Kwak \, Min-Oh Heo \, Jeonghee Kim \, Jung-Woo Ha \, By
 oung-Tak Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Multimodal Residual Learning for Visual QA\nJin-Hwa Kim
  \, Sang-Woo Lee \, Donghyun Kwak \, Min-Oh Heo \, Jeonghee Kim \, Jung-Wo
 o Ha \, Byoung-Tak Zhang\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7176\n\nDeep neural networks continue to advance the state-of-the-art o
 f image recognition tasks with various methods. However\, applications of 
 these methods to multimodality remain limited. We present Multimodal Resid
 ual Networks (MRN) for the multimodal residual learning of visual question
 -answering\, which extends the idea of the deep residual learning. Unlike 
 the deep residual learning\, MRN effectively learns the joint representati
 on from visual and language information. The main idea is to use element-w
 ise multiplication for the joint residual mappings exploiting the residual
  learning of the attentional models in recent studies. Various alternative
  models introduced by multimodality are explored based on our study. We ac
 hieve the state-of-the-art results on the Visual QA dataset for both Open-
 Ended and Multiple-Choice tasks. Moreover\, we introduce a novel method to
  visualize the attention effect of the joint representations for each lear
 ning block using back-propagation algorithm\, even though the visual featu
 res are collapsed without spatial information.
LOCATION:Area 5+6+7+8 #143
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimizing affinity-based binary hashing using auxiliary coordinat
 es | Ramin Raziperchikolaei \, Miguel A. Carreira-Perpinan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimizing affinity-based binary hashing using auxiliar
 y coordinates\nRamin Raziperchikolaei \, Miguel A. Carreira-Perpinan\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=7177\n\nIn supervised binar
 y hashing\, one wants to learn a function that maps a high-dimensional fea
 ture vector to a vector of binary codes\, for application to fast image re
 trieval. This typically results in a difficult optimization problem\, nonc
 onvex and nonsmooth\, because of the discrete variables involved. Much wor
 k has simply relaxed the problem during training\, solving a continuous op
 timization\, and truncating the codes a posteriori. This gives reasonable 
 results but is quite suboptimal. Recent work has tried to optimize the obj
 ective directly over the binary codes and achieved better results\, but th
 e hash function was still learned a posteriori\, which remains suboptimal.
  We propose a general framework for learning hash functions using affinity
 -based loss functions that uses auxiliary coordinates. This closes the loo
 p and optimizes jointly over the hash functions and the binary codes so th
 at they gradually match each other. The resulting algorithm can be seen as
  an iterated version of the procedure of optimizing first over the codes a
 nd then learning the hash function. Compared to this\, our optimization is
  guaranteed to obtain better hash functions while being not much slower\, 
 as demonstrated experimentally in various supervised datasets. In addition
 \, our framework facilitates the design of optimization algorithms for arb
 itrary types of loss and hash functions.
LOCATION:Area 5+6+7+8 #144
END:VEVENT
BEGIN:VEVENT
SUMMARY:Coresets for Scalable Bayesian Logistic Regression | Jonathan Hugg
 ins \, Trevor Campbell \, Tamara Broderick
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Coresets for Scalable Bayesian Logistic Regression\nJon
 athan Huggins \, Trevor Campbell \, Tamara Broderick\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7178\n\nThe use of Bayesian methods in larg
 e-scale data settings is attractive because of the rich hierarchical model
 s\, uncertainty quantification\, and prior specification they provide. Sta
 ndard Bayesian inference algorithms are computationally expensive\, howeve
 r\, making their direct application to large datasets difficult or infeasi
 ble. Recent work on scaling Bayesian inference has focused on modifying th
 e underlying algorithms to\, for example\, use only a random data subsampl
 e at each iteration. We leverage the insight that data is often redundant 
 to instead obtain a weighted subset of the data (called a coreset) that is
  much smaller than the original dataset. We can then use this small corese
 t in any number of existing posterior inference algorithms without modific
 ation. In this paper\, we develop an efficient coreset construction algori
 thm for Bayesian logistic regression models. We provide theoretical guaran
 tees on the size and approximation quality of the coreset -- both for fixe
 d\, known datasets\, and in expectation for a wide class of data generativ
 e models. Crucially\, the proposed approach also permits efficient constru
 ction of the coreset in both streaming and parallel settings\, with minima
 l additional effort. We demonstrate the efficacy of our approach on a numb
 er of synthetic and real-world datasets\, and find that\, in practice\, th
 e size of the coreset is independent of the original dataset size. Further
 more\, constructing the coreset takes a negligible amount of time compared
  to that required to run MCMC on it.
LOCATION:Area 5+6+7+8 #145
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Parallel Knowledge Gradient Method for Batch Bayesian Optimiza
 tion | Jian Wu \, Peter Frazier
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:The Parallel Knowledge Gradient Method for Batch Bayesi
 an Optimization\nJian Wu \, Peter Frazier\nhttp://nips.cc/Conferences/2016
 /Schedule?showEvent=7179\n\nIn many applications of black-box optimization
 \, one can evaluate multiple points simultaneously\, e.g. when evaluating 
 the performances of several different neural network architectures in a pa
 rallel computing environment.  In this paper\, we develop a novel batch Ba
 yesian optimization algorithm --- the parallel knowledge gradient method. 
 By construction\, this method provides the one-step Bayes optimal batch of
  points to sample. We provide an efficient strategy for computing this Bay
 es-optimal batch of points\, and we demonstrate that the parallel knowledg
 e gradient method finds global optima significantly faster than previous b
 atch Bayesian optimization algorithms on both synthetic test functions and
  when tuning hyperparameters of practical machine learning algorithms\, es
 pecially when function evaluations are noisy.
LOCATION:Area 5+6+7+8 #146
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Multiagent Communication with Backpropagation | Sainbayar
  Sukhbaatar \, arthur szlam \, Rob Fergus
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning Multiagent Communication with Backpropagation\
 nSainbayar Sukhbaatar \, arthur szlam \, Rob Fergus\nhttp://nips.cc/Confer
 ences/2016/Schedule?showEvent=7180\n\nMany tasks in AI require the collabo
 ration of multiple agents. Typically\, the communication protocol between 
 agents is manually specified and not altered during training. In this pape
 r we explore a simple neural model\, called CommNet\, that uses continuous
  communication for fully cooperative tasks. The model consists of multiple
  agents and the communication between them is learned alongside their poli
 cy. We apply this model to a diverse set of tasks\, demonstrating the abil
 ity of the agents to learn to communicate amongst themselves\, yielding im
 proved performance over non-communicative agents and baselines. In some ca
 ses\, it is possible to interpret the language devised by the agents\, rev
 ealing simple but effective strategies for solving the task at hand.
LOCATION:Area 5+6+7+8 #147
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Binary Classifier Aggregation for General Losses | Akshay 
 Balsubramani \, Yoav S Freund
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimal Binary Classifier Aggregation for General Losse
 s\nAkshay Balsubramani \, Yoav S Freund\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7181\n\nWe address the problem of aggregating an ensembl
 e of predictors with known loss bounds in a semi-supervised binary classif
 ication setting\, to minimize prediction loss incurred on the unlabeled da
 ta. We find the minimax optimal predictions for a very general class of lo
 ss functions including all convex and many non-convex losses\, extending a
  recent analysis of the problem for misclassification error. The result is
  a family of semi-supervised ensemble aggregation algorithms which are as 
 efficient as linear learning by convex optimization\, but are minimax opti
 mal without any relaxations. Their decision rules take a form familiar in 
 decision theory -- applying sigmoid functions to a notion of ensemble marg
 in -- without the assumptions typically made in margin-based learning.
LOCATION:Area 5+6+7+8 #148
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Generalized Reparameterization Gradient | Francisco R Ruiz \, 
 Michalis Titsias RC AUEB \, David Blei
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:The Generalized Reparameterization Gradient\nFrancisco 
 R Ruiz \, Michalis Titsias RC AUEB \, David Blei\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=7182\n\nThe reparameterization gradient has bec
 ome a widely used method to obtain Monte Carlo gradients to optimize the v
 ariational objective. However\, this technique does not easily apply to co
 mmonly used distributions such as beta or gamma without further approximat
 ions\, and most practical applications of the reparameterization gradient 
 fit Gaussian distributions. In this paper\, we introduce the generalized r
 eparameterization gradient\, a method that extends the reparameterization 
 gradient to a wider class of variational distributions. Generalized repara
 meterizations use invertible transformations of the latent variables which
  lead to transformed distributions that weakly depend on the variational p
 arameters. This results in new Monte Carlo gradients that combine reparame
 terization gradients and score function gradients. We demonstrate our appr
 oach on variational inference for two complex probabilistic models. The ge
 neralized reparameterization is effective: even a single sample from the v
 ariational distribution is enough to obtain a low-variance gradient.
LOCATION:Area 5+6+7+8 #149
END:VEVENT
BEGIN:VEVENT
SUMMARY:Conditional Generative Moment-Matching Networks | Yong Ren \, Jun 
 Zhu \, Jialian Li \, Yucen Luo
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Conditional Generative Moment-Matching Networks\nYong R
 en \, Jun Zhu \, Jialian Li \, Yucen Luo\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7183\n\nMaximum mean discrepancy (MMD) has been success
 fully applied to learn deep generative models for characterizing a joint d
 istribution of variables via kernel mean embedding. In this paper\, we pre
 sent conditional generative moment-matching networks (CGMMN)\, which learn
  a conditional distribution given some input variables based on a conditio
 nal maximum mean discrepancy (CMMD) criterion. The learning is performed b
 y stochastic gradient descent with the gradient calculated by back-propaga
 tion. We evaluate CGMMN on a wide range of tasks\, including predictive mo
 deling\, contextual generation\, and Bayesian dark knowledge\, which disti
 lls knowledge from a Bayesian model by learning a relatively small CGMMN s
 tudent network. Our results demonstrate competitive performance in all the
  tasks.
LOCATION:Area 5+6+7+8 #150
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Credit Assignment Compiler for Joint Prediction | Kai-Wei Chang 
 \, He He \, Stephane Ross \, Hal Daume III \, John Langford
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:A Credit Assignment Compiler for Joint Prediction\nKai-
 Wei Chang \, He He \, Stephane Ross \, Hal Daume III \, John Langford\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7184\n\nMany machine learn
 ing applications involve jointly predicting multiple mutually dependent ou
 tput variables. Learning to search is a family of methods where the comple
 x decision problem is cast into a sequence of decisions via a search space
 . Although these methods have shown promise both in theory and in practice
 \, implementing them has been burdensomely awkward. In this paper\, we sho
 w the search space can be defined by an arbitrary imperative program\, tur
 ning learning to search into a credit assignment compiler. Altogether with
  the algorithmic improvements for the compiler\, we radically reduce the c
 omplexity of programming and the running time. We demonstrate the feasibil
 ity of our approach on multiple joint prediction tasks. In all cases\, we 
 obtain accuracies as high as alternative approaches\, at drastically reduc
 ed execution and programming time.
LOCATION:Area 5+6+7+8 #151
END:VEVENT
BEGIN:VEVENT
SUMMARY:Short-Dot: Computing Large Linear Transforms Distributedly Using C
 oded Short Dot Products | Sanghamitra Dutta \, Viveck Cadambe \, Pulkit Gr
 over
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Short-Dot: Computing Large Linear Transforms Distribute
 dly Using Coded Short Dot Products\nSanghamitra Dutta \, Viveck Cadambe \,
  Pulkit Grover\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7185\n\
 nFaced with saturation of Moore's law and increasing size and dimension of
  data\, system designers have increasingly resorted to parallel and distri
 buted computing to reduce computation time of machine-learning algorithms.
  However\, distributed computing is often bottle necked by a small fractio
 n of slow processors called "stragglers" that reduce the speed of computat
 ion because the fusion node has to wait for all processors to complete the
 ir processing. To combat the effect of stragglers\, recent literature prop
 oses introducing redundancy in computations across processors\, e.g.\, usi
 ng repetition-based strategies or erasure codes. The fusion node can explo
 it this redundancy by completing the computation using outputs from only a
  subset of the processors\, ignoring the stragglers. In this paper\, we pr
 opose a novel technique - that we call "Short-Dot" - to introduce redundan
 t computations in a coding theory inspired fashion\, for computing linear 
 transforms of long vectors. Instead of computing long dot products as requ
 ired in the original linear transform\, we construct a larger number of re
 dundant and short dot products that can be computed more efficiently at in
 dividual processors. Further\, only a subset of these short dot products a
 re required at the fusion node to finish the computation successfully. We 
 demonstrate through probabilistic analysis as well as experiments on compu
 ting clusters that Short-Dot offers significant speed-up compared to exist
 ing techniques. We also derive trade-offs between the length of the dot-pr
 oducts and the resilience to stragglers (number of processors required to 
 finish)\, for any such strategy and compare it to that achieved by our str
 ategy.
LOCATION:Area 5+6+7+8 #152
END:VEVENT
BEGIN:VEVENT
SUMMARY:Spatio-Temporal Hilbert Maps for Continuous Occupancy Representati
 on in Dynamic Environments | Ransalu Senanayake \, Lionel Ott \, Simon O'C
 allaghan \, Fabio Ramos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Spatio-Temporal Hilbert Maps for Continuous Occupancy R
 epresentation in Dynamic Environments\nRansalu Senanayake \, Lionel Ott \,
  Simon O'Callaghan \, Fabio Ramos\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7186\n\nWe consider the problem of building continuous occupan
 cy representations in  dynamic environments for robotics applications. The
  problem has hardly been discussed previously due to the complexity of pat
 terns in urban environments\,  which have both spatial and temporal depend
 encies. We address the problem  as learning a kernel classifier on an effi
 cient feature space. The key novelty of  our approach is the incorporation
  of variations in the time domain into the spatial  domain. We propose a m
 ethod to propagate motion uncertainty into the kernel using a hierarchical
  model. The main benefit of this approach is that it can directly predict 
  the occupancy state of the map in the future from past observations\, bei
 ng a valuable  tool for robot trajectory planning under uncertainty. Our a
 pproach preserves the  main computational benefits of static Hilbert maps 
 — using stochastic gradient  descent for fast optimization of model para
 meters and incremental updates as  new data are captured. Experiments cond
 ucted in road intersections of an urban  environment demonstrated that spa
 tio-temporal Hilbert maps can accurately model  changes in the map while o
 utperforming other techniques on various aspects.
LOCATION:Area 5+6+7+8 #153
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning HMMs with Nonparametric Emissions via Spectral Decomposit
 ions of Continuous Matrices | Kirthevasan Kandasamy \, Maruan Al-Shedivat 
 \, Eric P Xing
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning HMMs with Nonparametric Emissions via Spectral
  Decompositions of Continuous Matrices\nKirthevasan Kandasamy \, Maruan Al
 -Shedivat \, Eric P Xing\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7187\n\nRecently\, there has been a surge of interest in using spectral
  methods for estimating latent variable models. However\, it is usually as
 sumed that the distribution of the observations conditioned on the latent 
 variables is either discrete or belongs to a parametric family. In this pa
 per\, we study the estimation of an $m$-state hidden Markov model (HMM) wi
 th only smoothness assumptions\, such as H\\"olderian conditions\, on the 
 emission densities. By leveraging some recent advances in continuous linea
 r algebra and numerical analysis\, we develop a computationally efficient 
 spectral algorithm for learning nonparametric HMMs. Our technique is based
  on computing an SVD on nonparametric estimates of density functions by vi
 ewing them as \\emph{continuous matrices}. We derive sample complexity bou
 nds via concentration results for nonparametric density estimation and nov
 el perturbation theory results for continuous matrices. We implement our m
 ethod using Chebyshev polynomial approximations. Our method is competitive
  with other baselines on synthetic and real problems and is also very comp
 utationally efficient.
LOCATION:Area 5+6+7+8 #154
END:VEVENT
BEGIN:VEVENT
SUMMARY:Integrated perception with recurrent multi-task neural networks | 
 Hakan Bilen \, Andrea Vedaldi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Integrated perception with recurrent multi-task neural 
 networks\nHakan Bilen \, Andrea Vedaldi\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7188\n\nModern discriminative predictors have been shown
  to match natural intelligences in specific perceptual tasks in image clas
 sification\, object and part detection\, boundary extraction\, etc. Howeve
 r\, a major advantage that natural intelligences still have is that they w
 ork well for all perceptual problems together\, solving them efficiently a
 nd coherently in an integrated manner. In order to capture some of these a
 dvantages in machine perception\, we ask two questions: whether deep neura
 l networks can learn universal image representations\, useful not only for
  a single task but for all of them\, and how the solutions to the differen
 t tasks can be integrated in this framework. We answer by proposing a new 
 architecture\, which we call multinet\, in which not only deep image featu
 res are shared between tasks\, but where tasks can interact in a recurrent
  manner by encoding the results of their analysis in a common shared repre
 sentation of the data. In this manner\, we show that the performance of in
 dividual tasks in standard benchmarks can be improved first by sharing fea
 tures between them and then\, more significantly\, by integrating their so
 lutions in the common representation.
LOCATION:Area 5+6+7+8 #155
END:VEVENT
BEGIN:VEVENT
SUMMARY:Blind Attacks on Machine Learners | Alex Beatson \, Zhaoran Wang \
 , Han Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Blind Attacks on Machine Learners\nAlex Beatson \, Zhao
 ran Wang \, Han Liu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=71
 89\n\nThe importance of studying the robustness of learners to malicious d
 ata is well established. While much work has been done establishing both r
 obust estimators and effective data injection attacks when the attacker is
  omniscient\, the ability of an attacker to provably harm learning while h
 aving access to little information is largely unstudied. We study the pote
 ntial of a “blind attacker” to provably limit a learner’s performanc
 e by data injection attack without observing the learner’s training set 
 or any parameter of the distribution from which it is drawn. We provide ex
 amples of simple yet effective attacks in two settings: firstly\, where an
  “informed learner” knows the strategy chosen by the attacker\, and se
 condly\, where a “blind learner” knows only the proportion of maliciou
 s data and some family to which the malicious distribution chosen by the a
 ttacker belongs. For each attack\, we analyze minimax rates of convergence
  and establish lower bounds on the learner’s minimax risk\, exhibiting l
 imits on a learner’s ability to learn under data injection attack even w
 hen the attacker is “blind”.
LOCATION:Area 5+6+7+8 #156
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimistic Gittins Indices | Eli Gutin \, Vivek Farias
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimistic Gittins Indices\nEli Gutin \, Vivek Farias\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7190\n\nStarting with t
 he Thomspon sampling algorithm\, recent years have seen a resurgence of in
 terest in Bayesian algorithms for the Multi-armed Bandit (MAB) problem. Th
 ese algorithms seek to exploit prior information on arm biases and while s
 everal have been shown to be regret optimal\, their design has not emerged
  from a principled approach. In contrast\, if one cared about Bayesian reg
 ret discounted over an infinite horizon at a fixed\, pre-specified rate\, 
 the celebrated Gittins index theorem offers an optimal algorithm. Unfortun
 ately\, the Gittins analysis does not appear to carry over to minimizing B
 ayesian regret over all sufficiently large horizons and computing a Gittin
 s index is onerous relative to essentially any incumbent index scheme for 
 the Bayesian MAB problem.   The present paper proposes a sequence of 'opti
 mistic' approximations to the Gittins index. We show that the use of these
  approximations in concert with the use of an increasing discount factor a
 ppears to offer a compelling alternative to a variety of index schemes pro
 posed for the Bayesian MAB problem in recent years. In addition\, we show 
 that the simplest of these approximations yields regret that matches the L
 ai-Robbins lower bound\, including achieving matching constants.
LOCATION:Area 5+6+7+8 #157
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sub-sampled Newton Methods with Non-uniform Sampling | Peng Xu \, 
 Jiyan Yang \, Farbod Roosta-Khorasani \, Christopher Ré \, Michael W Maho
 ney
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Sub-sampled Newton Methods with Non-uniform Sampling\nP
 eng Xu \, Jiyan Yang \, Farbod Roosta-Khorasani \, Christopher Ré \, Mich
 ael W Mahoney\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7191\n\n
 We consider the problem of finding the minimizer of a convex function $F: 
 \\mathbb R^d \\rightarrow \\mathbb R$ of the form $F(w) \\defeq \\sum{i=1}
 ^n fi(w) + R(w)$ where a low-rank factorization of $\\nabla^2 fi(w)$ is re
 adily available.We consider the regime where $n \\gg d$. We propose random
 ized Newton-type algorithms that exploit \\textit{non-uniform} sub-samplin
 g of ${\\nabla^2 fi(w)}_{i=1}^{n}$\, as well as inexact updates\, as means
  to reduce the computational complexity\, and are applicable to a wide ran
 ge of problems in machine learning. Two non-uniform sampling distributions
  based on {\\it block norm squares} and {\\it block partial leverage score
 s} are considered. Under certain assumptions\, we show that our algorithms
  inherit a linear-quadratic convergence rate in $w$ and achieve a lower co
 mputational complexity compared to similar existing methods.  In addition\
 , we show that our algorithms exhibit more robustness and better dependenc
 e on problem specific quantities\, such as the condition number. We numeri
 cally demonstrate the advantages of our algorithms on several real dataset
 s.
LOCATION:Area 5+6+7+8 #158
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learned Region Sparsity and Diversity Also Predicts Visual Attenti
 on | Zijun Wei \, Hossein Adeli \, Minh Hoai \, Greg Zelinsky \, Dimitris 
 Samaras
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learned Region Sparsity and Diversity Also Predicts Vis
 ual Attention\nZijun Wei \, Hossein Adeli \, Minh Hoai \, Greg Zelinsky \,
  Dimitris Samaras\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7192
 \n\nLearned region sparsity has achieved state-of-the-art performance in c
 lassification tasks by exploiting and integrating a sparse set of local in
 formation into global decisions. The underlying mechanism resembles how pe
 ople sample information from an image with their eye movements when making
  similar decisions. In this paper we incorporate the biologically plausibl
 e mechanism of Inhibition of Return into the learned region sparsity model
 \, thereby imposing diversity on the selected regions. We investigate how 
 these mechanisms of sparsity and diversity relate to visual attention by t
 esting our model on three different types of visual search tasks. We repor
 t state-of-the-art results in predicting the locations of human gaze fixat
 ions\, even though our model is trained only on image-level labels without
  object location annotations. Notably\, the classification performance of 
 the extended model  remains the same as the original. This work suggests a
  new computational perspective on visual attention mechanisms and shows ho
 w the inclusion of attention-based mechanisms can improve computer vision 
 techniques.
LOCATION:Area 5+6+7+8 #159
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Concentration Inequalities for Sequential Decision Proble
 ms | Shengjia Zhao \, Enze Zhou \, Ashish Sabharwal \, Stefano Ermon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Adaptive Concentration Inequalities for Sequential Deci
 sion Problems\nShengjia Zhao \, Enze Zhou \, Ashish Sabharwal \, Stefano E
 rmon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7193\n\nA key cha
 llenge in sequential decision problems is to determine how many samples ar
 e needed for an agent to make reliable decisions with good probabilistic g
 uarantees.   We introduce Hoeffding-like concentration inequalities that h
 old for a random\, adaptively chosen number of samples. Our inequalities a
 re tight under natural assumptions and can greatly simplify the analysis o
 f common sequential decision problems. In particular\, we apply them to se
 quential hypothesis testing\, best arm identification\, and sorting. The r
 esulting algorithms rival or exceed the state of the art both theoreticall
 y and empirically.
LOCATION:Area 5+6+7+8 #160
END:VEVENT
BEGIN:VEVENT
SUMMARY:Cooperative Graphical Models | Josip Djolonga \, Stefanie Jegelka 
 \, Sebastian Tschiatschek \, Andreas Krause
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Cooperative Graphical Models\nJosip Djolonga \, Stefani
 e Jegelka \, Sebastian Tschiatschek \, Andreas Krause\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=7194\n\nWe study a rich family of distribu
 tions that capture variable interactions significantly more expressive tha
 n those representable with low-treewidth or pairwise graphical models\, or
  log-supermodular models. We call these cooperative graphical models. Yet\
 , this family retains structure\, which we carefully exploit for efficient
  inference techniques. Our algorithms combine the polyhedral structure of 
 submodular functions in new ways with variational inference methods to obt
 ain both lower and upper bounds on the partition function. While our fully
  convex upper bound is minimized as an SDP or via tree-reweighted belief p
 ropagation\, our lower bound is tightened via belief propagation or mean-f
 ield algorithms. The resulting algorithms are easy to implement and\, as o
 ur experiments show\, effectively obtain good bounds and marginals for syn
 thetic and real-world examples.
LOCATION:Area 5+6+7+8 #161
END:VEVENT
BEGIN:VEVENT
SUMMARY:Correlated-PCA: Principal Components' Analysis when Data and Noise
  are Correlated | Namrata Vaswani \, Han Guo
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Correlated-PCA: Principal Components' Analysis when Dat
 a and Noise are Correlated\nNamrata Vaswani \, Han Guo\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7195\n\nGiven a matrix of observed data\,
  Principal Components Analysis (PCA) computes a small number of orthogonal
  directions that contain most of its variability. Provably accurate soluti
 ons for PCA have been in use for a long time. However\, to the best of our
  knowledge\, all existing theoretical guarantees for it assume that the da
 ta and the corrupting noise  are mutually independent\, or at least uncorr
 elated. This is valid in practice often\, but not always. In this paper\, 
 we study the PCA problem in the setting where the data and noise can be co
 rrelated. Such noise is often also referred to as ``data-dependent noise".
  We obtain a correctness result for the standard eigenvalue decomposition 
 (EVD) based solution to PCA under simple assumptions on the data-noise cor
 relation. We also develop and analyze a generalization of EVD\, cluster-EV
 D\, that improves upon EVD in certain regimes.
LOCATION:Area 5+6+7+8 #162
END:VEVENT
BEGIN:VEVENT
SUMMARY:Hierarchical Object Representation for Open-Ended Object Category 
 Learning and Recognition | Seyed Hamidreza Kasaei \, ana Tome \, Luis Lope
 s
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Hierarchical Object Representation for Open-Ended Objec
 t Category Learning and Recognition\nSeyed Hamidreza Kasaei \, ana Tome \,
  Luis Lopes\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7196\n\nMo
 st robots lack the ability to learn new objects from past experiences. To 
 migrate a robot to a new environment one must often completely re-generate
  the knowledge- base that it is running with. Since in open-ended domains 
 the set of categories to be learned is not predefined\, it is not feasible
  to assume that one can pre-program all object categories required by robo
 ts. Therefore\, autonomous robots must have the ability to continuously ex
 ecute learning and recognition in a concurrent and interleaved fashion. Th
 is paper proposes an open-ended 3D object recognition system which concurr
 ently learns both the object categories and the statistical features for e
 ncoding objects. In particular\, we propose an extension of Latent Dirichl
 et Allocation to learn structural semantic features (i.e. topics) from low
 -level feature co-occurrences for each category independently. Moreover\, 
 topics in each category are discovered in an unsupervised fashion and are 
 updated incrementally using new object views. The approach contains simila
 rities with the organization of the visual cortex and builds a hierarchy o
 f increasingly sophisticated representations. Results show the fulfilling 
 performance of this approach on different types of objects. Moreover\, thi
 s system demonstrates the capability of learning from few training example
 s and competes with state-of-the-art systems.
LOCATION:Area 5+6+7+8 #163
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Tagging with Markov Chain Optimization | Nir Rosenfeld \, 
 Amir Globerson
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Optimal Tagging with Markov Chain Optimization\nNir Ros
 enfeld \, Amir Globerson\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7197\n\nMany information systems use tags and keywords to describe and 
 annotate content. These allow for efficient organization and categorizatio
 n of items\, as well as facilitate relevant search queries. As such\, the 
 selected set of tags for an item can have a considerable effect on the vol
 ume of traffic that eventually reaches an item.  In tagging systems where 
 tags are exclusively chosen by an item's owner\, who in turn is interested
  in maximizing traffic\, a principled approach for assigning tags can prov
 e valuable. In this paper we introduce the problem of optimal tagging\, wh
 ere the task is to choose a subset of tags for a new item such that the pr
 obability of browsing users reaching that item is maximized.  We formulate
  the problem by modeling traffic using a Markov chain\, and asking how tra
 nsitions in this chain should be modified to maximize traffic into a certa
 in state of interest. The resulting optimization problem involves maximizi
 ng a certain function over subsets\, under a cardinality constraint.  We s
 how that the optimization problem is NP-hard\, but has a (1-1/e)-approxima
 tion via a simple greedy algorithm due to monotonicity and submodularity. 
 Furthermore\, the structure of the problem allows for an efficient computa
 tion of the greedy step. To demonstrate the effectiveness of our method\, 
 we perform experiments on three tagging datasets\, and show that the greed
 y algorithm outperforms other baselines.
LOCATION:Area 5+6+7+8 #164
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian optimization for automated model selection | Gustavo Malk
 omes \, Charles Schaff \, Roman Garnett
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Bayesian optimization for automated model selection\nGu
 stavo Malkomes \, Charles Schaff \, Roman Garnett\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7198\n\nDespite the success of kernel-based no
 nparametric methods\, kernel selection still requires considerable experti
 se\, and is often described as a “black art.” We present a sophisticat
 ed method for automatically searching for an appropriate kernel from an in
 finite space of potential choices. Previous efforts in this direction have
  focused on traversing a kernel grammar\, only examining the data via comp
 utation of marginal likelihood. Our proposed search method is based on Bay
 esian optimization in model space\, where we reason about model evidence a
 s a function to be maximized. We explicitly reason about the data distribu
 tion and how it induces similarity between potential model choices in term
 s of the explanations they can offer for observed data. In this light\, we
  construct a novel kernel between models to explain a given dataset. Our m
 ethod is capable of finding a model that explains a given dataset well wit
 hout any human assistance\, often with fewer computations of model evidenc
 e than previous approaches\, a claim we demonstrate empirically.
LOCATION:Area 5+6+7+8 #165
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multi-view Anomaly Detection via Robust Probabilistic Latent Varia
 ble Models | Tomoharu Iwata \, Makoto Yamada
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Multi-view Anomaly Detection via Robust Probabilistic L
 atent Variable Models\nTomoharu Iwata \, Makoto Yamada\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7199\n\nWe propose probabilistic latent v
 ariable models for multi-view anomaly detection\, which is the task of fin
 ding instances that have inconsistent views given multi-view data. With th
 e proposed model\, all views of a non-anomalous instance are assumed to be
  generated from a single latent vector. On the other hand\, an anomalous i
 nstance is assumed to have multiple latent vectors\, and its different vie
 ws are generated from different latent vectors. By inferring the number of
  latent vectors used for each instance with Dirichlet process priors\, we 
 obtain multi-view anomaly scores. The proposed model can be seen as a robu
 st extension of probabilistic canonical correlation analysis for noisy mul
 ti-view data. We present Bayesian inference procedures for the proposed mo
 del based on a stochastic EM algorithm. The effectiveness of the proposed 
 model is demonstrated in terms of performance when detecting multi-view an
 omalies.
LOCATION:Area 5+6+7+8 #166
END:VEVENT
BEGIN:VEVENT
SUMMARY:Inference by Reparameterization in Neural Population Codes | Rajku
 mar Vasudeva Raju \, Xaq Pitkow
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Inference by Reparameterization in Neural Population Co
 des\nRajkumar Vasudeva Raju \, Xaq Pitkow\nhttp://nips.cc/Conferences/2016
 /Schedule?showEvent=7200\n\nBehavioral experiments on humans and animals s
 uggest that the brain performs probabilistic inference to interpret its en
 vironment. Here we present a new general-purpose\, biologically-plausible 
 neural implementation of approximate inference. The neural network represe
 nts uncertainty using Probabilistic Population Codes (PPCs)\, which are di
 stributed neural representations that naturally encode probability distrib
 utions\, and support marginalization and evidence integration in a biologi
 cally-plausible manner. By connecting multiple PPCs together as a probabil
 istic graphical model\, we represent multivariate probability distribution
 s. Approximate inference in graphical models can be accomplished by messag
 e-passing algorithms that disseminate local information throughout the gra
 ph. An attractive and often accurate example of such an algorithm is Loopy
  Belief Propagation (LBP)\, which uses local marginalization and evidence 
 integration operations to perform approximate inference efficiently even f
 or complex models. Unfortunately\, a subtle feature of LBP renders it neur
 ally implausible. However\, LBP can be elegantly reformulated as a sequenc
 e of Tree-based Reparameterizations (TRP) of the graphical model. We re-ex
 press the TRP updates as a nonlinear dynamical system with both fast and s
 low timescales\, and show that this produces a neurally plausible solution
 . By combining all of these ideas\, we show that a network of PPCs can rep
 resent multivariate probability distributions and implement the TRP update
 s to perform probabilistic inference. Simulations with Gaussian graphical 
 models demonstrate that the neural network inference quality is comparable
  to the direct evaluation of LBP and robust to noise\, and thus provides a
  promising mechanism for general probabilistic inference in the population
  codes of the brain.
LOCATION:Area 5+6+7+8 #167
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient Neural Codes under Metabolic Constraints | Zhuo Wang \, 
 Xue-Xin Wei \, Alan A Stocker \, Daniel D Lee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Efficient Neural Codes under Metabolic Constraints\nZhu
 o Wang \, Xue-Xin Wei \, Alan A Stocker \, Daniel D Lee\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7201\n\nNeural codes are inevitably shap
 ed by various kinds of biological constraints\, \\emph{e.g.} noise and met
 abolic cost. Here we formulate a coding framework which explicitly deals w
 ith noise and the metabolic costs associated with the neural representatio
 n of information\, and analytically derive the optimal neural code for mon
 otonic response functions and arbitrary stimulus distributions. For a sing
 le neuron\, the theory predicts a family of optimal response functions dep
 ending on the metabolic budget and noise characteristics. Interestingly\, 
 the well-known histogram equalization solution can be viewed as a special 
 case when metabolic resources are unlimited. For a pair of neurons\, our t
 heory suggests that under more severe metabolic constraints\, ON-OFF codin
 g is an increasingly more efficient coding scheme compared to ON-ON or OFF
 -OFF. The advantage could be as large as one-fold\, substantially larger t
 han the previous estimation. Some of these predictions could be generalize
 d to the case of large neural populations. In particular\, these analytica
 l results may provide a theoretical basis for the predominant segregation 
 into ON- and OFF-cells in early visual processing areas. Overall\, we prov
 ide a unified framework for optimal neural codes with monotonic tuning cur
 ves in the brain\, and makes predictions that can be directly tested with 
 physiology experiments.
LOCATION:Area 5+6+7+8 #168
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Deep Parsimonious Representations | Renjie Liao \, Alex S
 chwing \, Richard Zemel \, Raquel Urtasun
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning Deep Parsimonious Representations\nRenjie Liao
  \, Alex Schwing \, Richard Zemel \, Raquel Urtasun\nhttp://nips.cc/Confer
 ences/2016/Schedule?showEvent=7202\n\nIn this paper we aim at facilitating
  generalization for deep networks while supporting interpretability of the
  learned representations. Towards this goal\, we propose a clustering base
 d regularization that encourages parsimonious representations. Our k-means
  style objective is easy to optimize and flexible  supporting various form
 s of clustering\, including sample and spatial clustering as well as co-cl
 ustering. We demonstrate the effectiveness of our approach on the tasks of
  unsupervised learning\, classification\, fine grained categorization and 
 zero-shot learning.
LOCATION:Area 5+6+7+8 #169
END:VEVENT
BEGIN:VEVENT
SUMMARY:An equivalence between high dimensional Bayes optimal inference an
 d M-estimation | Madhu Advani \, Surya Ganguli
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:An equivalence between high dimensional Bayes optimal i
 nference and M-estimation\nMadhu Advani \, Surya Ganguli\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7203\n\nDue to the computational diffic
 ulty of performing MMSE (minimum mean squared error) inference\, maximum a
  posteriori (MAP) is often used as a surrogate. However\, the accuracy of 
 MAP is suboptimal for high dimensional inference\, where the number of mod
 el parameters is of the same order as the number of samples. In this work 
 we demonstrate how MMSE performance is asymptotically achievable via optim
 ization with an appropriately selected convex penalty and regularization f
 unction which are a smoothed version of the widely applied MAP algorithm. 
 Our findings provide a new derivation and interpretation for recent optima
 l M-estimators discovered by El Karoui\, et. al. PNAS 2013 as well as exte
 nding to non-additive noise models. We demonstrate the performance of thes
 e optimal M-estimators with numerical simulations.  Overall\, at the heart
  of our work is the revelation of a remarkable equivalence between two see
 mingly very different computational problems: namely that of high dimensio
 nal Bayesian integration\, and high dimensional convex optimization.  In e
 ssence we show that the former computationally difficult integral may be c
 omputed by solving the latter\, simpler optimization problem.
LOCATION:Area 5+6+7+8 #170
END:VEVENT
BEGIN:VEVENT
SUMMARY:Minimizing Quadratic Functions in Constant Time | Kohei Hayashi \,
  Yuichi Yoshida
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Minimizing Quadratic Functions in Constant Time\nKohei 
 Hayashi \, Yuichi Yoshida\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=7204\n\nA sampling-based optimization method for quadratic functions i
 s   proposed. Our method approximately solves the following   $n$-dimensio
 nal quadratic minimization problem in constant time\,   which is independe
 nt of $n$:   $z^=\\min_{\\bv \\in \\bbR^n}\\bracket{\\bv}{A \\bv} +   n\\b
 racket{\\bv}{\\diag(\\bd)\\bv} + n\\bracket{\\bb}{\\bv}$\,   where $A \\in
  \\bbR^{n \\times n}$ is a matrix and $\\bd\,\\bb \\in \\bbR^n$   are vect
 ors. Our theoretical analysis specifies the number of   samples $k(\\delta
 \, \\epsilon)$ such that the approximated solution   $z$ satisfies $|z - z
 ^| = O(\\epsilon n^2)$ with probability   $1-\\delta$. The empirical perfo
 rmance (accuracy and runtime) is   positively confirmed by numerical exper
 iments.
LOCATION:Area 5+6+7+8 #171
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Structured Sparsity in Deep Neural Networks | Wei Wen \, 
 Chunpeng Wu \, Yandan Wang \, Yiran Chen \, Hai Li
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Learning Structured Sparsity in Deep Neural Networks\nW
 ei Wen \, Chunpeng Wu \, Yandan Wang \, Yiran Chen \, Hai Li\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=7205\n\nHigh demand for computation
  resources severely hinders deployment of large-scale Deep Neural Networks
  (DNN) in resource constrained devices. In this work\, we propose a Struct
 ured Sparsity Learning (SSL) method to regularize the structures (i.e.\, f
 ilters\, channels\, filter shapes\, and layer depth) of DNNs. SSL can: (1)
  learn a compact structure from a bigger DNN to reduce computation cost\; 
 (2) obtain a hardware-friendly structured sparsity of DNN to efficiently a
 ccelerate the DNN’s evaluation. Experimental results show that SSL achie
 ves on average 5.1X and 3.1X speedups of convolutional layer computation o
 f AlexNet against CPU and GPU\, respectively\, with off-the-shelf librarie
 s. These speedups are about twice speedups of non-structured sparsity\; (3
 ) regularize the DNN structure to improve classification accuracy. The res
 ults show that for CIFAR-10\, regularization on layer depth reduces a 20-l
 ayer Deep Residual Network (ResNet) to 18 layers while improves the accura
 cy from 91.25% to 92.60%\, which is still higher than that of original Res
 Net with 32 layers. For AlexNet\, SSL reduces the error by ~1%.
LOCATION:Area 5+6+7+8 #172
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adversarial Multiclass Classification: A Risk Minimization Perspec
 tive | Rizal Fathony \, Anqi Liu \, Kaiser Asif \, Brian Ziebart
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Adversarial Multiclass Classification: A Risk Minimizat
 ion Perspective\nRizal Fathony \, Anqi Liu \, Kaiser Asif \, Brian Ziebart
 \nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7206\n\nRecently prop
 osed adversarial classification methods have shown promising results for c
 ost sensitive and multivariate losses. In contrast with empirical risk min
 imization (ERM) methods\, which use convex surrogate losses to approximate
  the desired non-convex target loss function\, adversarial methods minimiz
 e non-convex losses by treating the properties of the training data as bei
 ng uncertain and worst case within a minimax game. Despite this difference
  in formulation\, we recast adversarial classification under zero-one loss
  as an ERM method with a novel prescribed loss function. We demonstrate a 
 number of theoretical and practical advantages over the very closely relat
 ed hinge loss ERM methods. This establishes adversarial classification und
 er the zero-one loss as a method that fills the long standing gap in multi
 class hinge loss classification\, simultaneously guaranteeing Fisher consi
 stency and universal consistency\, while also providing dual parameter spa
 rsity and high accuracy predictions in practice.
LOCATION:Area 5+6+7+8 #173
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unified Methods for Exploiting Piecewise Linear Structure in Conve
 x Optimization | Tyler B Johnson \, Carlos Guestrin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Unified Methods for Exploiting Piecewise Linear Structu
 re in Convex Optimization\nTyler B Johnson \, Carlos Guestrin\nhttp://nips
 .cc/Conferences/2016/Schedule?showEvent=7207\n\nWe develop methods for rap
 idly identifying important components of a convex optimization problem for
  the purpose of achieving fast convergence times. By considering a novel p
 roblem formulation—the minimization of a sum of piecewise functions—we
  describe a principled and general mechanism for exploiting piecewise line
 ar structure in convex optimization. This result leads to a theoretically 
 justified working set algorithm and a novel screening test\, which general
 ize and improve upon many prior results on exploiting structure in convex 
 optimization. In empirical comparisons\, we study the scalability of our m
 ethods. We find that screening scales surprisingly poorly with the size of
  the problem\, while our working set algorithm convincingly outperforms al
 ternative approaches.
LOCATION:Area 5+6+7+8 #174
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast and Provably Good Seedings for k-Means | Olivier Bachem \, Ma
 rio Lucic \, Hamed Hassani \, Andreas Krause
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Fast and Provably Good Seedings for k-Means\nOlivier Ba
 chem \, Mario Lucic \, Hamed Hassani \, Andreas Krause\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7208\n\nSeeding - the task of finding ini
 tial cluster centers - is critical in obtaining high-quality clusterings f
 or k-Means. However\, k-means++ seeding\, the state of the art algorithm\,
  does not scale well to massive datasets as it is inherently sequential an
 d requires k full passes through the data. It was recently shown that Mark
 ov chain Monte Carlo sampling can be used to efficiently approximate the s
 eeding step of k-means++. However\, this result requires assumptions on th
 e data generating distribution.  We propose a simple yet fast seeding algo
 rithm that producesprovablygood clusterings evenwithout assumptionson the 
 data. Our analysis shows that the algorithm allows for a favourable trade-
 off between solution quality and computational cost\, speeding up k-means+
 + seeding by up to several orders of magnitude. We validate our theoretica
 l results in extensive experiments on a variety of real-world data sets.
LOCATION:Area 5+6+7+8 #175
END:VEVENT
BEGIN:VEVENT
SUMMARY:Testing for Differences in Gaussian Graphical Models:  Application
 s to Brain Connectivity | Eugene Belilovsky \, Gaël Varoquaux \, Matthew 
 B Blaschko
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Testing for Differences in Gaussian Graphical Models:  
 Applications to Brain Connectivity\nEugene Belilovsky \, Gaël Varoquaux \
 , Matthew B Blaschko\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7
 209\n\nFunctional brain networks are well described and estimated from dat
 a with Gaussian Graphical Models (GGMs)\, e.g.\\ using sparse inverse cova
 riance estimators. Comparing functional connectivity of subjects in two po
 pulations calls for comparing these estimated GGMs. Our goal is to identif
 y differences in GGMs known to have similar structure. We characterize the
  uncertainty of differences with confidence intervals obtained using a par
 ametric distribution on parameters of a sparse estimator. Sparse penalties
  enable statistical guarantees and interpretable models even in high-dimen
 sional and low-sample settings. Characterizing the distributions of sparse
  models is inherently challenging as the penalties produce a biased estima
 tor. Recent work invokes the sparsity assumptions to effectively remove th
 e bias from a sparse estimator such as the lasso.  These distributions can
  be used to give confidence intervals on edges in GGMs\, and by extension 
 their differences. However\, in the case of comparing GGMs\, these estimat
 ors do not make use of any assumed joint structure among the GGMs. Inspire
 d by priors from brain functional connectivity we derive the distribution 
 of parameter differences under a joint penalty when parameters are known t
 o be sparse in the difference. This leads us to introduce the debiased mul
 ti-task fused lasso\, whose distribution can be characterized in an effici
 ent manner. We then show how the debiased lasso and multi-task fused lasso
  can be used to obtain confidence intervals on edge differences in GGMs. W
 e validate the techniques proposed on a set of synthetic examples as well 
 as neuro-imaging dataset created for the study of autism.
LOCATION:Area 5+6+7+8 #176
END:VEVENT
BEGIN:VEVENT
SUMMARY:Synthesis of MCMC and Belief Propagation | Sung-Soo Ahn \, Michael
  Chertkov \, Jinwoo Shin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Synthesis of MCMC and Belief Propagation\nSung-Soo Ahn 
 \, Michael Chertkov \, Jinwoo Shin\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=7210\n\nMarkov Chain Monte Carlo (MCMC) and Belief Propagatio
 n (BP) are the most popular algorithms for computational inference in Grap
 hical Models (GM).  In principle\, MCMC is an exact probabilistic method w
 hich\, however\, often suffers from exponentially slow mixing. In contrast
 \, BP  is a deterministic method\, which is typically fast\,  empirically 
 very successful\, however in general lacking control of accuracy over loop
 y graphs.  In this paper\, we introduce MCMC algorithms correcting the app
 roximation error of BP\, i.e.\,  we provide a way to compensate for BP err
 ors via a consecutive BP-aware MCMC.  Our framework is based on the Loop C
 alculus (LC) approach  which allows to express the BP error  as a sum of w
 eighted generalized loops. Although the full series is computationally int
 ractable\,  it is known that a truncated series\, summing up all 2-regular
  loops\, is computable in polynomial-time for planar pair-wise binary GMs 
 and it also provides a highly accurate approximation empirically. Motivate
 d by this\, we\, first\, propose a polynomial-time approximation MCMC sche
 me for the truncated series of general (non-planar) pair-wise binary model
 s.  Our main idea here is to use the Worm algorithm\, known to provide fas
 t mixing in other (related) problems\, and then  design an appropriate rej
 ection scheme to sample 2-regular loops. Furthermore\, we also design an e
 fficient rejection-free MCMC scheme  for approximating the full series.  T
 he main novelty underlying our design  is in utilizing the concept of cycl
 e basis\, which provides an efficient decomposition of the generalized loo
 ps. In essence\, the proposed MCMC schemes run on transformed GM built upo
 n  the non-trivial BP solution\, and our experiments show that this synthe
 sis of BP and MCMC  outperforms both direct MCMC and bare BP schemes.
LOCATION:Area 5+6+7+8 #177
END:VEVENT
BEGIN:VEVENT
SUMMARY:Value Iteration Networks | Aviv Tamar \, Sergey Levine \, Pieter A
 bbeel \, YI WU \, Garrett Thomas
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Value Iteration Networks\nAviv Tamar \, Sergey Levine \
 , Pieter Abbeel \, YI WU \, Garrett Thomas\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7211\n\nWe introduce the value iteration network (VIN
 ): a fully differentiable neural network with a `planning module' embedded
  within. VINs can learn to plan\, and are suitable for predicting outcomes
  that involve planning-based reasoning\, such as policies for reinforcemen
 t learning. Key to our approach is a novel differentiable approximation of
  the value-iteration algorithm\, which can be represented as a convolution
 al neural network\, and trained end-to-end using standard backpropagation.
  We evaluate VIN based policies on discrete and continuous path-planning d
 omains\, and on a natural-language based search task. We show that by lear
 ning an explicit planning computation\, VIN policies generalize better to 
 new\, unseen domains.
LOCATION:Area 5+6+7+8 #178
END:VEVENT
BEGIN:VEVENT
SUMMARY:Sequential Neural Models with Stochastic Layers | Marco Fraccaro \
 , Søren Kaae Sønderby \, Ulrich Paquet \, Ole Winther
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Sequential Neural Models with Stochastic Layers\nMarco 
 Fraccaro \, Søren Kaae Sønderby \, Ulrich Paquet \, Ole Winther\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=7212\n\nHow can we efficiently
  propagate uncertainty in a latent state representation with recurrent neu
 ral networks?  This paper introduces stochastic recurrent neural networks 
 which glue a deterministic recurrent neural network and a state space mode
 l together to form a stochastic and sequential neural generative model. Th
 e clear separation of deterministic and stochastic layers allows a structu
 red variational inference network to track the factorization of the model
 ’s posterior distribution. By retaining both the nonlinear recursive str
 ucture of a recurrent neural network and averaging over the uncertainty in
  a latent path\, like a state space model\, we improve the state of the ar
 t results on the Blizzard and TIMIT speech modeling data sets by a large m
 argin\, while achieving comparable performances to competing methods on po
 lyphonic music modeling.
LOCATION:Area 5+6+7+8 #179
END:VEVENT
BEGIN:VEVENT
SUMMARY:Graphons\, mergeons\, and so on! | Justin Eldridge \, Mikhail Belk
 in \, Yusu Wang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Graphons\, mergeons\, and so on!\nJustin Eldridge \, Mi
 khail Belkin \, Yusu Wang\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=7213\n\nIn this work we develop a theory of hierarchical clustering fo
 r graphs. Our modelling assumption is that graphs are sampled from a graph
 on\, which is a powerful and general model for generating graphs and analy
 zing large networks.  Graphons are a  far richer class of graph models tha
 n stochastic blockmodels\, the primary setting for recent progress in the 
 statistical theory of graph clustering. We define what it means for an alg
 orithm to produce the ``correct" clustering\, give sufficient conditions i
 n which a method is statistically consistent\, and provide an explicit alg
 orithm satisfying these properties.
LOCATION:Area 5+6+7+8 #180
END:VEVENT
BEGIN:VEVENT
SUMMARY:Hierarchical Clustering via Spreading Metrics | Aurko Roy \, Sebas
 tian Pokutta
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Hierarchical Clustering via Spreading Metrics\nAurko Ro
 y \, Sebastian Pokutta\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7214\n\nWe study the cost function for  hierarchical clusterings introduc
 ed by [Dasgupta\, 2015]  where hierarchies are treated as first-class obje
 cts rather than deriving their cost from projections into flat clusters. I
 t was also shown in [Dasgupta\, 2015] that a top-down algorithm  returns a
  hierarchical clustering of cost at most (O\\left(\\alphan \\log n\\right)
 ) times the cost of the optimal hierarchical clustering\, where (\\alphan)
  is the approximation ratio of the Sparsest Cut subroutine used. Thus usin
 g the best known approximation algorithm for Sparsest Cut due to Arora-Rao
 -Vazirani\,  the top down algorithm returns a hierarchical clustering of c
 ost at most  (O\\left(\\log^{3/2} n\\right)) times the cost of the optimal
  solution. We improve this by giving an (O(\\log{n}))-approximation algori
 thm for this problem. Our main technical ingredients are a combinatorial c
 haracterization of ultrametrics induced by this cost function\, deriving a
 n Integer Linear Programming (ILP) formulation for this family of ultramet
 rics\, and showing how to iteratively round an LP relaxation of this formu
 lation by  using the idea of \\emph{sphere growing} which has been extensi
 vely used in the context of graph  partitioning. We also prove that our al
 gorithm returns an (O(\\log{n}))-approximate  hierarchical clustering for 
 a generalization of this cost function also studied in [Dasgupta\, 2015]. 
 Experiments show that the hierarchies found by using the ILP formulation a
 s well  as our rounding algorithm often have better projections into flat 
 clusters than the standard linkage based algorithms. We conclude with an i
 napproximability result for this problem\, namely that no polynomial sized
  LP or SDP can be used to obtain a constant factor approximation for this 
 problem.
LOCATION:Area 5+6+7+8 #181
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Learning for Predicting Human Strategic Behavior | Jason S Ha
 rtford \, James R Wright \, Kevin Leyton-Brown
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Deep Learning for Predicting Human Strategic Behavior\n
 Jason S Hartford \, James R Wright \, Kevin Leyton-Brown\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7215\n\nPredicting the behavior of huma
 n participants in strategic settings is an important problem in many domai
 ns. Most existing work either assumes that participants are perfectly rati
 onal\, or attempts to directly model each participant's cognitive processe
 s based on insights from cognitive psychology and experimental economics. 
 In this work\, we present an alternative\, a deep learning approach that a
 utomatically performs cognitive modeling without relying on such expert kn
 owledge.  We introduce a novel architecture that allows a single network t
 o generalize across different input and output dimensions by using matrix 
 units rather than scalar units\, and show that its performance significant
 ly outperforms that of the previous state of the art\, which relies on exp
 ert-constructed features.
LOCATION:Area 5+6+7+8 #182
END:VEVENT
BEGIN:VEVENT
SUMMARY:Global Analysis of Expectation Maximization for Mixtures of Two Ga
 ussians | Ji Xu \, Daniel Hsu \,
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Global Analysis of Expectation Maximization for Mixture
 s of Two Gaussians\nJi Xu \, Daniel Hsu \,\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7216\n\nExpectation Maximization (EM) is among the mo
 st popular algorithms for estimating parameters of statistical models.  Ho
 wever\, EM\, which is an iterative algorithm based on the maximum likeliho
 od principle\, is generally only guaranteed to find stationary points of t
 he likelihood objective\, and these points may be far from any maximizer. 
  This article addresses this disconnect between the statistical principles
  behind EM and its algorithmic properties.  Specifically\, it provides a g
 lobal analysis of EM for specific models in which the observations compris
 e an i.i.d. sample from a mixture of two Gaussians.  This is achieved by (
 i) studying the sequence of parameters from idealized execution of EM in t
 he infinite sample limit\, and fully characterizing the limit points of th
 e sequence in terms of the initial parameters\; and then (ii) based on thi
 s convergence analysis\, establishing statistical consistency (or lack the
 reof) for the actual sequence of parameters produced by EM.
LOCATION:Area 5+6+7+8 #183
END:VEVENT
BEGIN:VEVENT
SUMMARY:Supervised learning through the lens of compression | Ofir David \
 , Shay Moran \, Amir Yehudayoff
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Supervised learning through the lens of compression\nOf
 ir David \, Shay Moran \, Amir Yehudayoff\nhttp://nips.cc/Conferences/2016
 /Schedule?showEvent=7217\n\nThis work continues the study of the relations
 hip between sample compression schemes and statistical learning\, which ha
 s been mostly investigated within the framework of binary classification. 
 We first extend the investigation to multiclass categorization: we prove t
 hat in this case learnability is equivalent to compression of logarithmic 
 sample size and that the uniform convergence property implies compression 
 of constant size. We use the compressibility-learnability equivalence to s
 how that (i) for multiclass categorization\, PAC and agnostic PAC learnabi
 lity are equivalent\, and (ii) to derive a compactness theorem for learnab
 ility. We then consider supervised learning under general loss functions: 
 we show that in this case\, in order to maintain the compressibility-learn
 ability equivalence\, it is necessary to consider an approximate variant o
 f compression. We use it to show that PAC and agnostic PAC are not equival
 ent\, even when the loss function has only three values.
LOCATION:Area 5+6+7+8 #184
END:VEVENT
BEGIN:VEVENT
SUMMARY:Matrix Completion has No Spurious Local Minimum | Rong Ge \, Jason
  Lee \, Tengyu Ma
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Matrix Completion has No Spurious Local Minimum\nRong G
 e \, Jason Lee \, Tengyu Ma\nhttp://nips.cc/Conferences/2016/Schedule?show
 Event=7218\n\nMatrix completion is a basic machine learning problem that h
 as wide applications\, especially in collaborative filtering and recommend
 er systems. Simple non-convex optimization algorithms are popular and effe
 ctive in practice. Despite recent progress in proving various non-convex a
 lgorithms converge from a good initial point\, it remains unclear why rand
 om or arbitrary initialization suffices in practice. We prove that the com
 monly used non-convex objective function for matrix completion has no spur
 ious local minima --- all local minima must also be global. Therefore\, ma
 ny popular optimization algorithms such as (stochastic) gradient descent c
 an provably solve matrix completion with \\textit{arbitrary} initializatio
 n in polynomial time.
LOCATION:Area 5+6+7+8 #185
END:VEVENT
BEGIN:VEVENT
SUMMARY:Clustering with Same-Cluster Queries | Hassan Ashtiani \, Shrinu K
 ushagra \, Shai Ben-David
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Clustering with Same-Cluster Queries\nHassan Ashtiani \
 , Shrinu Kushagra \, Shai Ben-David\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7219\n\nWe propose a framework for Semi-Supervised Active Cl
 ustering framework (SSAC)\, where the learner is allowed to interact with 
 a domain expert\, asking whether two given instances belong to the same cl
 uster or not. We study the query and computational complexity of clusterin
 g in this framework. We consider a setting where the expert conforms to a 
 center-based clustering with a notion of margin.  We show that there is a 
 trade off between computational complexity and query complexity\; We prove
  that for the case of $k$-means clustering (i.e.\, when the expert conform
 s to a solution of $k$-means)\, having access to relatively few such queri
 es allows efficient solutions to otherwise NP hard problems.  In particula
 r\, we provide a probabilistic polynomial-time (BPP) algorithm  for cluste
 ring in this setting that asks $O\\big(k^2\\log k + k\\log n)$ same-cluste
 r queries and runs with time complexity $O\\big(kn\\log n)$ (where $k$ is 
 the number of clusters and $n$ is the number of instances). The success of
  the algorithm is guaranteed for data satisfying the margin condition unde
 r which\, without queries\, we show that the problem is NP hard. We also p
 rove a lower bound on the number of queries needed to have a computational
 ly efficient clustering algorithm in this setting.
LOCATION:Area 5+6+7+8 #186
END:VEVENT
BEGIN:VEVENT
SUMMARY:MetaGrad: Multiple Learning Rates in Online Learning | Tim van Erv
 en \, Wouter M Koolen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:MetaGrad: Multiple Learning Rates in Online Learning\nT
 im van Erven \, Wouter M Koolen\nhttp://nips.cc/Conferences/2016/Schedule?
 showEvent=7220\n\nIn online convex optimization it is well known that cert
 ain subclasses of objective functions are much easier than arbitrary conve
 x functions. We are interested in designing adaptive methods that can auto
 matically get fast rates in as many such subclasses as possible\, without 
 any manual tuning. Previous adaptive methods are able to interpolate betwe
 en strongly convex and general convex functions. We present a new method\,
  MetaGrad\, that adapts to a much broader class of functions\, including e
 xp-concave and strongly convex functions\, but also various types of stoch
 astic and non-stochastic functions without any curvature. For instance\, M
 etaGrad can achieve logarithmic regret on the unregularized hinge loss\, e
 ven though it has no curvature\, if the data come from a favourable probab
 ility distribution. MetaGrad's main feature is that it simultaneously cons
 iders multiple learning rates. Unlike all previous methods with provable r
 egret guarantees\, however\, its learning rates are not monotonically decr
 easing over time and are not tuned based on a theoretically derived bound 
 on the regret. Instead\, they are weighted directly proportional to their 
 empirical performance on the data using a tilted exponential weights maste
 r algorithm.
LOCATION:Area 5+6+7+8 #187
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Feature Extraction by Time-Contrastive Learning and N
 onlinear ICA | Aapo Hyvarinen \, Hiroshi Morioka
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Unsupervised Feature Extraction by Time-Contrastive Lea
 rning and Nonlinear ICA\nAapo Hyvarinen \, Hiroshi Morioka\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7221\n\nNonlinear independent compone
 nt analysis (ICA) provides an appealing framework for unsupervised feature
  learning\, but the models proposed so far are not identifiable. Here\, we
  first propose a new intuitive principle of unsupervised deep learning fro
 m time series which uses the nonstationary structure of the data. Our lear
 ning principle\, time-contrastive learning (TCL)\,  finds a representation
  which allows optimal discrimination of time segments (windows). Surprisin
 gly\, we show how TCL can be related to a nonlinear ICA model\, when ICA i
 s redefined to include temporal nonstationarities. In particular\, we show
  that TCL combined with linear ICA estimates the nonlinear ICA model up to
  point-wise transformations of the sources\, and this solution is unique -
 -- thus providing the first identifiability result for nonlinear ICA which
  is rigorous\, constructive\, as well as very general.
LOCATION:Area 5+6+7+8 #188
END:VEVENT
BEGIN:VEVENT
SUMMARY:Phased LSTM: Accelerating Recurrent Network Training for Long or E
 vent-based Sequences | Daniel Neil \, Michael Pfeiffer \, Shih-Chii Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Phased LSTM: Accelerating Recurrent Network Training fo
 r Long or Event-based Sequences\nDaniel Neil \, Michael Pfeiffer \, Shih-C
 hii Liu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7222\n\nRecurr
 ent Neural Networks (RNNs) have become the state-of-the-art choice for ext
 racting patterns from temporal sequences. Current RNN models are ill suite
 d to process irregularly sampled data triggered by events generated in con
 tinuous time by sensors or other neurons. Such data can occur\, for exampl
 e\, when the input comes from novel event-driven artificial sensors which 
 generate sparse\, asynchronous streams of events or from multiple conventi
 onal sensors with different update intervals. In this work\, we introduce 
 the Phased LSTM model\, which extends the LSTM unit by adding a new time g
 ate. This gate is controlled by a parametrized oscillation with a frequenc
 y range which require updates of the memory cell only during a small perce
 ntage of the cycle. Even with the sparse updates imposed by the oscillatio
 n\, the Phased LSTM network achieves faster convergence than regular LSTMs
  on tasks which require learning of long sequences.   The model naturally 
 integrates inputs from sensors of arbitrary sampling rates\, thereby openi
 ng new areas of investigation for processing asynchronous sensory events t
 hat carry timing information.  It also greatly improves the performance of
  LSTMs in standard RNN applications\, and does so with an order-of-magnitu
 de fewer computes.
LOCATION:Area 5+6+7+8 #189
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tractable Operations for Arithmetic Circuits of Probabilistic Mode
 ls | Yujia Shen \, Arthur Choi \, Adnan Darwiche
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Tractable Operations for Arithmetic Circuits of Probabi
 listic Models\nYujia Shen \, Arthur Choi \, Adnan Darwiche\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7223\n\nWe consider tractable represe
 ntations of probability distributions and the polytime operations they sup
 port.  In particular\, we consider a recently proposed arithmetic circuit 
 representation\, the Probabilistic Sentential Decision Diagram (PSDD).  We
  show that PSDD supports a polytime multiplication operator\, while they d
 o not support a polytime operator for summing-out variables.  A polytime m
 ultiplication operator make PSDDs suitable for a broader class of applicat
 ions compared to arithmetic circuits\, which do not in general support mul
 tiplication.  As one example\, we show that PSDD multiplication leads to a
  very simple but effective compilation algorithm for probabilistic graphic
 al models: represent each model factor as a PSDD\, and then multiply them.
LOCATION:Area 5+6+7+8 #190
END:VEVENT
BEGIN:VEVENT
SUMMARY:Using Fast Weights to Attend to the Recent Past | Jimmy Ba \, Geof
 frey E Hinton \, Volodymyr Mnih \, Joel Z Leibo \, Catalin Ionescu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Using Fast Weights to Attend to the Recent Past\nJimmy 
 Ba \, Geoffrey E Hinton \, Volodymyr Mnih \, Joel Z Leibo \, Catalin Iones
 cu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7224\n\nUntil recen
 tly\, research on artificial neural networks was largely restricted to sys
 tems with only two types of variable: Neural activities that represent the
  current or recent input and weights that learn to capture regularities am
 ong inputs\, outputs and payoffs. There is no good reason for this restric
 tion. Synapses have dynamics at many different time-scales and this sugges
 ts that artificial neural networks might benefit from variables that chang
 e slower than activities but much faster than the standard weights.  These
  ``fast weights'' can be used to store temporary memories of the recent pa
 st and they provide a neurally plausible way of implementing the type of a
 ttention to the past that has recently proven helpful in sequence-to-seque
 nce models. By using fast weights we can avoid the need to store copies of
  neural activity patterns.
LOCATION:Area 5+6+7+8 #191
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian Intermittent Demand Forecasting for Large Inventories | M
 atthias W Seeger \, David Salinas \, Valentin Flunkert
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Bayesian Intermittent Demand Forecasting for Large Inve
 ntories\nMatthias W Seeger \, David Salinas \, Valentin Flunkert\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7225\n\nWe present a scalable a
 nd robust Bayesian method for demand forecasting in the context of a large
  e-commerce platform\, paying special attention to intermittent and bursty
  target statistics. Inference is approximated by the Newton-Raphson algori
 thm\, reduced to linear-time Kalman smoothing\, which allows us to operate
  on several orders of magnitude larger problems than previous related work
 . In a study on large real-world sales datasets\, our method outperforms c
 ompeting approaches on fast and medium moving items.
LOCATION:Area 5+6+7+8 #192
END:VEVENT
BEGIN:VEVENT
SUMMARY:Blazing the trails before beating the path: Sample-efficient Monte
 -Carlo planning | Jean-Bastien Grill \, Michal Valko \, Remi Munos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:Blazing the trails before beating the path: Sample-effi
 cient Monte-Carlo planning\nJean-Bastien Grill \, Michal Valko \, Remi Mun
 os\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7226\n\nWe study th
 e sampling-based planning problem in Markov decision processes (MDPs) that
  we can access only through a generative model\, usually referred to as Mo
 nte-Carlo planning. Our objective is to return a good estimate of the opti
 mal value function at any state while minimizing the number of calls to th
 e generative model\, i.e. the sample complexity. We propose a new algorith
 m\, TrailBlazer\, able to handle MDPs with a finite or an infinite number 
 of transitions from state-action to next states. TrailBlazer is an adaptiv
 e algorithm that exploits possible structures of the MDP by exploring only
  a subset of states reachable by following near-optimal policies. We provi
 de bounds on its sample complexity that depend on a measure of the quantit
 y of near-optimal states. The algorithm behavior can be considered as an e
 xtension of Monte-Carlo sampling (for estimating an expectation) to proble
 ms that alternate maximization (over actions) and expectation (over next s
 tates). Finally\, another appealing feature of TrailBlazer is that it is s
 imple to implement and computationally efficient.
LOCATION:Area 5+6+7+8 #193
END:VEVENT
BEGIN:VEVENT
SUMMARY:SDP Relaxation with Randomized Rounding for Energy Disaggregation 
 | Kiarash Shaloudegi \, András György \, Csaba Szepesvari \, Wilsun Xu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161206T213000
DESCRIPTION:Poster:SDP Relaxation with Randomized Rounding for Energy Disa
 ggregation\nKiarash Shaloudegi \, András György \, Csaba Szepesvari \, W
 ilsun Xu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7227\n\nWe de
 velop a scalable\, computationally efficient method for the task of energy
  disaggregation for home appliance monitoring. In this problem the goal is
  to estimate the energy consumption of each appliance based on the total e
 nergy-consumption signal of a household. The current state of the art mode
 ls the problem as inference in factorial HMMs\, and finds an approximate s
 olution to the resulting quadratic integer program via quadratic programmi
 ng. Here we take a more principled approach\, better suited to integer pro
 gramming problems\, and find an approximate optimum by combining convex se
 midefinite relaxations with randomized rounding\, as well as with a scalab
 le ADMM method that exploits the special structure of the resulting semide
 finite program. Simulation results demonstrate the superiority of our meth
 ods both in synthetic and real-world datasets.
LOCATION:Area 5+6+7+8 #194
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast Mixing Markov Chains for Strongly Rayleigh Measures\, DPPs\, 
 and Constrained Sampling | Chengtao Li \, Suvrit Sra \, Stefanie Jegelka
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Fast Mixing Markov Chains for Strongly Rayleigh Measure
 s\, DPPs\, and Constrained Sampling\nChengtao Li \, Suvrit Sra \, Stefanie
  Jegelka\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7228\n\nWe st
 udy probability measures induced by set functions with constraints. Such m
 easures arise in a variety of real-world settings\, where prior knowledge\
 , resource limitations\, or other pragmatic considerations impose constrai
 nts. We consider the task of rapidly sampling from such constrained measur
 es\, and develop fast Markov chain samplers for them. Our first main resul
 t is for MCMC sampling from Strongly Rayleigh (SR) measures\, for which we
  present sharp polynomial bounds on the mixing time. As a corollary\, this
  result yields a fast mixing sampler for Determinantal Point Processes (DP
 Ps)\, yielding (to our knowledge) the first provably fast MCMC sampler for
  DPPs since their inception over four decades ago. Beyond SR measures\, we
  develop MCMC samplers for probabilistic models with hard constraints and 
 identify sufficient conditions under which their chains mix rapidly. We il
 lustrate our claims by empirically verifying the dependence of mixing time
 s on the key factors governing our theoretical bounds.
LOCATION:Area 5+6+7+8 #1
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Learning of 3D Structure from Images | Danilo Jimenez
  Rezende \, S. M. Ali Eslami \, Shakir Mohamed \, Peter Battaglia \, Max J
 aderberg \, Nicolas Heess
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Unsupervised Learning of 3D Structure from Images\nDani
 lo Jimenez Rezende \, S. M. Ali Eslami \, Shakir Mohamed \, Peter Battagli
 a \, Max Jaderberg \, Nicolas Heess\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7229\n\nA key goal of computer vision is to recover the unde
 rlying 3D structure that gives rise to 2D observations of the world. If en
 dowed with 3D understanding\, agents can abstract away from the complexity
  of the rendering process to form stable\, disentangled representations of
  scene elements. In this paper we learn strong deep generative models of 3
 D structures\, and recover these structures from 2D images via probabilist
 ic inference. We demonstrate high-quality samples and report log-likelihoo
 ds on several datasets\, including ShapeNet\, and establish the first benc
 hmarks in the literature. We also show how these models and their inferenc
 e networks can be trained jointly\, end-to-end\, and directly from 2D imag
 es without any use of ground-truth 3D labels. This demonstrates for the fi
 rst time the feasibility of learning to infer 3D representations of the wo
 rld in a purely unsupervised manner.
LOCATION:Area 5+6+7+8 #2
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Multiple Quantile Graphical Model | Alnur Ali \, J. Zico Kolte
 r \, Ryan J Tibshirani
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:The Multiple Quantile Graphical Model\nAlnur Ali \, J. 
 Zico Kolter \, Ryan J Tibshirani\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7230\n\nWe introduce the Multiple Quantile Graphical Model (MQG
 M)\, which extends the neighborhood selection approach of Meinshausen and 
 Buhlmann for learning sparse graphical models.  The latter is defined by t
 he basic subproblem of modeling the conditional mean of one variable as a 
 sparse function of all others.  Our approach models a set of conditional q
 uantiles of one variable as a sparse function of all others\, and hence of
 fers a much richer\, more expressive class of conditional distribution est
 imates.  We establish that\, under suitable regularity conditions\, the MQ
 GM identifies the exact conditional independencies with probability tendin
 g to one as the problem size grows\, even outside of the usual homoskedast
 ic Gaussian data model. We develop an efficient algorithm for fitting the 
 MQGM using the alternating direction method of multipliers.  We also descr
 ibe a strategy for sampling from the joint distribution that underlies the
  MQGM estimate. Lastly\, we present detailed experiments that demonstrate 
 the flexibility and effectiveness of the MQGM in modeling hetereoskedastic
  non-Gaussian data.
LOCATION:Area 5+6+7+8 #3
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Learning from Noisy Networks with Applications to Hi-
 C Data | Bo Wang \, Junjie Zhu \, Armin Pourshafeie \, Oana Ursu \, Serafi
 m Batzoglou \, Anshul Kundaje
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Unsupervised Learning from Noisy Networks with Applicat
 ions to Hi-C Data\nBo Wang \, Junjie Zhu \, Armin Pourshafeie \, Oana Ursu
  \, Serafim Batzoglou \, Anshul Kundaje\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7231\n\nComplex networks play an important role in a ple
 thora of disciplines in natural sciences. Cleaning up noisy observed netwo
 rks\, poses an important challenge in network analysis Existing methods ut
 ilize labeled data to alleviate the noise effect in the network.  However\
 , labeled data is usually expensive to collect while unlabeled data can be
  gathered cheaply. In this paper\, we propose an optimization framework to
  mine useful structures from noisy networks in an unsupervised manner. The
  key feature of our optimization framework is its ability to utilize local
  structures as well as global patterns in the network. We extend our metho
 d to incorporate multi-resolution networks in order to add further resista
 nce to high-levels of noise. We also generalize our framework to utilize p
 artial labels to enhance the performance. We specifically focus our method
  on multi-resolution Hi-C data by recovering clusters of genomic regions t
 hat co-localize in 3D space. Additionally\, we use Capture-C-generated par
 tial labels to further denoise the Hi-C network. We empirically demonstrat
 e the effectiveness of our framework in denoising the network and improvin
 g community detection results.
LOCATION:Area 5+6+7+8 #4
END:VEVENT
BEGIN:VEVENT
SUMMARY:Towards Unifying Hamiltonian Monte Carlo and Slice Sampling | Yizh
 e Zhang \, Xiangyu Wang \, Changyou Chen \, Ricardo Henao \, Kai Fan \, La
 wrence Carin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Towards Unifying Hamiltonian Monte Carlo and Slice Samp
 ling\nYizhe Zhang \, Xiangyu Wang \, Changyou Chen \, Ricardo Henao \, Kai
  Fan \, Lawrence Carin\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7232\n\nWe unify slice sampling and Hamiltonian Monte Carlo (HMC) samplin
 g\, demonstrating their connection via the Hamiltonian-Jacobi equation fro
 m Hamiltonian mechanics. This insight enables extension of HMC and slice s
 ampling to a broader family of samplers\, called Monomial Gamma Samplers (
 MGS). We provide a theoretical analysis of the mixing performance of such 
 samplers\, proving that in the limit of a single parameter\, the MGS draws
  decorrelated samples from the desired target distribution. We further sho
 w that as this parameter tends toward this limit\, performance gains are a
 chieved at a cost of increasing numerical difficulty and some practical co
 nvergence issues. Our theoretical results are validated with synthetic dat
 a and real-world applications.
LOCATION:Area 5+6+7+8 #5
END:VEVENT
BEGIN:VEVENT
SUMMARY:Differential Privacy without Sensitivity | Kentaro Minami \, HItom
 i Arai \, Issei Sato \, Hiroshi Nakagawa
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Differential Privacy without Sensitivity\nKentaro Minam
 i \, HItomi Arai \, Issei Sato \, Hiroshi Nakagawa\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7233\n\nThe exponential mechanism is a genera
 l method to construct a randomized estimator that satisfies $(\\varepsilon
 \, 0)$-differential privacy. Recently\, Wang et al. showed that the Gibbs 
 posterior\, which is a data-dependent probability distribution that contai
 ns the Bayesian posterior\, is essentially equivalent to the exponential m
 echanism under certain boundedness conditions on the loss function. While 
 the exponential mechanism provides a way to build an $(\\varepsilon\, 0)$-
 differential private algorithm\, it requires boundedness of the loss funct
 ion\, which is quite stringent for some learning problems. In this paper\,
  we focus on $(\\varepsilon\, \\delta)$-differential privacy of Gibbs post
 eriors with convex and Lipschitz loss functions. Our result extends the cl
 assical exponential mechanism\, allowing the loss functions to have an unb
 ounded sensitivity.
LOCATION:Area 5+6+7+8 #6
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generalized Correspondence-LDA Models (GC-LDA) for Identifying Fun
 ctional Regions in the Brain | Timothy Rubin \, Oluwasanmi Koyejo \, Micha
 el Jones \, Tal Yarkoni
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Generalized Correspondence-LDA Models (GC-LDA) for Iden
 tifying Functional Regions in the Brain\nTimothy Rubin \, Oluwasanmi Koyej
 o \, Michael Jones \, Tal Yarkoni\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7234\n\nThis paper presents Generalized Correspondence-LDA (GC
 -LDA)\, a generalization of the Correspondence-LDA model that allows for v
 ariable spatial representations to be associated with topics\, and increas
 ed flexibility in terms of the strength of the correspondence between data
  types induced by the model. We present three variants of GC-LDA\, each of
  which associates topics with a different spatial representation\, and app
 ly them to a corpus of neuroimaging data. In the context of this dataset\,
  each topic corresponds to a functional brain region\, where the region's 
 spatial extent is captured by a probability distribution over neural activ
 ity\, and the region's cognitive function is captured by a probability dis
 tribution over linguistic terms. We illustrate the qualitative improvement
 s offered by GC-LDA in terms of the types of topics extracted with alterna
 tive spatial representations\, as well as the model's ability to incorpora
 te a-priori knowledge from the neuroimaging literature. We furthermore dem
 onstrate that the novel features of GC-LDA improve predictions for missing
  data.
LOCATION:Area 5+6+7+8 #7
END:VEVENT
BEGIN:VEVENT
SUMMARY:Kronecker Determinantal Point Processes | Zelda E. Mariet \, Suvri
 t Sra
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Kronecker Determinantal Point Processes\nZelda E. Marie
 t \, Suvrit Sra\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7235\n
 \nDeterminantal Point Processes (DPPs) are probabilistic models over all s
 ubsets a ground set of N items. They have recently gained prominence in se
 veral applications that rely on diverse subsets. However\, their applicabi
 lity to large problems is still limited due to O(N^3) complexity of core t
 asks such as sampling and learning. We enable efficient sampling and learn
 ing for DPPs by introducing KronDPP\, a DPP model whose kernel matrix deco
 mposes as a tensor product of multiple smaller kernel matrices. This decom
 position immediately enables fast exact sampling. But contrary to what one
  may expect\, leveraging the Kronecker product structure for speeding up D
 PP learning turns out to be more difficult. We overcome this challenge\, a
 nd derive batch and stochastic optimization algorithms for efficiently lea
 rning the parameters of a KronDPP.
LOCATION:Area 5+6+7+8 #8
END:VEVENT
BEGIN:VEVENT
SUMMARY:Variance Reduction in Stochastic Gradient Langevin Dynamics | Kuma
 r Avinava Dubey \, Sashank J. Reddi \, Sinead A Williamson \, Barnabas Poc
 zos \, Alexander J Smola \, Eric P Xing
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Variance Reduction in Stochastic Gradient Langevin Dyna
 mics\nKumar Avinava Dubey \, Sashank J. Reddi \, Sinead A Williamson \, Ba
 rnabas Poczos \, Alexander J Smola \, Eric P Xing\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7236\n\nStochastic gradient-based Monte Carlo 
 methods such as stochastic gradient Langevin dynamics are useful tools for
  posterior inference on large scale datasets in many machine learning appl
 ications. These methods scale to large datasets by using noisy gradients c
 alculated using a mini-batch or subset of the dataset. However\, the high 
 variance inherent in these noisy gradients degrades performance and leads 
 to slower mixing. In this paper\, we present techniques for reducing varia
 nce in stochastic gradient Langevin dynamics\, yielding novel  stochastic 
 Monte Carlo methods that improve performance by reducing the variance in t
 he stochastic gradient. We show that our proposed method has better theore
 tical guarantees on convergence rate than stochastic Langevin dynamics. Th
 is is complemented by impressive empirical results obtained on  a variety 
 of real world datasets\, and on four different machine learning tasks (reg
 ression\, classification\, independent component analysis and mixture mode
 ling). These theoretical and empirical contributions combine to make a com
 pelling case for using variance reduction in stochastic Monte Carlo method
 s.
LOCATION:Area 5+6+7+8 #9
END:VEVENT
BEGIN:VEVENT
SUMMARY:Online Pricing with Strategic and Patient Buyers | Michal Feldman 
 \, Tomer Koren \, Roi Livni \, Yishay Mansour \, Aviv Zohar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Online Pricing with Strategic and Patient Buyers\nMicha
 l Feldman \, Tomer Koren \, Roi Livni \, Yishay Mansour \, Aviv Zohar\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7237\n\nWe consider a sell
 er with an unlimited supply of a single good\, who is faced with a stream 
 of $T$ buyers. Each buyer has a window of time in which she would like to 
 purchase\, and would buy at the lowest price in that window\, provided tha
 t this price is lower than her private value (and otherwise\, would not bu
 y at all). In this setting\, we give an algorithm that attains $O(T^{2/3})
 $ regret over any sequence of $T$ buyers with respect to the best fixed pr
 ice in hindsight\, and prove that no algorithm can perform better in the w
 orst case.
LOCATION:Area 5+6+7+8 #10
END:VEVENT
BEGIN:VEVENT
SUMMARY:Exploiting the Structure: Stochastic Gradient Methods Using Raw Cl
 usters | Zeyuan Allen-Zhu \, Yang Yuan \, Karthik Sridharan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Exploiting the Structure: Stochastic Gradient Methods U
 sing Raw Clusters\nZeyuan Allen-Zhu \, Yang Yuan \, Karthik Sridharan\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7238\n\nThe amount of data
  available in the world is growing faster than our ability to deal with it
 . However\, if we take advantage of the internal structure\, data may beco
 me much smaller for machine learning purposes. In this paper we focus on o
 ne of the fundamental machine learning tasks\, empirical risk minimization
  (ERM)\, and provide faster algorithms with the help from the clustering s
 tructure of the data.  We introduce a simple notion of raw clustering that
  can be efficiently computed from the data\, and propose two algorithms ba
 sed on clustering information. Our accelerated algorithm ClusterACDM is bu
 ilt on a novel Haar transformation applied to the dual space of the ERM pr
 oblem\, and our variance-reduction based algorithm ClusterSVRG introduces 
 a new gradient estimator using clustering. Our algorithms outperform their
  classical counterparts ACDM and SVRG respectively.
LOCATION:Area 5+6+7+8 #11
END:VEVENT
BEGIN:VEVENT
SUMMARY:Clustering Signed Networks with the Geometric Mean of Laplacians |
  Pedro Mercado \, Francesco Tudisco \, Matthias Hein
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Clustering Signed Networks with the Geometric Mean of L
 aplacians\nPedro Mercado \, Francesco Tudisco \, Matthias Hein\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7239\n\nSigned networks allow to 
 model positive and negative relationships. We analyze existing extensions 
 of spectral clustering to signed networks. It turns out that existing appr
 oaches do not recover the ground truth clustering in several situations wh
 ere either the positive or the negative network structures contain no nois
 e. Our analysis shows that these problems arise as existing approaches tak
 e some form of arithmetic mean of the Laplacians of the positive and negat
 ive part. As a solution we propose to use the geometric mean of the Laplac
 ians of positive and negative part and show that it outperforms the existi
 ng approaches. While the geometric mean of matrices is computationally exp
 ensive\, we show that eigenvectors of the geometric mean can be computed e
 fficiently\, leading to a numerical scheme for sparse matrices which is of
  independent interest.
LOCATION:Area 5+6+7+8 #12
END:VEVENT
BEGIN:VEVENT
SUMMARY:Robust Spectral Detection of Global Structures in the Data by Lear
 ning a Regularization | Pan Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Robust Spectral Detection of Global Structures in the D
 ata by Learning a Regularization\nPan Zhang\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7240\n\nSpectral methods are popular in detecting gl
 obal structures in the given data that can be represented as a matrix. How
 ever when the data matrix is sparse or noisy\, classic spectral methods us
 ually fail to work\, due to localization of eigenvectors (or singular vect
 ors) induced by the sparsity or noise. In this work\, we propose a general
  method to solve the localization problem by learning a regularization mat
 rix from the localized eigenvectors. Using matrix perturbation analysis\, 
 we demonstrate that the learned  regularizations suppress down the eigenva
 lues associated with localized  eigenvectors and enable us to recover the 
 informative eigenvectors representing the global structure. We show applic
 ations of our method in several inference problems: community detection in
  networks\, clustering from pairwise similarities\, rank estimation and ma
 trix completion problems. Using extensive experiments\, we illustrate that
  our method solves the localization problem and works down to the  theoret
 ical detectability limits in different kinds of synthetic data. This is in
  contrast with existing spectral algorithms based on data matrix\, non-bac
 ktracking matrix\, Laplacians and those with rank-one regularizations\, wh
 ich perform poorly in the sparse case with noise.
LOCATION:Area 5+6+7+8 #13
END:VEVENT
BEGIN:VEVENT
SUMMARY:Perspective Transformer Nets: Learning Single-View 3D Object Recon
 struction without 3D Supervision | Xinchen Yan \, Jimei Yang \, Ersin Yume
 r \, Yijie Guo \, Honglak Lee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Perspective Transformer Nets: Learning Single-View 3D O
 bject Reconstruction without 3D Supervision\nXinchen Yan \, Jimei Yang \, 
 Ersin Yumer \, Yijie Guo \, Honglak Lee\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7241\n\nUnderstanding the 3D world is a fundamental prob
 lem in computer vision. However\, learning a good representation of 3D obj
 ects is still an open problem due to the high dimensionality of the data a
 nd many factors of variation involved. In this work\, we investigate the t
 ask of single-view 3D object reconstruction from a learning agent's perspe
 ctive. We formulate the learning process as an interaction between 3D and 
 2D representations and propose an encoder-decoder network with a novel pro
 jection loss defined by the projective transformation. More importantly\, 
 the projection loss enables the unsupervised learning using 2D observation
  without explicit 3D supervision. We demonstrate the ability of the model 
 in generating 3D volume from a single 2D image with three sets of experime
 nts: (1) learning from single-class objects\; (2) learning from multi-clas
 s objects and (3) testing on novel object classes. Results show superior p
 erformance and better generalization ability for 3D object reconstruction 
 when the projection loss is involved.
LOCATION:Area 5+6+7+8 #14
END:VEVENT
BEGIN:VEVENT
SUMMARY:Launch and Iterate: Reducing Prediction Churn | Mahdi Milani Fard 
 \, Quentin Cormier \, Kevin Canini \, Maya Gupta
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Launch and Iterate: Reducing Prediction Churn\nMahdi Mi
 lani Fard \, Quentin Cormier \, Kevin Canini \, Maya Gupta\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7242\n\nPractical applications of mac
 hine learning often involve successive training iterations with changes to
  features and training examples. Ideally\, changes in the output of any ne
 w model should only be improvements (wins) over the previous iteration\, b
 ut in practice the predictions may change neutrally for many examples\, re
 sulting in extra net-zero wins and losses\, referred to as unnecessary chu
 rn. These changes in the predictions are problematic for usability for som
 e applications\, and make it harder and more expensive to measure if a cha
 nge is statistically significant positive. In this paper\, we formulate th
 e problem and present a stabilization operator to regularize a classifier 
 towards a previous classifier. We use a Markov chain Monte Carlo stabiliza
 tion operator to produce a model with more consistent predictions without 
 adversely affecting accuracy. We investigate the properties of the proposa
 l with theoretical analysis. Experiments on benchmark datasets for differe
 nt classification algorithms demonstrate the method and the resulting redu
 ction in churn.
LOCATION:Area 5+6+7+8 #15
END:VEVENT
BEGIN:VEVENT
SUMMARY:Data Poisoning Attacks on Factorization-Based Collaborative Filter
 ing | Bo Li \, Yining Wang \, Aarti Singh \, Yevgeniy Vorobeychik
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Data Poisoning Attacks on Factorization-Based Collabora
 tive Filtering\nBo Li \, Yining Wang \, Aarti Singh \, Yevgeniy Vorobeychi
 k\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7244\n\nRecommendati
 on and collaborative filtering systems are important in modern information
  and e-commerce applications.  As these systems are becoming increasingly 
 popular in industry\, their outputs could affect business decision making\
 , introducing incentives for an adversarial party to compromise the availa
 bility or integrity of such systems. We introduce a data poisoning attack 
 on collaborative filtering systems.  We demonstrate how a powerful attacke
 r with full knowledge of the learner can generate malicious data so as to 
 maximize his/her malicious objectives\, while at the same time mimicking n
 ormal user behaviors to avoid being detected. While the complete knowledge
  assumption seems extreme\, it enables a robust assessment of the vulnerab
 ility of collaborative filtering schemes to highly motivated attacks. We p
 resent efficient solutions for two popular factorization-based collaborati
 ve filtering algorithms: the alternative minimization formulation and the 
 nuclear norm minimization method. Finally\, we test the effectiveness of o
 ur proposed algorithms on real-world data and discuss potential defensive 
 strategies.
LOCATION:Area 5+6+7+8 #16
END:VEVENT
BEGIN:VEVENT
SUMMARY:Scaling Memory-Augmented Neural Networks with Sparse Reads and Wri
 tes | Jack Rae \, Jonathan J Hunt \, Ivo Danihelka \, Timothy Harley \, An
 drew W Senior \, Gregory Wayne \, Alex Graves \, Tim Lillicrap
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Scaling Memory-Augmented Neural Networks with Sparse Re
 ads and Writes\nJack Rae \, Jonathan J Hunt \, Ivo Danihelka \, Timothy Ha
 rley \, Andrew W Senior \, Gregory Wayne \, Alex Graves \, Tim Lillicrap\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7245\n\nNeural networks
  augmented with external memory have the ability to learn algorithmic solu
 tions to complex tasks. These models appear promising for applications suc
 h as language modeling and machine translation. However\, they scale poorl
 y in both space and time as the amount of memory grows --- limiting their 
 applicability to real-world domains. Here\, we present an end-to-end diffe
 rentiable memory access scheme\, which we call Sparse Access Memory (SAM)\
 , that retains the representational power of the original approaches whils
 t training efficiently with very large memories. We show that SAM achieves
  asymptotic lower bounds in space and time complexity\, and find that an i
 mplementation runs $1\,!000\\times$ faster and with $3\,!000\\times$ less 
 physical memory than non-sparse models. SAM learns with comparable data ef
 ficiency to existing models on a range of synthetic tasks and one-shot Omn
 iglot character recognition\, and can scale to tasks requiring $100\,!000$
 s of time steps and memories. As well\, we show how our approach can be ad
 apted for models that maintain temporal associations between memories\, as
  with the recently introduced Differentiable Neural Computer.
LOCATION:Area 5+6+7+8 #17
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Architectures in a Solvable Model of Deep Networks | Jonat
 han Kadmon \, Haim Sompolinsky
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Optimal Architectures in a Solvable Model of Deep Netwo
 rks\nJonathan Kadmon \, Haim Sompolinsky\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7246\n\nDeep neural networks have received a considerab
 le attention due to the success of their training for real world machine l
 earning applications. They are also of great interest to the understanding
  of sensory processing in cortical sensory hierarchies. The purpose of thi
 s work is to advance our theoretical understanding of the computational be
 nefits of these architectures. Using a simple model of clustered noisy inp
 uts and a simple learning rule\, we provide analytically derived recursion
  relations describing the propagation of the signals along the deep networ
 k. By analysis of these equations\, and defining performance measures\, we
  show that these model networks have optimal depths. We further explore th
 e dependence of the optimal architecture on the system parameters.
LOCATION:Area 5+6+7+8 #18
END:VEVENT
BEGIN:VEVENT
SUMMARY:Scalable Adaptive Stochastic Optimization Using Random Projections
  | Gabriel Krummenacher \, Brian McWilliams \, Yannic Kilcher \, Joachim M
  Buhmann \, Nicolai Meinshausen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Scalable Adaptive Stochastic Optimization Using Random 
 Projections\nGabriel Krummenacher \, Brian McWilliams \, Yannic Kilcher \,
  Joachim M Buhmann \, Nicolai Meinshausen\nhttp://nips.cc/Conferences/2016
 /Schedule?showEvent=7247\n\nAdaptive stochastic gradient methods such as A
 daGrad have gained popularity in particular for training deep neural netwo
 rks. The most commonly used and studied variant maintains a diagonal matri
 x approximation to second order information by accumulating past gradients
  which are used to tune the step size adaptively. In certain situations th
 e full-matrix variant of AdaGrad is expected to attain better performance\
 , however in high dimensions it is computationally impractical. We present
  Ada-LR and RadaGrad two computationally efficient approximations to full-
 matrix AdaGrad based on randomized dimensionality reduction. They are able
  to capture dependencies between features and achieve similar performance 
 to full-matrix AdaGrad but at a much smaller computational cost. We show t
 hat the regret of Ada-LR is close to the regret of full-matrix AdaGrad whi
 ch can have an up-to exponentially smaller dependence on the dimension tha
 n the diagonal variant. Empirically\, we show that Ada-LR and RadaGrad per
 form similarly to full-matrix AdaGrad. On the task of training convolution
 al neural networks as well as recurrent neural networks\, RadaGrad achieve
 s faster convergence than diagonal AdaGrad.
LOCATION:Area 5+6+7+8 #19
END:VEVENT
BEGIN:VEVENT
SUMMARY:Spectral Learning of Dynamic Systems from Nonequilibrium Data | Ha
 o Wu \, Frank Noe
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Spectral Learning of Dynamic Systems from Nonequilibriu
 m Data\nHao Wu \, Frank Noe\nhttp://nips.cc/Conferences/2016/Schedule?show
 Event=7248\n\nObservable operator models (OOMs) and related models are one
  of the most important and powerful tools for modeling and analyzing stoch
 astic systems. They exactly describe dynamics of finite-rank systems and c
 an be efficiently and consistently estimated through spectral learning und
 er the assumption of identically distributed data. In this paper\, we inve
 stigate the properties of spectral learning without this assumption due to
  the requirements of analyzing large-time scale systems\, and show that th
 e equilibrium dynamics of a system can be extracted from nonequilibrium ob
 servation data by imposing an equilibrium constraint. In addition\, we pro
 pose a binless extension of spectral learning for continuous data. In comp
 arison with the other continuous-valued spectral algorithms\, the binless 
 algorithm can achieve consistent estimation of equilibrium dynamics with o
 nly linear complexity.
LOCATION:Area 5+6+7+8 #20
END:VEVENT
BEGIN:VEVENT
SUMMARY:Local Minimax Complexity of Stochastic Convex Optimization | sabya
 sachi chatterjee \, John C Duchi \, John Lafferty \, Yuancheng Zhu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Local Minimax Complexity of Stochastic Convex Optimizat
 ion\nsabyasachi chatterjee \, John C Duchi \, John Lafferty \, Yuancheng Z
 hu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7249\n\nWe extend t
 he traditional worst-case\, minimax analysis of stochastic convex optimiza
 tion by introducing a localized form of minimax complexity for individual 
 functions.  Our main result gives function-specific lower and upper bounds
  on the number of stochastic subgradient evaluations needed to optimize ei
 ther the function or its ``hardest local alternative'' to a given numerica
 l precision.  The bounds are expressed in terms of a localized and computa
 tional analogue of the modulus of continuity that is central to statistica
 l minimax analysis. We show how the computational modulus of continuity ca
 n be explicitly calculated in concrete cases\, and relates to the curvatur
 e of the function at the optimum.  We also prove a superefficiency result 
 that demonstrates it is a meaningful benchmark\, acting as a computational
  analogue of the Fisher information in statistical estimation. The nature 
 and practical implications of the results are demonstrated in simulations.
LOCATION:Area 5+6+7+8 #21
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Theoretically Grounded Application of Dropout in Recurrent Neura
 l Networks | Yarin Gal \, Zoubin Ghahramani
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Theoretically Grounded Application of Dropout in Recu
 rrent Neural Networks\nYarin Gal \, Zoubin Ghahramani\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=7250\n\nRecurrent neural networks (RNNs) s
 tand at the forefront of many recent developments in deep learning. Yet a 
 major difficulty with these models is their tendency to overfit\, with dro
 pout shown to fail when applied to recurrent layers. Recent results at the
  intersection of Bayesian modelling and deep learning offer a Bayesian int
 erpretation of common deep learning techniques such as dropout. This groun
 ding of dropout in approximate Bayesian inference suggests an extension of
  the theoretical results\, offering insights into the use of dropout with 
 RNN models. We apply this new variational inference based dropout techniqu
 e in LSTM and GRU models\, assessing it on language modelling and sentimen
 t analysis tasks. The new approach outperforms existing techniques\, and t
 o the best of our knowledge improves on the single model state-of-the-art 
 in language modelling with the Penn Treebank (73.4 test perplexity). This 
 extends our arsenal of variational tools in deep learning.
LOCATION:Area 5+6+7+8 #22
END:VEVENT
BEGIN:VEVENT
SUMMARY:Brains on Beats | Umut Güçlü \, Jordy Thielen \, Michael Hanke 
 \, Marcel van Gerven \, Marcel A. J. van Gerven
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Brains on Beats\nUmut Güçlü \, Jordy Thielen \, Mich
 ael Hanke \, Marcel van Gerven \, Marcel A. J. van Gerven\nhttp://nips.cc/
 Conferences/2016/Schedule?showEvent=7251\n\nWe developed task-optimized de
 ep neural networks (DNNs) that achieved state-of-the-art performance in di
 fferent evaluation scenarios for automatic music tagging. These DNNs were 
 subsequently used to probe the neural representations of music. Representa
 tional similarity analysis revealed the existence of a representational gr
 adient across the superior temporal gyrus (STG). Anterior STG was shown to
  be more sensitive to low-level stimulus features encoded in shallow DNN l
 ayers whereas posterior STG was shown to be more sensitive to high-level s
 timulus features encoded in deep DNN layers.
LOCATION:Area 5+6+7+8 #23
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Communication-Efficient Parallel Algorithm for Decision Tree | Q
 i Meng \, Guolin Ke \, Taifeng Wang \, Wei Chen \, Qiwei Ye \, Zhi-Ming Ma
  \, Tieyan Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Communication-Efficient Parallel Algorithm for Decisi
 on Tree\nQi Meng \, Guolin Ke \, Taifeng Wang \, Wei Chen \, Qiwei Ye \, Z
 hi-Ming Ma \, Tieyan Liu\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7252\n\nDecision tree (and its extensions such as Gradient Boosting Dec
 ision Trees and Random Forest) is a widely used machine learning algorithm
 \, due to its practical effectiveness and model interpretability. With the
  emergence of big data\, there is an increasing need to parallelize the tr
 aining process of decision tree. However\, most existing attempts along th
 is line suffer from high communication costs. In this paper\, we propose a
  new algorithm\, called \\emph{Parallel Voting Decision Tree (PV-Tree)}\, 
 to tackle this challenge. After partitioning the training data onto a numb
 er of (e.g.\, $M$) machines\, this algorithm performs both local voting an
 d global voting in each iteration. For local voting\, the top-$k$ attribut
 es are selected from each machine according to its local data. Then\, the 
 indices of these top attributes are aggregated by a server\, and the globa
 lly top-$2k$ attributes are determined by a majority voting among these lo
 cal candidates. Finally\, the full-grained histograms of the globally top-
 $2k$ attributes are collected from local machines in order to identify the
  best (most informative) attribute and its split point. PV-Tree can achiev
 e a very low communication cost (independent of the total number of attrib
 utes) and thus can scale out very well. Furthermore\, theoretical analysis
  shows that this algorithm can learn a near optimal decision tree\, since 
 it can find the best attribute with a large probability. Our experiments o
 n real-world datasets show that PV-Tree significantly outperforms the exis
 ting parallel decision tree algorithms in the tradeoff between accuracy an
 d efficiency.
LOCATION:Area 5+6+7+8 #24
END:VEVENT
BEGIN:VEVENT
SUMMARY:Leveraging Sparsity for Efficient Submodular Data Summarization | 
 Erik Lindgren \, Shanshan Wu \, Alexandros Dimakis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Leveraging Sparsity for Efficient Submodular Data Summa
 rization\nErik Lindgren \, Shanshan Wu \, Alexandros Dimakis\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=7253\n\nThe facility location probl
 em is widely used for summarizing large datasets and has additional applic
 ations in sensor placement\, image retrieval\, and clustering. One difficu
 lty of this problem is that submodular optimization algorithms require the
  calculation of pairwise benefits for all items in the dataset. This is in
 feasible for large problems\, so recent work proposed to only calculate ne
 arest neighbor benefits. One limitation is that several strong assumptions
  were invoked to obtain provable approximation guarantees. In this paper w
 e establish that these extra assumptions are not necessary—solving the s
 parsified problem will be almost optimal under the standard assumptions of
  the problem. We then analyze a different method of sparsification that is
  a better model for methods such as Locality Sensitive Hashing to accelera
 te the nearest neighbor computations and extend the use of the problem to 
 a broader family of similarities. We validate our approach by demonstratin
 g that it rapidly generates interpretable summaries.
LOCATION:Area 5+6+7+8 #25
END:VEVENT
BEGIN:VEVENT
SUMMARY:Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and 
 Peer Prediction | Jacob Steinhardt \, Gregory Valiant \, Moses Charikar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Avoiding Imposters and Delinquents: Adversarial Crowdso
 urcing and Peer Prediction\nJacob Steinhardt \, Gregory Valiant \, Moses C
 harikar\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7254\n\nWe con
 sider a crowdsourcing model in which n workers are asked to rate the quali
 ty of n items previously generated by other workers. An unknown set of $\\
 alpha n$ workers generate reliable ratings\, while the remaining workers m
 ay behave arbitrarily and possibly adversarially. The manager of the exper
 iment can also manually evaluate the quality of a small number of items\, 
 and wishes to curate together almost all of the high-quality items with at
  most an fraction of low-quality items. Perhaps surprisingly\, we show tha
 t this is possible with an amount of work required of the manager\, and ea
 ch worker\, that does not scale with n: the dataset can be curated with $\
 \tilde{O}(1/\\beta\\alpha\\epsilon^4)$ ratings per worker\, and $\\tilde{O
 }(1/\\beta\\epsilon^2)$ ratings by the manager\, where $\\beta$ is the fra
 ction of high-quality items. Our results extend to the more general settin
 g of peer prediction\, including peer grading in online classrooms.
LOCATION:Area 5+6+7+8 #26
END:VEVENT
BEGIN:VEVENT
SUMMARY:Designing smoothing functions for improved worst-case competitive 
 ratio in online optimization | Reza Eghbali \, Maryam Fazel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Designing smoothing functions for improved worst-case c
 ompetitive ratio in online optimization\nReza Eghbali \, Maryam Fazel\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7255\n\nOnline optimizatio
 n covers problems such as online resource allocation\, online bipartite ma
 tching\, adwords (a central problem in e-commerce and advertising)\, and a
 dwords with separable concave returns.  We analyze the worst case competit
 ive ratio of two primal-dual algorithms for a class of online convex (coni
 c) optimization problems that contains the previous examples as special ca
 ses defined on the positive orthant.  We derive a sufficient condition on 
 the objective function that guarantees a constant worst case competitive r
 atio (greater than or equal to $\\frac{1}{2}$) for monotone objective func
 tions.  We provide new examples of online problems on the positive orthant
  % and the positive semidefinite cone  that satisfy the sufficient conditi
 on.  We show how smoothing can improve the competitive ratio of these algo
 rithms\, and in particular for separable functions\, we show that the opti
 mal smoothing can be derived by solving a convex optimization problem. Thi
 s result allows us to directly optimize the competitive ratio bound over a
  class of smoothing functions\, and hence  design effective smoothing cust
 omized for a given cost function.
LOCATION:Area 5+6+7+8 #27
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Forget-me-not Process | Kieran Milan \, Joel Veness \, James K
 irkpatrick \, Michael Bowling \, Anna Koop \, Demis Hassabis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:The Forget-me-not Process\nKieran Milan \, Joel Veness 
 \, James Kirkpatrick \, Michael Bowling \, Anna Koop \, Demis Hassabis\nht
 tp://nips.cc/Conferences/2016/Schedule?showEvent=7256\n\nWe introduce the 
 Forget-me-not Process\, an efficient\, non-parametric meta-algorithm for o
 nline probabilistic sequence prediction for piecewise stationary\, repeati
 ng sources. Our method works by taking a Bayesian approach to partition a 
 stream of data into postulated task-specific segments\, while simultaneous
 ly building a model for each task. We provide regret guarantees with respe
 ct to piecewise stationary data sources under the logarithmic loss\, and v
 alidate the method empirically across a range of sequence prediction and t
 ask identification problems.
LOCATION:Area 5+6+7+8 #28
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generating Videos with Scene Dynamics | Carl Vondrick \, Hamed Pir
 siavash \, Antonio Torralba
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Generating Videos with Scene Dynamics\nCarl Vondrick \,
  Hamed Pirsiavash \, Antonio Torralba\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=7257\n\nWe capitalize on large amounts of unlabeled video 
 in order to learn a model of scene dynamics for both video recognition tas
 ks (e.g. action classification) and video generation tasks (e.g. future pr
 ediction). We propose a generative adversarial network for video with a sp
 atio-temporal convolutional architecture that untangles the scene's foregr
 ound from the background. Experiments suggest this model can generate tiny
  videos up to a second at full frame rate better than simple baselines\, a
 nd we show its utility at predicting plausible futures of static images. M
 oreover\, experiments and visualizations show the model internally learns 
 useful features for recognizing actions with minimal supervision\, suggest
 ing scene dynamics are a promising signal for representation learning. We 
 believe generative video models can impact many applications in video unde
 rstanding and simulation.
LOCATION:Area 5+6+7+8 #29
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Robustness of Estimator Composition | Pingfan Tang \, Jeff M P
 hillips
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:The Robustness of Estimator Composition\nPingfan Tang \
 , Jeff M Phillips\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7258
 \n\nWe formalize notions of robustness for composite estimators via the no
 tion of a breakdown point.  A composite estimator successively applies two
  (or more) estimators: on data decomposed into disjoint parts\, it applies
  the first estimator on each part\, then the second estimator on the outpu
 ts of the first estimator. And so on\, if the composition is of more than 
 two estimators. Informally\, the breakdown point is the minimum fraction o
 f data points which if significantly modified will also significantly modi
 fy the output of the estimator\, so it is typically desirable to have a la
 rge breakdown point. Our main result shows that\, under mild conditions on
  the individual estimators\, the breakdown point of the composite estimato
 r is the product of the breakdown points of the individual estimators. We 
 also demonstrate several scenarios\, ranging from regression to statistica
 l testing\, where this analysis is easy to apply\, useful in understanding
  worst case robustness\, and sheds powerful insights onto the associated d
 ata analysis.
LOCATION:Area 5+6+7+8 #30
END:VEVENT
BEGIN:VEVENT
SUMMARY:Improved Deep Metric Learning with Multi-class N-pair Loss Objecti
 ve | Kihyuk Sohn
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Improved Deep Metric Learning with Multi-class N-pair L
 oss Objective\nKihyuk Sohn\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7259\n\nDeep metric learning has gained much popularity in recent yea
 rs\, following the success of deep learning. However\, existing frameworks
  of deep metric learning based on contrastive loss and triplet loss often 
 suffer from slow convergence\, partially because they employ only one nega
 tive example while not interacting with the other negative classes in each
  update. In this paper\, we propose to address this problem with a new met
 ric learning objective called multi-class N-pair loss. The proposed object
 ive function firstly generalizes triplet loss by allowing joint comparison
  among more than one negative examples – more specifically\, N-1 negativ
 e examples – and secondly reduces the computational burden of evaluating
  deep embedding vectors via an efficient batch construction strategy using
  only N pairs of examples\, instead of (N+1)×N. We demonstrate the superi
 ority of our proposed loss to the triplet loss as well as other competing 
 loss functions for a variety of tasks on several visual recognition benchm
 ark\, including fine-grained object recognition and verification\, image c
 lustering and retrieval\, and face verification and identification.
LOCATION:Area 5+6+7+8 #31
END:VEVENT
BEGIN:VEVENT
SUMMARY:Preference Completion from Partial Rankings | Suriya Gunasekar \, 
 Oluwasanmi Koyejo \, Joydeep Ghosh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Preference Completion from Partial Rankings\nSuriya Gun
 asekar \, Oluwasanmi Koyejo \, Joydeep Ghosh\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=7260\n\nWe propose a novel and efficient algorithm 
 for the collaborative preference completion problem\, which involves joint
 ly estimating individualized rankings for a set of entities over a shared 
 set of items\, based on a limited number of observed affinity values. Our 
 approach exploits the observation that while preferences are often recorde
 d as numerical scores\, the predictive quantity of interest is the underly
 ing rankings. Thus\, attempts to closely match the recorded scores may lea
 d to overfitting and impair generalization performance. Instead\, we propo
 se an estimator that directly fits the underlying preference order\, combi
 ned with nuclear norm constraints to encourage low--rank parameters.  Besi
 des (approximate) correctness of the ranking order\, the proposed estimato
 r makes no generative assumption on the numerical scores of the observatio
 ns. One consequence is that  the proposed estimator can fit any consistent
  partial ranking over a subset of the items represented as a directed acyc
 lic graph (DAG)\, generalizing standard techniques that can only fit prefe
 rence scores. Despite this generality\, for supervision representing total
  or blockwise total orders\, the computational complexity of our algorithm
  is within a $\\log$ factor of the standard algorithms for nuclear norm re
 gularization based estimates for matrix completion. We further show promis
 ing empirical results for a novel and challenging application of collabora
 tively ranking of the associations between brain--regions and cognitive ne
 uroscience terms.
LOCATION:Area 5+6+7+8 #32
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian optimization under mixed constraints  with a slack-variab
 le augmented Lagrangian | Victor Picheny \, Robert B Gramacy \, Stefan Wil
 d \, Sebastien Le Digabel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Bayesian optimization under mixed constraints  with a s
 lack-variable augmented Lagrangian\nVictor Picheny \, Robert B Gramacy \, 
 Stefan Wild \, Sebastien Le Digabel\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7261\n\nAn augmented Lagrangian (AL) can convert a constrain
 ed optimization problem into a sequence of simpler (e.g.\, unconstrained) 
 problems which are then usually solved with local solvers. Recently\, surr
 ogate-based Bayesian optimization (BO) sub-solvers have been successfully 
 deployed in the AL framework for a more global search in the presence of i
 nequality constraints\; however a drawback was that expected improvement (
 EI) evaluations relied on Monte Carlo. Here we introduce an alternative sl
 ack variable AL\, and show that in this formulation the EI may be evaluate
 d with library routines. The slack variables furthermore facilitate equali
 ty as well as inequality constraints\, and mixtures thereof. We show our n
 ew slack "ALBO" compares favorably to the original. Its superiority over c
 onventional alternatives is reinforced on several new mixed constraint exa
 mples.
LOCATION:Area 5+6+7+8 #33
END:VEVENT
BEGIN:VEVENT
SUMMARY:Privacy Odometers and Filters: Pay-as-you-Go Composition | Ryan M 
 Rogers \, Salil Vadhan \, Aaron Roth \, Jonathan Ullman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Privacy Odometers and Filters: Pay-as-you-Go Compositio
 n\nRyan M Rogers \, Salil Vadhan \, Aaron Roth \, Jonathan Ullman\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=7262\n\nIn this paper we initi
 ate the study of adaptive composition in differential privacy when the len
 gth of the composition\, and the privacy parameters themselves can be chos
 en adaptively\, as a function of the outcome of previously run analyses. T
 his case is much more delicate than the setting covered by existing compos
 ition theorems\, in which the algorithms themselves can be chosen adaptive
 ly\, but the privacy parameters must be fixed up front. Indeed\, it isn't 
 even clear how to define differential privacy in the adaptive parameter se
 tting. We proceed by defining two objects which cover the two main use cas
 es of composition theorems. A privacy filter is a stopping time rule that 
 allows an analyst to halt a computation before his pre-specified privacy b
 udget is exceeded. A privacy odometer allows the analyst to track realized
  privacy loss as he goes\, without needing to pre-specify a privacy budget
 . We show that unlike the case in which privacy parameters are fixed\, in 
 the adaptive parameter setting\, these two use cases are distinct. We show
  that there exist privacy filters with bounds comparable (up to constants)
  with existing privacy composition theorems. We also give a privacy odomet
 er that nearly matches non-adaptive private composition theorems\, but is 
 sometimes worse by a small asymptotic factor. Moreover\, we show that this
  is inherent\, and that any valid privacy odometer in the adaptive paramet
 er setting must lose this factor\, which shows a formal separation between
  the filter and odometer use-cases.
LOCATION:Area 5+6+7+8 #34
END:VEVENT
BEGIN:VEVENT
SUMMARY:Large Margin Discriminant Dimensionality Reduction in Prediction S
 pace | Mohammad Saberian \, Jose Costa Pereira \, Nuno Nvasconcelos \, Can
  Xu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Large Margin Discriminant Dimensionality Reduction in P
 rediction Space\nMohammad Saberian \, Jose Costa Pereira \, Nuno Nvasconce
 los \, Can Xu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7263\n\n
 In this paper we establish a duality between boosting and SVM\, and use th
 is to derive a novel discriminant dimensionality reduction algorithm. In p
 articular\, using the multiclass formulation of boosting and SVM we note t
 hat both use a combination of mapping and linear classification to maximiz
 e the multiclass margin. In SVM this is implemented using a pre-defined ma
 pping (induced by the kernel) and optimizing the linear classifiers. In bo
 osting the linear classifiers are pre-defined and the mapping (predictor) 
 is learned through combination of weak learners. We argue that the interme
 diate mapping\, e.g. boosting predictor\, is preserving the discriminant a
 spects of the data and by controlling the dimension of this mapping it is 
 possible to achieve discriminant low dimensional representations for the d
 ata. We use the aforementioned duality and propose a new method\, Large Ma
 rgin Discriminant Dimensionality Reduction (LADDER) that jointly learns th
 e mapping and the linear classifiers in an efficient manner. This leads to
  a data-driven mapping which can embed data into any number of dimensions.
  Experimental results show that this embedding can significantly improve p
 erformance on tasks such as hashing and image/scene classification.
LOCATION:Area 5+6+7+8 #35
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tight Complexity Bounds for Optimizing Composite Objectives | Blak
 e E Woodworth \, Nati Srebro
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Tight Complexity Bounds for Optimizing Composite Object
 ives\nBlake E Woodworth \, Nati Srebro\nhttp://nips.cc/Conferences/2016/Sc
 hedule?showEvent=7264\n\nWe provide tight upper and lower bounds on the co
 mplexity of minimizing the average of m convex functions using gradient an
 d prox oracles of the component functions. We show a significant gap betwe
 en the complexity of deterministic vs randomized optimization. For smooth 
 functions\, we show that accelerated gradient descent (AGD) and an acceler
 ated variant of SVRG are optimal in the deterministic and randomized setti
 ngs respectively\, and that a gradient oracle is sufficient for the optima
 l rate. For non-smooth functions\, having access to prox oracles reduces t
 he complexity and we present optimal methods based on smoothing that impro
 ve over methods using just gradient accesses.
LOCATION:Area 5+6+7+8 #36
END:VEVENT
BEGIN:VEVENT
SUMMARY:Automatic Neuron Detection in Calcium Imaging Data Using Convoluti
 onal Networks | Noah Apthorpe \, Alexander Riordan \, Robert Aguilar \, Ja
 n Homann \, Yi Gu \, David Tank \, H. Sebastian Seung
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Automatic Neuron Detection in Calcium Imaging Data Usin
 g Convolutional Networks\nNoah Apthorpe \, Alexander Riordan \, Robert Agu
 ilar \, Jan Homann \, Yi Gu \, David Tank \, H. Sebastian Seung\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=7265\n\nCalcium imaging is an im
 portant technique for monitoring the activity of thousands of neurons simu
 ltaneously. As calcium imaging datasets grow in size\, automated detection
  of individual neurons is becoming important. Here we apply a supervised l
 earning approach to this problem and show that convolutional networks can 
 achieve near-human accuracy and superhuman speed. Accuracy is superior to 
 the popular PCA/ICA method based on precision and recall relative to groun
 d truth annotation by a human expert. These results suggest that convoluti
 onal networks are an efficient and flexible tool for the analysis of large
 -scale calcium imaging data.
LOCATION:Area 5+6+7+8 #37
END:VEVENT
BEGIN:VEVENT
SUMMARY:Hierarchical Deep Reinforcement Learning: Integrating Temporal Abs
 traction and Intrinsic Motivation | Tejas D Kulkarni \, Karthik Narasimhan
  \, Ardavan Saeedi \, Josh Tenenbaum
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Hierarchical Deep Reinforcement Learning: Integrating T
 emporal Abstraction and Intrinsic Motivation\nTejas D Kulkarni \, Karthik 
 Narasimhan \, Ardavan Saeedi \, Josh Tenenbaum\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=7266\n\nLearning goal-directed behavior in enviro
 nments with sparse feedback is a major challenge for reinforcement learnin
 g algorithms. One of the key difficulties is insufficient exploration\, re
 sulting in an agent being unable to learn robust policies. Intrinsically m
 otivated agents can explore new behavior for their own sake rather than to
  directly solve external goals. Such intrinsic behaviors could eventually 
 help the agent solve tasks posed by the environment. We present hierarchic
 al-DQN (h-DQN)\, a framework to integrate hierarchical action-value functi
 ons\, operating at different temporal scales\, with goal-driven intrinsica
 lly motivated deep reinforcement learning. A top-level q-value function le
 arns a policy over intrinsic goals\, while a lower-level function learns a
  policy over atomic actions to satisfy the given goals. h-DQN allows for f
 lexible goal specifications\, such as functions over entities and relation
 s. This provides an efficient space for exploration in complicated environ
 ments. We demonstrate the strength of our approach on two problems with ve
 ry sparse and delayed feedback: (1) a complex discrete stochastic decision
  process with stochastic transitions\, and (2) the classic ATARI game -- `
 Montezuma's Revenge'.
LOCATION:Area 5+6+7+8 #38
END:VEVENT
BEGIN:VEVENT
SUMMARY:Conditional Image Generation with PixelCNN Decoders | Aaron van de
 n Oord \, Nal Kalchbrenner \, Lasse Espeholt \, koray kavukcuoglu \, Oriol
  Vinyals \, Alex Graves
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Conditional Image Generation with PixelCNN Decoders\nAa
 ron van den Oord \, Nal Kalchbrenner \, Lasse Espeholt \, koray kavukcuogl
 u \, Oriol Vinyals \, Alex Graves\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7267\n\nThis work explores conditional image generation with a
  new image density model based on the PixelCNN architecture. The model can
  be conditioned on any vector\, including descriptive labels or tags\, or 
 latent embeddings created by other networks. When conditioned on class lab
 els from the ImageNet database\, the model is able to generate diverse\, r
 ealistic scenes representing distinct animals\, objects\, landscapes and s
 tructures. When conditioned on an embedding produced by a convolutional ne
 twork given a single image of an unseen face\, it generates a variety of n
 ew portraits of the same person with different facial expressions\, poses 
 and lighting conditions. We also show that conditional PixelCNN can serve 
 as a powerful decoder in an image autoencoder. Additionally\, the gated co
 nvolutional layers in the proposed model improve the log-likelihood of Pix
 elCNN to match the state-of-the-art performance of PixelRNN on ImageNet\, 
 with greatly reduced computational cost.
LOCATION:Area 5+6+7+8 #39
END:VEVENT
BEGIN:VEVENT
SUMMARY:Natural-Parameter Networks: A Class of Probabilistic Neural Networ
 ks | Hao Wang \, Xingjian SHI \, Dit-Yan Yeung
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Natural-Parameter Networks: A Class of Probabilistic Ne
 ural Networks\nHao Wang \, Xingjian SHI \, Dit-Yan Yeung\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7268\n\nNeural networks (NN) have achie
 ved state-of-the-art performance in various applications. Unfortunately in
  applications where training data is insufficient\, they are often prone t
 o overfitting. One effective way to alleviate this problem is to exploit t
 he Bayesian approach by using Bayesian neural networks (BNN). Another shor
 tcoming of NN is the lack of flexibility to customize different distributi
 ons for the weights and neurons according to the data\, as is often done i
 n probabilistic graphical models. To address these problems\, we propose a
  class of probabilistic neural networks\, dubbed natural-parameter network
 s (NPN)\, as a novel and lightweight Bayesian treatment of NN. NPN allows 
 the usage of arbitrary exponential-family distributions to model the weigh
 ts and neurons. Different from traditional NN and BNN\, NPN takes distribu
 tions as input and goes through layers of transformation before producing 
 distributions to match the target output distributions. As a Bayesian trea
 tment\, efficient backpropagation (BP) is performed to learn the natural p
 arameters for the distributions over both the weights and neurons. The out
 put distributions of each layer\, as byproducts\, may be used as second-or
 der representations for the associated tasks such as link prediction. Expe
 riments on real-world datasets show that NPN can achieve state-of-the-art 
 performance.
LOCATION:Area 5+6+7+8 #40
END:VEVENT
BEGIN:VEVENT
SUMMARY:Long-term Causal Effects via Behavioral Game Theory | Panagiotis T
 oulis \, David C Parkes
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Long-term Causal Effects via Behavioral Game Theory\nPa
 nagiotis Toulis \, David C Parkes\nhttp://nips.cc/Conferences/2016/Schedul
 e?showEvent=7269\n\nPlanned experiments are the gold standard in reliably 
 comparing the causal effect of switching from a baseline policy to a new p
 olicy. % One critical shortcoming of classical experimental methods\, howe
 ver\, is that they typically do not take into account the dynamic nature o
 f response to policy changes. For instance\, in an experiment where we see
 k to understand the effects of a new ad pricing policy on auction revenue\
 , agents may adapt their bidding in response to the experimental pricing c
 hanges. Thus\, causal effects of the new pricing policy after such adaptat
 ion period\, the {\\em long-term causal effects}\, are not captured by the
  classical methodology even though they clearly are more indicative of the
  value of the new policy. %  Here\, we formalize a framework to define and
  estimate long-term causal effects of   policy changes in multiagent econo
 mies.  Central to our approach is behavioral game theory\, which we levera
 ge   to formulate the ignorability assumptions that are necessary for caus
 al inference.  Under such assumptions we estimate long-term causal effects
  through a latent space approach\, where a behavioral model of how agents 
 act conditional on their latent behaviors is combined with a temporal mode
 l of how behaviors evolve over time.
LOCATION:Area 5+6+7+8 #41
END:VEVENT
BEGIN:VEVENT
SUMMARY:PerforatedCNNs: Acceleration through Elimination of Redundant Conv
 olutions | Mikhail Figurnov \, Aizhan Ibraimova \, Dmitry P Vetrov \, Push
 meet Kohli
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:PerforatedCNNs: Acceleration through Elimination of Red
 undant Convolutions\nMikhail Figurnov \, Aizhan Ibraimova \, Dmitry P Vetr
 ov \, Pushmeet Kohli\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7
 270\n\nWe propose a novel approach to reduce the computational cost of eva
 luation of convolutional neural networks\, a factor that has hindered thei
 r deployment in low-power devices such as mobile phones. Inspired by the l
 oop perforation technique from source code optimization\, we speed up the 
 bottleneck convolutional layers by skipping their evaluation in some of th
 e spatial positions. We propose and analyze several strategies of choosing
  these positions. We demonstrate that perforation can accelerate modern co
 nvolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Ad
 ditionally\, we show that perforation is complementary to the recently pro
 posed acceleration method of Zhang et al.
LOCATION:Area 5+6+7+8 #42
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Probabilistic Programming Approach To Probabilistic Data Analysi
 s | Feras Saad \, Vikash K Mansinghka
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Probabilistic Programming Approach To Probabilistic D
 ata Analysis\nFeras Saad \, Vikash K Mansinghka\nhttp://nips.cc/Conference
 s/2016/Schedule?showEvent=7271\n\nProbabilistic techniques are central to 
 data analysis\, but different approaches can be challenging to apply\, com
 bine\, and compare. This paper introduces composable generative population
  models (CGPMs)\, a computational abstraction that extends directed graphi
 cal models and can be used to describe and compose a broad class of probab
 ilistic data analysis techniques. Examples include discriminative machine 
 learning\, hierarchical Bayesian models\, multivariate kernel methods\, cl
 ustering algorithms\, and arbitrary probabilistic programs. We demonstrate
  the integration of CGPMs into BayesDB\, a probabilistic programming platf
 orm that can express data analysis tasks using a modeling definition langu
 age and structured query language. The practical value is illustrated in t
 wo ways. First\, the paper describes an analysis on a database of Earth sa
 tellites\, which identifies records that probably violate Kepler’s Third
  Law by composing causal probabilistic programs with non-parametric Bayes 
 in 50 lines of probabilistic code. Second\, it reports the lines of code a
 nd accuracy of CGPMs compared with baseline solutions from standard machin
 e learning libraries.
LOCATION:Area 5+6+7+8 #43
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Bayesian networks with ancestral constraints | Eunice Yuh
 -Jie Chen \, Yujia Shen \, Arthur Choi \, Adnan Darwiche
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning Bayesian networks with ancestral constraints\n
 Eunice Yuh-Jie Chen \, Yujia Shen \, Arthur Choi \, Adnan Darwiche\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7272\n\nWe consider the probl
 em of learning Bayesian networks optimally\, when subject to background kn
 owledge in the form of ancestral constraints. Our approach is based on a r
 ecently proposed framework for optimal structure learning based on non-dec
 omposable scores\, which is general enough to accommodate ancestral constr
 aints. The proposed framework exploits oracles for learning structures usi
 ng decomposable scores\, which cannot accommodate ancestral constraints si
 nce they are non-decomposable. We show how to empower these oracles by pas
 sing them decomposable constraints that they can handle\, which are inferr
 ed from ancestral constraints that they cannot handle. Empirically\, we de
 monstrate that our approach can be orders-of-magnitude more efficient than
  alternative frameworks\, such as those based on integer linear programmin
 g.
LOCATION:Area 5+6+7+8 #44
END:VEVENT
BEGIN:VEVENT
SUMMARY:Solving Random Systems of Quadratic Equations via Truncated Genera
 lized Gradient Flow | Gang Wang \, Georgios Giannakis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Solving Random Systems of Quadratic Equations via Trunc
 ated Generalized Gradient Flow\nGang Wang \, Georgios Giannakis\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=7273\n\nThis paper puts forth a 
 novel algorithm\, termed \\emph{truncated generalized gradient flow} (TGGF
 )\, to solve for $\\bm{x}\\in\\mathbb{R}^n/\\mathbb{C}^n$ a system of $m$ 
 quadratic equations $yi=|\\langle\\bm{a}i\,\\bm{x}\\rangle|^2$\, $i=1\,2\,
 \\ldots\,m$\, which even for $\\left{\\bm{a}i\\in\\mathbb{R}^n/\\mathbb{C}
 ^n\\right}{i=1}^m$ random is known to be \\emph{NP-hard} in general. We pr
 ove that as soon as the number of equations $m$ is on the order of the num
 ber of unknowns $n$\, TGGF recovers the solution exactly (up to a global u
 nimodular constant) with high probability and complexity growing linearly 
 with the time required to read the data $\\left{\\left(\\bm{a}i\;\\\,yi\\r
 ight)\\right}_{i=1}^m$. Specifically\, TGGF proceeds in two stages: s1) A 
 novel \\emph{orthogonality-promoting} initialization that is obtained with
  simple power iterations\; and\, s2) a refinement of the initial estimate 
 by successive updates of scalable \\emph{truncated generalized gradient it
 erations}. The former is in sharp contrast to the existing spectral initia
 lizations\, while the latter handles the rather challenging nonconvex and 
 nonsmooth \\emph{amplitude-based} cost function. Numerical tests demonstra
 te that: i) The novel orthogonality-promoting initialization method return
 s more accurate and robust estimates relative to its spectral counterparts
 \; and ii) even with the same initialization\, our refinement/truncation o
 utperforms Wirtinger-based alternatives\, all corroborating the superior p
 erformance of TGGF over state-of-the-art algorithms.
LOCATION:Area 5+6+7+8 #45
END:VEVENT
BEGIN:VEVENT
SUMMARY:Balancing Suspense and Surprise: Timely Decision Making with Endog
 enous Information Acquisition | Ahmed Ibrahim \, Mihaela Van Der Schaar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Balancing Suspense and Surprise: Timely Decision Making
  with Endogenous Information Acquisition\nAhmed Ibrahim \, Mihaela Van Der
  Schaar\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7274\n\nWe dev
 elop a Bayesian model for decision-making under time pressure with endogen
 ous information acquisition. In our model\, the decision-maker decides whe
 n to observe (costly) information by sampling an underlying continuous-tim
 e stochastic process (time series) that conveys information about the pote
 ntial occurrence/non-occurrence of an adverse event which will terminate t
 he decision-making process. In her attempt to predict the occurrence of th
 e adverse event\, the decision-maker follows a policy that determines when
  to acquire information from the time series (continuation)\, and when to 
 stop acquiring information and make a final prediction (stopping). We show
  that the optimal policy has a "rendezvous" structure\, i.e. a structure i
 n which whenever a new information sample is gathered from the time series
 \, the optimal "date" for acquiring the next sample becomes computable. Th
 e optimal interval between two information samples balances a trade-off be
 tween the decision maker’s "surprise"\, i.e. the drift in her posterior 
 belief after observing new information\, and "suspense"\, i.e. the probabi
 lity that the adverse event occurs in the time interval between two inform
 ation samples. Moreover\, we characterize the continuation and stopping re
 gions in the decision-maker’s state-space\, and show that they depend no
 t only on the decision-maker’s beliefs\, but also on the "context"\, i.e
 . the current realization of the time series.
LOCATION:Area 5+6+7+8 #46
END:VEVENT
BEGIN:VEVENT
SUMMARY:Structure-Blind Signal Recovery | Dmitry Ostrovsky \, Zaid Harchao
 ui \, Anatoli Juditsky \, Arkadi S Nemirovski
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Structure-Blind Signal Recovery\nDmitry Ostrovsky \, Za
 id Harchaoui \, Anatoli Juditsky \, Arkadi S Nemirovski\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7275\n\nWe consider the problem of recov
 ering a signal observed in Gaussian noise. If the set of signals is convex
  and compact\, and can be specified beforehand\, one can use classical lin
 ear estimators that achieve a risk within a constant factor of the minimax
  risk. However\, when the set is unspecified\, designing an estimator that
  is blind to the hidden structure of the signal remains a challenging prob
 lem. We propose a new family of estimators to recover signals observed in 
 Gaussian noise. Instead of specifying the set where the signal lives\, we 
 assume the existence of a well-performing linear estimator. Proposed estim
 ators enjoy exact oracle inequalities and can be efficiently computed thro
 ugh convex optimization. We present several numerical illustrations that s
 how the potential of the approach.
LOCATION:Area 5+6+7+8 #47
END:VEVENT
BEGIN:VEVENT
SUMMARY:Spatiotemporal Residual Networks for Video Action Recognition | Ch
 ristoph Feichtenhofer \, Axel Pinz \, Richard Wildes
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Spatiotemporal Residual Networks for Video Action Recog
 nition\nChristoph Feichtenhofer \, Axel Pinz \, Richard Wildes\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7276\n\nTwo-stream Convolutional 
 Networks (ConvNets) have shown strong performance for human action recogni
 tion in videos. Recently\, Residual Networks (ResNets) have arisen as a ne
 w technique to train extremely deep architectures. In this paper\, we intr
 oduce spatiotemporal ResNets as a combination of these two approaches. Our
  novel architecture generalizes ResNets for the spatiotemporal domain by i
 ntroducing residual connections in two ways. First\, we inject residual co
 nnections between the appearance and motion pathways of a two-stream archi
 tecture to allow spatiotemporal interaction between the two streams. Secon
 d\, we transform pretrained image ConvNets into spatiotemporal networks by
  equipping these with learnable convolutional filters that are initialized
  as temporal residual connections and operate on adjacent feature maps in 
 time.  This approach slowly increases the spatiotemporal receptive field a
 s the depth of the model increases and naturally integrates image ConvNet 
 design principles. The whole model is trained end-to-end to allow hierarch
 ical learning of complex spatiotemporal features. We evaluate our novel sp
 atiotemporal ResNet using two widely used action recognition benchmarks wh
 ere it exceeds the previous state-of-the-art.
LOCATION:Area 5+6+7+8 #48
END:VEVENT
BEGIN:VEVENT
SUMMARY:CMA-ES with Optimal Covariance Update and Storage Complexity | Osw
 in Krause \, Dídac Rodríguez Arbonès \, Christian Igel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:CMA-ES with Optimal Covariance Update and Storage Compl
 exity\nOswin Krause \, Dídac Rodríguez Arbonès \, Christian Igel\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=7277\n\nThe covariance matri
 x adaptation evolution strategy (CMA-ES) is arguably one of the most power
 ful real-valued derivative-free optimization algorithms\, finding many app
 lications in machine learning. The CMA-ES is a Monte Carlo method\, sampli
 ng from a sequence of multi-variate Gaussian distributions. Given the func
 tion values at the sampled points\, updating and storing the covariance ma
 trix dominates the time and space complexity in each iteration of the algo
 rithm. We propose a numerically stable quadratic-time covariance matrix up
 date scheme with minimal memory requirements based on maintaining triangul
 ar Cholesky factors. This requires a modification of the cumulative step-s
 ize adaption (CSA) mechanism in the CMA-ES\, in which we replace the inver
 se of the square root of the covariance matrix by the inverse of the trian
 gular Cholesky factor. Because the triangular Cholesky factor changes smoo
 thly with the matrix square root\, this modification does not change the b
 ehavior of the CMA-ES in terms of required objective function evaluations 
 as verified empirically. Thus\, the described algorithm can and should rep
 lace the standard CMA-ES if updating and storing the covariance matrix mat
 ters.
LOCATION:Area 5+6+7+8 #49
END:VEVENT
BEGIN:VEVENT
SUMMARY:Latent Attention For If-Then Program Synthesis | Chang Liu \, Xiny
 un Chen \, Eui Chul Shin \, Mingcheng Chen \, Dawn Song
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Latent Attention For If-Then Program Synthesis\nChang L
 iu \, Xinyun Chen \, Eui Chul Shin \, Mingcheng Chen \, Dawn Song\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=7278\n\nAutomatic translation 
 from natural language descriptions into programs is a long-standing challe
 nging problem. In this work\, we consider a simple yet important sub-probl
 em: translation from textual  descriptions to If-Then programs. We devise 
 a novel neural network architecture for this task which we train end-to-en
 d. Specifically\, we introduce Latent Attention\, which computes multiplic
 ative weights for the words in the description in a two-stage process with
  the goal of better leveraging the natural language structures that indica
 te the relevant parts for predicting program elements. Our architecture re
 duces the error rate by 28.57% compared to prior art. We also propose a on
 e-shot learning scenario of If-Then program synthesis and simulate it with
  our existing dataset. We demonstrate a variation on the training procedur
 e for this scenario that outperforms the original procedure\, significantl
 y closing the gap to the model trained with all data.
LOCATION:Area 5+6+7+8 #50
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimizati
 on with Stochastic Asynchronous PALM | Damek Davis \, Brent Edmunds \, Mad
 eleine Udell
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:The Sound of APALM Clapping: Faster Nonsmooth Nonconvex
  Optimization with Stochastic Asynchronous PALM\nDamek Davis \, Brent Edmu
 nds \, Madeleine Udell\nhttp://nips.cc/Conferences/2016/Schedule?showEvent
 =7279\n\nWe introduce the Stochastic Asynchronous Proximal Alternating Lin
 earized Minimization (SAPALM) method\, a block coordinate stochastic proxi
 mal-gradient method for solving nonconvex\, nonsmooth optimization problem
 s. SAPALM is the first asynchronous parallel optimization method that prov
 ably converges on a large class of nonconvex\, nonsmooth problems. We prov
 e that SAPALM matches the best known rates of convergence --- among synchr
 onous or asynchronous methods --- on this problem class. We provide upper 
 bounds on the number of workers for which we can expect to see a linear sp
 eedup\, which match the best bounds known for less complex problems\, and 
 show that in practice SAPALM achieves this linear speedup. We demonstrate 
 state-of-the-art performance on several matrix factorization problems.
LOCATION:Area 5+6+7+8 #51
END:VEVENT
BEGIN:VEVENT
SUMMARY:An Efficient Streaming Algorithm for the Submodular Cover Problem 
 | Ashkan Norouzi-Fard \, Abbas Bazzi \, Ilija Bogunovic \, Marwa El Halabi
  \, Ya-Ping Hsieh \, Volkan Cevher
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:An Efficient Streaming Algorithm for the Submodular Cov
 er Problem\nAshkan Norouzi-Fard \, Abbas Bazzi \, Ilija Bogunovic \, Marwa
  El Halabi \, Ya-Ping Hsieh \, Volkan Cevher\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=7280\n\nWe initiate the study of the classical Subm
 odular Cover (SC) problem in the data streaming model which we refer to as
  the Streaming Submodular Cover (SSC). We show that any single pass stream
 ing algorithm using sublinear memory in the size of the stream will fail t
 o provide any non-trivial approximation guarantees for SSC. Hence\, we con
 sider a relaxed version of SSC\, where we only seek to find a partial cove
 r. We design the first Efficient bicriteria Submodular Cover Streaming (ES
 C-Streaming) algorithm for this problem\, and provide theoretical guarante
 es for its performance supported by numerical evidence. Our algorithm find
 s solutions that are competitive with the near-optimal offline greedy algo
 rithm despite requiring only a single pass over the data stream. In our nu
 merical experiments\, we evaluate the performance of ESC-Streaming on acti
 ve set selection and large-scale graph cover problems.
LOCATION:Area 5+6+7+8 #52
END:VEVENT
BEGIN:VEVENT
SUMMARY:Attend\, Infer\, Repeat: Fast Scene Understanding with Generative 
 Models | S. M. Ali Eslami \, Nicolas Heess \, Theophane Weber \, Yuval Tas
 sa \, David Szepesvari \, koray kavukcuoglu \, Geoffrey E Hinton
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Attend\, Infer\, Repeat: Fast Scene Understanding with 
 Generative Models\nS. M. Ali Eslami \, Nicolas Heess \, Theophane Weber \,
  Yuval Tassa \, David Szepesvari \, koray kavukcuoglu \, Geoffrey E Hinton
 \nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7281\n\nWe present a 
 framework for efficient inference in structured image models that explicit
 ly reason about objects. We achieve this by performing probabilistic infer
 ence using a recurrent neural network that attends to scene elements and p
 rocesses them one at a time. Crucially\, the model itself learns to choose
  the appropriate number of inference steps. We use this scheme to learn to
  perform inference in partially specified 2D models (variable-sized variat
 ional auto-encoders) and fully specified 3D models (probabilistic renderer
 s). We show that such models learn to identify multiple objects - counting
 \, locating and classifying the elements of a scene - without any supervis
 ion\, e.g.\, decomposing 3D images with various numbers of objects in a si
 ngle forward pass of a neural network at unprecedented speed. We further s
 how that the networks produce accurate inferences when compared to supervi
 sed counterparts\, and that their structure leads to improved generalizati
 on.
LOCATION:Area 5+6+7+8 #53
END:VEVENT
BEGIN:VEVENT
SUMMARY:An ensemble diversity approach to supervised binary hashing | Migu
 el A. Carreira-Perpinan \, Ramin Raziperchikolaei
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:An ensemble diversity approach to supervised binary has
 hing\nMiguel A. Carreira-Perpinan \, Ramin Raziperchikolaei\nhttp://nips.c
 c/Conferences/2016/Schedule?showEvent=7282\n\nBinary hashing is a well-kno
 wn approach for fast approximate nearest-neighbor search in information re
 trieval. Much work has focused on affinity-based objective functions invol
 ving the hash functions or binary codes. These objective functions encode 
 neighborhood information between data points and are often inspired by man
 ifold learning algorithms. They ensure that the hash functions differ from
  each other through constraints or penalty terms that encourage codes to b
 e orthogonal or dissimilar across bits\, but this couples the binary varia
 bles and complicates the already difficult optimization. We propose a much
  simpler approach: we train each hash function (or bit) independently from
  each other\, but introduce diversity among them using techniques from cla
 ssifier ensembles. Surprisingly\, we find that not only is this faster and
  trivially parallelizable\, but it also improves over the more complex\, c
 oupled objective function\, and achieves state-of-the-art precision and re
 call in experiments with image retrieval.
LOCATION:Area 5+6+7+8 #54
END:VEVENT
BEGIN:VEVENT
SUMMARY:End-to-End Goal-Driven Web Navigation | Rodrigo Nogueira \, Kyungh
 yun Cho
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:End-to-End Goal-Driven Web Navigation\nRodrigo Nogueira
  \, Kyunghyun Cho\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7283
 \n\nWe propose a goal-driven web navigation as a benchmark task for evalua
 ting an agent with abilities to understand natural language and plan on pa
 rtially observed environments. In this challenging task\, an agent navigat
 es through a website\, which is represented as a graph consisting of web p
 ages as nodes and hyperlinks as directed edges\, to find a web page in whi
 ch a query appears. The agent is required to have sophisticated high-level
  reasoning based on natural languages and efficient sequential decision-ma
 king capability to succeed. We release a software tool\, called WebNav\, t
 hat automatically transforms a website into this goal-driven web navigatio
 n task\, and as an example\, we make WikiNav\, a dataset constructed from 
 the English Wikipedia. We extensively evaluate different variants of neura
 l net based artificial agents on WikiNav and observe that the proposed goa
 l-driven web navigation well reflects the advances in models\, making it a
  suitable benchmark for evaluating future progress. Furthermore\, we exten
 d the WikiNav with question-answer pairs from Jeopardy! and test the propo
 sed agent based on recurrent neural networks against strong inverted index
  based search engines. The artificial agents trained on WikiNav outperform
 s the engined based approaches\, demonstrating the capability of the propo
 sed goal-driven navigation as a good proxy for measuring the progress in r
 eal-world tasks such as focused crawling and question-answering.
LOCATION:Area 5+6+7+8 #55
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Power of Adaptivity in Identifying Statistical Alternatives | 
 Kevin Jamieson \, Daniel Haas \, Benjamin Recht
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:The Power of Adaptivity in Identifying Statistical Alte
 rnatives\nKevin Jamieson \, Daniel Haas \, Benjamin Recht\nhttp://nips.cc/
 Conferences/2016/Schedule?showEvent=7284\n\nThis paper studies the trade-o
 ff between two different kinds of pure exploration: breadth versus depth. 
 We focus on the most biased coin problem\, asking how many total coin flip
 s are required to identify aheavy'' coin from an infinite bag containing b
 othheavy'' coins with mean $\\theta1 \\in (0\,1)$\, and ``light" coins wit
 h mean $\\theta0 \\in (0\,\\theta1)$\, where heavy coins are drawn from th
 e bag with proportion $\\alpha \\in (0\,1/2)$. When $\\alpha\,\\theta0\,\\
 theta1$ are unknown\, the key difficulty of this problem lies in distingui
 shing whether the two kinds of coins have very similar means\, or whether 
 heavy coins are just extremely rare. While existing solutions to this prob
 lem require some prior knowledge of the parameters $\\theta0\,\\theta_1\,\
 \alpha$\, we propose an adaptive algorithm that requires no such knowledge
  yet still obtains near-optimal sample complexity guarantees. In contrast\
 , we provide a lower bound showing that non-adaptive strategies require at
  least quadratically more samples.  In characterizing this gap between ada
 ptive and nonadaptive strategies\,  we make connections to anomaly detecti
 on and prove lower bounds on the sample complexity of differentiating betw
 een a single parametric distribution and a mixture of two such distributio
 ns.
LOCATION:Area 5+6+7+8 #56
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Probabilistic Framework for Deep Learning | Ankit B Patel \, Min
 h Tan Nguyen \, Richard Baraniuk
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Probabilistic Framework for Deep Learning\nAnkit B Pa
 tel \, Minh Tan Nguyen \, Richard Baraniuk\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7285\n\nWe develop a probabilistic framework for deep
  learning based on the Deep Rendering Mixture Model (DRMM)\, a new generat
 ive probabilistic model that explicitly capture variations in data due to 
 latent task nuisance variables. We demonstrate that max-sum inference in t
 he DRMM yields an algorithm that exactly reproduces the operations in deep
  convolutional neural networks (DCNs)\, providing a first principles deriv
 ation. Our framework provides new insights into the successes and shortcom
 ings of DCNs as well as a principled route to their improvement. DRMM trai
 ning via the Expectation-Maximization (EM) algorithm is a powerful alterna
 tive to DCN back-propagation\, and initial training results are promising.
  Classification based on the DRMM and other variants outperforms DCNs in s
 upervised digit classification\, training 2-3x faster while achieving simi
 lar accuracy. Moreover\, the DRMM is applicable to semi-supervised and uns
 upervised learning tasks\, achieving results that are state-of-the-art in 
 several categories on the MNIST benchmark and comparable to state of the a
 rt on the CIFAR10 benchmark.
LOCATION:Area 5+6+7+8 #57
END:VEVENT
BEGIN:VEVENT
SUMMARY:Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels
  | Ilya Tolstikhin \, Bharath K. Sriperumbudur \, Prof. Bernhard Schölkop
 f
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Minimax Estimation of Maximum Mean Discrepancy with Rad
 ial Kernels\nIlya Tolstikhin \, Bharath K. Sriperumbudur \, Prof. Bernhard
  Schölkopf\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7286\n\nMa
 ximum Mean Discrepancy (MMD) is a distance on the space of probability mea
 sures which has found numerous applications in machine learning and nonpar
 ametric testing. This distance is based on the notion of embedding probabi
 lities in a reproducing kernel Hilbert space. In this paper\, we present t
 he first known lower bounds for the estimation of MMD based on finite samp
 les. Our lower bounds hold for any radial universal kernel on $\\R^d$ and 
 match the existing upper bounds up to constants that depend only on the pr
 operties of the kernel. Using these lower bounds\, we establish the minima
 x rate optimality of the empirical estimator and its $U$-statistic variant
 \, which are usually employed in applications.
LOCATION:Area 5+6+7+8 #58
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Neural Compilation | Rudy R Bunel \, Alban Desmaison \, P
 awan K Mudigonda \, Pushmeet Kohli \, Philip Torr
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Adaptive Neural Compilation\nRudy R Bunel \, Alban Desm
 aison \, Pawan K Mudigonda \, Pushmeet Kohli \, Philip Torr\nhttp://nips.c
 c/Conferences/2016/Schedule?showEvent=7287\n\nThis paper proposes an adapt
 ive neural-compilation framework to address the problem of learning effici
 ent program. Traditional code optimisation strategies used in compilers ar
 e based on applying pre-specified set of transformations that make the cod
 e faster to execute without changing its semantics. In contrast\, our work
  involves adapting programs to make them more efficient while considering 
 correctness only on a target input distribution. Our approach is inspired 
 by the recent works on differentiable representations of programs. We show
  that it is possible to compile programs written in a low-level  language 
 to a differentiable representation. We also show how programs in this repr
 esentation can be optimised to make them efficient on a target distributio
 n of inputs. Experimental results demonstrate that our approach enables le
 arning specifically-tuned algorithms for given data distributions with a h
 igh success rate.
LOCATION:Area 5+6+7+8 #59
END:VEVENT
BEGIN:VEVENT
SUMMARY:Tagger: Deep Unsupervised Perceptual Grouping | Klaus Greff \, Ant
 ti Rasmus \, Mathias Berglund \, Tele Hao \, Harri Valpola \, Juergen Schm
 idhuber
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Tagger: Deep Unsupervised Perceptual Grouping\nKlaus Gr
 eff \, Antti Rasmus \, Mathias Berglund \, Tele Hao \, Harri Valpola \, Ju
 ergen Schmidhuber\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7288
 \n\nWe present a framework for efficient perceptual inference that explici
 tly reasons about the segmentation of its inputs and features.  Rather tha
 n being trained for any specific segmentation\, our framework learns the g
 rouping process in an unsupervised manner or alongside any supervised task
 . We enable a neural network to group the representations of different obj
 ects in an iterative manner through a differentiable mechanism.  We achiev
 e very fast convergence by allowing the system to amortize the joint itera
 tive inference of the groupings and their representations.  In contrast to
  many other recently proposed methods for addressing multi-object scenes\,
  our system does not assume the inputs to be images and can therefore dire
 ctly handle other modalities. We evaluate our method on multi-digit classi
 fication of very cluttered images that require texture segmentation. Remar
 kably our method achieves improved classification performance over convolu
 tional networks despite being fully connected\, by making use of the group
 ing mechanism. Furthermore\, we observe that our system greatly improves u
 pon the semi-supervised result of a baseline Ladder network on our dataset
 . These results are evidence that grouping is a powerful tool that can hel
 p to improve sample efficiency.
LOCATION:Area 5+6+7+8 #60
END:VEVENT
BEGIN:VEVENT
SUMMARY:A scaled Bregman theorem with applications | Richard Nock \, Adity
 a Menon \, Cheng Soon Ong
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A scaled Bregman theorem with applications\nRichard Noc
 k \, Aditya Menon \, Cheng Soon Ong\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7289\n\nBregman divergences play a central role in the desig
 n and analysis of a range of machine learning algorithms through a handful
  of popular theorems. We present a new theorem which shows that ``Bregman 
 distortions'' (employing a potentially non-convex generator) may be exactl
 y re-written as a scaled Bregman divergence computed over transformed data
 . This property can be viewed from the standpoints of geometry (a scaled i
 sometry with adaptive metrics) or convex optimization (relating generalize
 d perspective transforms). Admissible distortions include {geodesic distan
 ces} on curved manifolds and projections or gauge-normalisation.  Our theo
 rem allows one to leverage to the wealth and convenience of Bregman diverg
 ences when analysing algorithms relying on the aforementioned Bregman dist
 ortions.  We illustrate this with three novel applications of our theorem:
  a reduction from multi-class density ratio to class-probability estimatio
 n\, a new adaptive projection free yet norm-enforcing  dual norm mirror de
 scent algorithm\,  and a reduction from clustering on flat manifolds to cl
 ustering on curved manifolds. Experiments on each of these domains validat
 e the analyses and suggest that the scaled Bregman theorem might be a wort
 hy addition to the popular handful of Bregman divergence properties that h
 ave been pervasive in machine learning.
LOCATION:Area 5+6+7+8 #61
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning feed-forward one-shot learners | Luca Bertinetto \, João
  F. Henriques \, Jack Valmadre \, Philip Torr \, Andrea Vedaldi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning feed-forward one-shot learners\nLuca Bertinett
 o \, João F. Henriques \, Jack Valmadre \, Philip Torr \, Andrea Vedaldi\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7290\n\nOne-shot learn
 ing is usually tackled by using generative models or discriminative embedd
 ings. Discriminative methods based on deep learning\, which are very effec
 tive in other learning scenarios\, are ill-suited for one-shot learning as
  they need large amounts of training data. In this paper\, we propose a me
 thod to learn the parameters of a deep model in one shot. We construct the
  learner as a second deep network\, called a learnet\, which predicts the 
 parameters of a pupil network from a single exemplar. In this manner we ob
 tain an efficient feed-forward one-shot learner\, trained end-to-end by mi
 nimizing a one-shot classification objective in a learning to learn formul
 ation. In order to make the construction feasible\, we propose a number of
  factorizations of the parameters of the pupil network. We demonstrate enc
 ouraging results by learning characters from single exemplars in Omniglot\
 , and by tracking visual objects from a single initial exemplar in the Vis
 ual Object Tracking benchmark.
LOCATION:Area 5+6+7+8 #62
END:VEVENT
BEGIN:VEVENT
SUMMARY:Error Analysis of Generalized Nyström Kernel Regression | Hong Ch
 en \, Haifeng Xia \, Heng Huang \, Weidong Cai
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Error Analysis of Generalized Nyström Kernel Regressio
 n\nHong Chen \, Haifeng Xia \, Heng Huang \, Weidong Cai\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7291\n\nNystr\\"{o}m method has been us
 ed successfully to improve the computational efficiency of  kernel ridge r
 egression (KRR). Recently\, theoretical analysis of Nystr\\"{o}m KRR\, inc
 luding generalization bound and convergence rate\, has been established ba
 sed on  reproducing kernel Hilbert space (RKHS) associated with the symmet
 ric positive semi-definite kernel. However\, in real world applications\, 
 RKHS is not always  optimal  and  kernel function is not necessary to be  
 symmetric or positive semi-definite.  In this paper\, we consider  the gen
 eralized Nystr\\"{o}m  kernel regression (GNKR) with $\\ell_2$ coefficient
  regularization\, where the kernel just requires the continuity and bounde
 dness.  Error analysis is provided to characterize its generalization perf
 ormance  and the column norm sampling is introduced to construct the refin
 ed hypothesis space. In particular\,  the fast learning rate with polynomi
 al decay is reached for the GNKR. Experimental analysis demonstrates the s
 atisfactory performance of GNKR with the column norm sampling.
LOCATION:Area 5+6+7+8 #63
END:VEVENT
BEGIN:VEVENT
SUMMARY:Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estim
 ation | Weihao Gao \, Sewoong Oh \, Pramod Viswanath
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Breaking the Bandwidth Barrier: Geometrical Adaptive En
 tropy Estimation\nWeihao Gao \, Sewoong Oh \, Pramod Viswanath\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7292\n\nEstimators of information
  theoretic measures such as entropy and mutual information from samples ar
 e a basic workhorse for many downstream applications in modern data scienc
 e. State of the art approaches have been either geometric (nearest neighbo
 r (NN) based) or kernel based (with bandwidth chosen to be data independen
 t and vanishing sub linearly in the sample size). In this paper we combine
  both these approaches to design new estimators of entropy and mutual info
 rmation that strongly outperform all state of the art methods. Our estimat
 or uses bandwidth choice of fixed $k$-NN distances\; such a choice is both
  data dependent and linearly vanishing in the sample size and necessitates
  a bias cancellation term that  is  universal and independent of the under
 lying distribution. As a byproduct\, we obtain a unified way of obtaining 
 both kernel and NN estimators.  The corresponding theoretical contribution
  relating the geometry of NN distances to asymptotic order statistics  is 
 of independent mathematical interest.
LOCATION:Area 5+6+7+8 #64
END:VEVENT
BEGIN:VEVENT
SUMMARY:Asynchronous Parallel Greedy Coordinate Descent | Yang You \, Xian
 gru Lian \, Ji Liu \, Hsiang-Fu Yu \, Inderjit S Dhillon \, James Demmel \
 , Cho-Jui Hsieh
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Asynchronous Parallel Greedy Coordinate Descent\nYang Y
 ou \, Xiangru Lian \, Ji Liu \, Hsiang-Fu Yu \, Inderjit S Dhillon \, Jame
 s Demmel \, Cho-Jui Hsieh\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=7293\n\nn this paper\, we propose and study an Asynchronous parallel G
 reedy Coordinate Descent (Asy-GCD) algorithm for minimizing a smooth funct
 ion with bounded constraints. At each iteration\, workers asynchronously c
 onduct greedy coordinate descent updates on a block of variables.  In the 
 first part of the paper\, we analyze the theoretical behavior of Asy-GCD a
 nd prove a linear convergence rate.  In the second part\, we develop an ef
 ficient kernel SVM solver based on Asy-GCD in the shared memory multi-core
  setting.  Since our algorithm is fully asynchronous---each core does not 
 need to idle and wait for the other cores---the  resulting algorithm enjoy
 s good speedup and outperforms existing multi-core kernel SVM solvers incl
 uding asynchronous stochastic coordinate descent and multi-core LIBSVM.
LOCATION:Area 5+6+7+8 #65
END:VEVENT
BEGIN:VEVENT
SUMMARY:Structured Prediction Theory Based on Factor Graph Complexity | Co
 rinna Cortes \, Vitaly Kuznetsov \, Mehryar Mohri \, Scott Yang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Structured Prediction Theory Based on Factor Graph Comp
 lexity\nCorinna Cortes \, Vitaly Kuznetsov \, Mehryar Mohri \, Scott Yang\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7294\n\nWe present a g
 eneral theoretical analysis of structured prediction with a series of new 
 results. We give new data-dependent margin guarantees for structured predi
 ction for a very wide family of loss functions and a general family of hyp
 otheses\, with an arbitrary factor graph decomposition. These are the tigh
 test margin bounds known for both standard multi-class and general structu
 red prediction problems.  Our guarantees are expressed in terms of a data-
 dependent complexity measure\, \\emph{factor graph complexity}\, which we 
 show can be estimated from data and bounded in terms of familiar quantitie
 s for several commonly used hypothesis sets\, and a sparsity measure for f
 eatures and graphs. Our proof techniques include generalizations of Talagr
 and's contraction lemma that can be of independent interest. We further ex
 tend our theory by leveraging the principle of Voted Risk Minimization (VR
 M) and show that learning is possible even with complex factor graphs.  We
  present new learning bounds for this advanced setting\, which we use to d
 evise two new algorithms\, \\emph{Voted Conditional Random Field} (VCRF) a
 nd \\emph{Voted Structured Boosting} (StructBoost). These algorithms can m
 ake use of complex features and factor graphs and yet benefit from favorab
 le learning guarantees. We also report the results of experiments with VCR
 F on several datasets to validate our theory.
LOCATION:Area 5+6+7+8 #66
END:VEVENT
BEGIN:VEVENT
SUMMARY:Parameter Learning for Log-supermodular Distributions | Tatiana Sh
 pakova \, Francis Bach
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Parameter Learning for Log-supermodular Distributions\n
 Tatiana Shpakova \, Francis Bach\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7295\n\nWe consider log-supermodular models on binary variables
 \, which are probabilistic models with negative log-densities which are su
 bmodular. These models provide probabilistic interpretations of common com
 binatorial optimization tasks such as image segmentation. In this paper\, 
 we focus primarily on parameter estimation in the models from  known upper
 -bounds on the intractable  log-partition function. We show that the bound
  based on separable optimization on the base polytope of the submodular fu
 nction is always inferior to a bound based on ``perturb-and-MAP'' ideas. T
 hen\, to learn parameters\, given that our approximation of the log-partit
 ion function is an expectation (over our own randomization)\, we use a sto
 chastic subgradient technique to maximize a lower-bound on the log-likelih
 ood. This can also be extended to conditional maximum likelihood. We illus
 trate our new results in a set of experiments in binary image denoising\, 
 where we highlight the flexibility of a probabilistic model to learn with 
 missing data.
LOCATION:Area 5+6+7+8 #67
END:VEVENT
BEGIN:VEVENT
SUMMARY:Exact Recovery of Hard Thresholding Pursuit | Xiaotong Yuan \, Pin
 g Li \, Tong Zhang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Exact Recovery of Hard Thresholding Pursuit\nXiaotong Y
 uan \, Ping Li \, Tong Zhang\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7296\n\nThe Hard Thresholding Pursuit (HTP) is a class of truncated
  gradient descent methods for finding sparse solutions of $\\ell_0$-constr
 ained loss minimization problems. The HTP-style methods have been shown to
  have strong approximation guarantee and impressive numerical performance 
 in high dimensional statistical learning applications. However\, the curre
 nt theoretical treatment of these methods has traditionally been restricte
 d to the analysis of parameter estimation consistency. It remains an open 
 problem to analyze the support recovery performance (a.k.a.\, sparsistency
 ) of this type of methods for recovering the global minimizer of the origi
 nal NP-hard problem. In this paper\, we bridge this gap by showing\, for t
 he first time\, that exact recovery of the global sparse minimizer is poss
 ible for HTP-style methods under restricted strong condition number boundi
 ng conditions. We further show that HTP-style methods are able to recover 
 the support of certain relaxed sparse solutions without assuming bounded r
 estricted strong condition number. Numerical results on simulated data con
 firms our theoretical predictions.
LOCATION:Area 5+6+7+8 #68
END:VEVENT
BEGIN:VEVENT
SUMMARY:New Liftable Classes for First-Order Probabilistic Inference | Sey
 ed Mehran Kazemi \, Angelika Kimmig \, Guy Van den Broeck \, David Poole
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:New Liftable Classes for First-Order Probabilistic Infe
 rence\nSeyed Mehran Kazemi \, Angelika Kimmig \, Guy Van den Broeck \, Dav
 id Poole\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7297\n\nStati
 stical relational models provide compact encodings of probabilistic depend
 encies in relational domains\, but result in highly intractable graphical 
 models. The goal of lifted inference is to carry out probabilistic inferen
 ce without needing to reason about each individual separately\, by instead
  treating exchangeable\, undistinguished objects as a whole. In this paper
 \, we study the domain recursion inference rule\, which\, despite its cent
 ral role in early theoretical results on domain-lifted inference\, has lat
 er been believed redundant. We show that this rule is more powerful than e
 xpected\, and in fact significantly extends the range of models for which 
 lifted inference runs in time polynomial in the number of individuals in t
 he domain. This includes an open problem called S4\, the symmetric transit
 ivity model\, and a first-order logic encoding of the birthday paradox. We
  further identify new classes S2FO2 and S2RU of domain-liftable theories\,
  which respectively subsume FO2 and recursively unary theories\, the large
 st classes of domain-liftable theories known so far\, and show that using 
 domain recursion can achieve exponential speedup even in theories that can
 not fully be lifted with the existing set of inference rules.
LOCATION:Area 5+6+7+8 #69
END:VEVENT
BEGIN:VEVENT
SUMMARY:Variational Inference in Mixed Probabilistic Submodular Models | J
 osip Djolonga \, Sebastian Tschiatschek \, Andreas Krause
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Variational Inference in Mixed Probabilistic Submodular
  Models\nJosip Djolonga \, Sebastian Tschiatschek \, Andreas Krause\nhttp:
 //nips.cc/Conferences/2016/Schedule?showEvent=7298\n\nWe consider the prob
 lem of variational inference in probabilistic models with both log-submodu
 lar and log-supermodular higher-order potentials. These models can represe
 nt arbitrary distributions over binary variables\, and thus generalize the
  commonly used pairwise Markov random fields and models with log-supermodu
 lar potentials only\, for which efficient approximate inference algorithms
  are known. While inference in the considered models is #P-hard in general
 \, we present efficient approximate algorithms exploiting recent advances 
 in the field of discrete optimization. We demonstrate the effectiveness of
  our approach in a large set of experiments\, where our model allows reaso
 ning about preferences over sets of items with complements and substitutes
 .
LOCATION:Area 5+6+7+8 #70
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unifying Count-Based Exploration and Intrinsic Motivation | Marc B
 ellemare \, Sriram Srinivasan \, Georg Ostrovski \, Tom Schaul \, David Sa
 xton \, Remi Munos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Unifying Count-Based Exploration and Intrinsic Motivati
 on\nMarc Bellemare \, Sriram Srinivasan \, Georg Ostrovski \, Tom Schaul \
 , David Saxton \, Remi Munos\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7299\n\nWe consider an agent's uncertainty about its environment an
 d the problem of generalizing this uncertainty across states. Specifically
 \, we focus on the problem of exploration in non-tabular reinforcement lea
 rning. Drawing inspiration from the intrinsic motivation literature\, we u
 se density models to measure uncertainty\, and propose a novel algorithm f
 or deriving a pseudo-count from an arbitrary density model. This technique
  enables us to generalize count-based exploration algorithms to the non-ta
 bular case. We apply our ideas to Atari 2600 games\, providing sensible ps
 eudo-counts from raw pixels. We transform these pseudo-counts into explora
 tion bonuses and obtain significantly improved exploration in a number of 
 hard games\, including the infamously difficult Montezuma's Revenge.
LOCATION:Area 5+6+7+8 #71
END:VEVENT
BEGIN:VEVENT
SUMMARY:Approximate maximum entropy principles via Goemans-Williamson with
  applications to provable variational methods | Andrej Risteski \, Yuanzhi
  Li
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Approximate maximum entropy principles via Goemans-Will
 iamson with applications to provable variational methods\nAndrej Risteski 
 \, Yuanzhi Li\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7300\n\n
 The well known maximum-entropy principle due to Jaynes\, which states that
  given mean parameters\, the maximum entropy distribution matching them is
  in an exponential family has been very popular in machine learning due to
  its “Occam’s razor” interpretation. Unfortunately\, calculating the
  potentials in the maximum entropy distribution is intractable [BGS14]. We
  provide computationally efficient versions of this principle when the mea
 n parameters are pairwise moments: we design distributions that approximat
 ely match given pairwise moments\, while having entropy which is comparabl
 e to the maximum entropy distribution matching those moments.  We addition
 ally provide surprising applications of the approximate maximum entropy pr
 inciple to designing provable variational methods for partition function c
 alculations for Ising models without any assumptions on the potentials of 
 the model. More precisely\, we show that we can get approximation guarante
 es for the log-partition function comparable to those in the low-temperatu
 re limit\, which is the setting of optimization of quadratic forms over th
 e hypercube. ([AN06])
LOCATION:Area 5+6+7+8 #72
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Multi-step Inertial Forward-Backward Splitting Method for Non-co
 nvex Optimization | Jingwei Liang \, Jalal Fadili \, Gabriel Peyré
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Multi-step Inertial Forward-Backward Splitting Method
  for Non-convex Optimization\nJingwei Liang \, Jalal Fadili \, Gabriel Pey
 ré\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7301\n\nIn this pa
 per\, we propose a multi-step inertial Forward--Backward splitting algorit
 hm for minimizing the sum of two non-necessarily convex functions\, one of
  which is proper lower semi-continuous while the other is differentiable w
 ith a Lipschitz continuous gradient. We first prove global convergence of 
 the scheme with the help of the Kurdyka–Łojasiewicz property. Then\, wh
 en the non-smooth part is also partly smooth relative to a smooth submanif
 old\, we establish finite identification of the latter and provide sharp l
 ocal linear convergence analysis. The proposed method is illustrated on a 
 few problems arising from statistics and machine learning.
LOCATION:Area 5+6+7+8 #73
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast and Flexible Monotonic Functions with Ensembles of Lattices |
  Mahdi Milani Fard \, Kevin Canini \, Andrew Cotter \, Jan Pfeifer \, Maya
  Gupta
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Fast and Flexible Monotonic Functions with Ensembles of
  Lattices\nMahdi Milani Fard \, Kevin Canini \, Andrew Cotter \, Jan Pfeif
 er \, Maya Gupta\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7302\
 n\nFor many machine learning problems\, there are some inputs that are kno
 wn to be positively (or negatively) related to the output\, and in such ca
 ses training the model to respect that monotonic relationship can provide 
 regularization\, and makes the model more interpretable. However\, flexibl
 e monotonic functions are computationally challenging to learn beyond a fe
 w features. We break through this barrier by learning ensembles of monoton
 ic calibrated interpolated look-up tables (lattices). A key contribution i
 s an automated algorithm for selecting feature subsets for the ensemble ba
 se models.  We demonstrate that compared to random forests\, these ensembl
 es produce similar or better accuracy\, while providing guaranteed monoton
 icity consistent with prior knowledge\, smaller model size and faster eval
 uation.
LOCATION:Area 5+6+7+8 #74
END:VEVENT
BEGIN:VEVENT
SUMMARY:Architectural Complexity Measures of Recurrent Neural Networks | S
 aizheng Zhang \, Yuhuai Wu \, Tong Che \, Zhouhan Lin \, Roland Memisevic 
 \, Ruslan Salakhutdinov \, Yoshua Bengio
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Architectural Complexity Measures of Recurrent Neural N
 etworks\nSaizheng Zhang \, Yuhuai Wu \, Tong Che \, Zhouhan Lin \, Roland 
 Memisevic \, Ruslan Salakhutdinov \, Yoshua Bengio\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7303\n\nIn this paper\, we systematically ana
 lyze the connecting architectures of recurrent neural networks (RNNs). Our
  main contribution is twofold: first\, we present a rigorous graph-theoret
 ic framework describing the connecting architectures of RNNs in general. S
 econd\, we propose three architecture complexity measures of RNNs: (a) the
  recurrent depth\, which captures the RNN’s over-time nonlinear complexi
 ty\, (b) the feedforward depth\, which captures the local input-output non
 linearity (similar to the “depth” in feedforward neural networks (FNNs
 ))\, and (c) the recurrent skip coefficient which captures how rapidly the
  information propagates over time. We rigorously prove each measure’s ex
 istence and computability. Our experimental results show that RNNs might b
 enefit from larger recurrent depth and feedforward depth. We further demon
 strate that increasing recurrent skip coefficient offers performance boost
 s on long term dependency problems.
LOCATION:Area 5+6+7+8 #75
END:VEVENT
BEGIN:VEVENT
SUMMARY:Online Convex Optimization with Unconstrained Domains and Losses |
  Ashok Cutkosky \, Kwabena A Boahen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Online Convex Optimization with Unconstrained Domains a
 nd Losses\nAshok Cutkosky \, Kwabena A Boahen\nhttp://nips.cc/Conferences/
 2016/Schedule?showEvent=7304\n\nWe propose an online convex optimization a
 lgorithm (RescaledExp) that achieves optimal regret in the unconstrained s
 etting without prior knowledge of any bounds on the loss functions. We pro
 ve a lower bound showing an exponential separation between the regret of e
 xisting algorithms that require a known bound on the loss functions and an
 y algorithm that does not require such knowledge. RescaledExp matches this
  lower bound asymptotically in the number of iterations. RescaledExp is na
 turally hyperparameter-free and we demonstrate empirically that it matches
  prior optimization algorithms that require hyperparameter optimization.
LOCATION:Area 5+6+7+8 #76
END:VEVENT
BEGIN:VEVENT
SUMMARY:Split LBI: An Iterative Regularization Path with Structural Sparsi
 ty | Chendi Huang \, Xinwei Sun \, Jiechao Xiong \, Yuan Yao
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Split LBI: An Iterative Regularization Path with Struct
 ural Sparsity\nChendi Huang \, Xinwei Sun \, Jiechao Xiong \, Yuan Yao\nht
 tp://nips.cc/Conferences/2016/Schedule?showEvent=7305\n\nAn iterative regu
 larization path with structural sparsity is proposed in this paper based o
 n variable splitting and the Linearized Bregman Iteration\, hence called \
 \emph{Split LBI}. Despite its simplicity\, Split LBI outperforms the popul
 ar generalized Lasso in both theory and experiments. A theory of path cons
 istency is presented that equipped with a proper early stopping\, Split LB
 I may achieve model selection consistency under a family of Irrepresentabl
 e Conditions which can be weaker than the necessary and sufficient conditi
 on for generalized Lasso. Furthermore\, some $\\ell_2$ error bounds are al
 so given at the minimax optimal rates. The utility and benefit of the algo
 rithm are illustrated by applications on both traditional image denoising 
 and a novel example on partial order ranking.
LOCATION:Area 5+6+7+8 #77
END:VEVENT
BEGIN:VEVENT
SUMMARY:Variational Autoencoder for Deep Learning of Images\, Labels and C
 aptions | Yunchen Pu \, Zhe Gan \, Ricardo Henao \, Xin Yuan \, Chunyuan L
 i \, Andrew Stevens \, Lawrence Carin
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Variational Autoencoder for Deep Learning of Images\, L
 abels and Captions\nYunchen Pu \, Zhe Gan \, Ricardo Henao \, Xin Yuan \, 
 Chunyuan Li \, Andrew Stevens \, Lawrence Carin\nhttp://nips.cc/Conference
 s/2016/Schedule?showEvent=7306\n\nA novel variational autoencoder is devel
 oped to model images\, as well as associated labels or captions. The Deep 
 Generative Deconvolutional Network (DGDN) is used as a decoder of the late
 nt image features\, and a deep Convolutional Neural Network (CNN) is used 
 as an image encoder\; the CNN is used to approximate a distribution for th
 e latent DGDN features/code. The latent code is also linked to generative 
 models for labels (Bayesian support vector machine) or captions (recurrent
  neural network). When predicting a label/caption for a new image at test\
 , averaging is performed across the distribution of latent codes\; this is
  computationally efficient as a consequence of the learned CNN-based encod
 er. Since the framework is capable of modeling the image in the presence/a
 bsence of associated labels/captions\, a new semi-supervised setting is ma
 nifested for CNN learning with images\; the framework even allows unsuperv
 ised CNN learning\, based on images alone.
LOCATION:Area 5+6+7+8 #78
END:VEVENT
BEGIN:VEVENT
SUMMARY:Recovery Guarantee of Non-negative Matrix Factorization  via Alter
 nating Updates | Yuanzhi Li \, Yingyu Liang \, Andrej Risteski
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Recovery Guarantee of Non-negative Matrix Factorization
   via Alternating Updates\nYuanzhi Li \, Yingyu Liang \, Andrej Risteski\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7307\n\nNon-negative ma
 trix factorization is a popular tool for  decomposing data into feature an
 d weight matrices under non-negativity constraints. It enjoys practical su
 ccess but is poorly understood theoretically. This paper proposes an algor
 ithm that alternates between decoding the weights and updating the feature
 s\, and shows that assuming a generative model of the data\, it provably r
 ecovers the ground-truth under fairly mild conditions. In particular\, its
  only essential requirement on features is linear independence. Furthermor
 e\, the algorithm uses ReLU to exploit the non-negativity for decoding the
  weights\, and thus can tolerate adversarial noise that can potentially be
  as large as the signal\, and can tolerate unbiased noise much larger than
  the signal. The analysis relies on a carefully designed coupling between 
 two potential functions\, which we believe is of independent interest.
LOCATION:Area 5+6+7+8 #79
END:VEVENT
BEGIN:VEVENT
SUMMARY:Proximal Deep Structured Models | Shenlong Wang \, Sanja Fidler \,
  Raquel Urtasun
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Proximal Deep Structured Models\nShenlong Wang \, Sanja
  Fidler \, Raquel Urtasun\nhttp://nips.cc/Conferences/2016/Schedule?showEv
 ent=7308\n\nMany problems in real-world applications involve predicting co
 ntinuous-valued random variables that are statistically related.  In this 
 paper\, we propose a powerful deep structured model that is able to learn 
 complex non-linear functions which encode the   dependencies between conti
 nuous output variables.  We show that  inference in our  model using proxi
 mal methods can be efficiently solved as a feed-foward pass of a special  
 type of  deep recurrent neural network. We demonstrate the  effectiveness 
 of our approach in the tasks of image denoising\, depth refinement and opt
 ical flow estimation.
LOCATION:Area 5+6+7+8 #80
END:VEVENT
BEGIN:VEVENT
SUMMARY:Safe Policy Improvement by Minimizing Robust Baseline Regret | Moh
 ammad Ghavamzadeh \, Marek Petrik \, Yinlam Chow
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Safe Policy Improvement by Minimizing Robust Baseline R
 egret\nMohammad Ghavamzadeh \, Marek Petrik \, Yinlam Chow\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7309\n\nAn important problem in seque
 ntial decision-making under uncertainty is to use limited data to  compute
  a safe policy\, i.e.\, a policy that is guaranteed to perform at least as
  well as a given baseline strategy. In this paper\, we develop and analyze
  a new model-based approach to compute a safe policy when we have access t
 o an inaccurate dynamics model of the system with known accuracy guarantee
 s. Our proposed robust method uses this (inaccurate) model to directly min
 imize the (negative) regret w.r.t. the baseline policy. Contrary to the ex
 isting approaches\, minimizing the regret allows one to improve the baseli
 ne policy in states with accurate dynamics and seamlessly fall back to the
  baseline policy\, otherwise. We show that our formulation is NP-hard and 
 propose an approximate algorithm. Our empirical results on several domains
  show that even this relatively simple approximate algorithm can significa
 ntly outperform standard approaches.
LOCATION:Area 5+6+7+8 #81
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Pseudo-Bayesian Algorithm for Robust PCA | Tae-Hyun Oh \, Yasuyu
 ki Matsushita \, In Kweon \, David Wipf
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Pseudo-Bayesian Algorithm for Robust PCA\nTae-Hyun Oh
  \, Yasuyuki Matsushita \, In Kweon \, David Wipf\nhttp://nips.cc/Conferen
 ces/2016/Schedule?showEvent=7310\n\nCommonly used in many applications\, r
 obust PCA represents an algorithmic attempt to reduce the sensitivity of c
 lassical PCA to outliers.  The basic idea is to learn a decomposition of s
 ome data matrix of interest into low rank and sparse components\, the latt
 er representing unwanted outliers.  Although the resulting problem is typi
 cally NP-hard\, convex relaxations provide a computationally-expedient alt
 ernative with theoretical support.  However\, in practical regimes perform
 ance guarantees break down and a variety of non-convex alternatives\, incl
 uding Bayesian-inspired models\, have been proposed to boost estimation qu
 ality.  Unfortunately though\, without additional a priori knowledge none 
 of these methods can significantly expand the critical operational range s
 uch that exact principal subspace recovery is possible.  Into this mix we 
 propose a novel pseudo-Bayesian algorithm that explicitly compensates for 
 design weaknesses in many existing non-convex approaches leading to state-
 of-the-art performance with a sound analytical foundation.
LOCATION:Area 5+6+7+8 #82
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning values across many orders of magnitude | Hado van Hasselt
  \, Arthur Guez \, Arthur Guez \, Matteo Hessel \, Volodymyr Mnih \, David
  Silver
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning values across many orders of magnitude\nHado v
 an Hasselt \, Arthur Guez \, Arthur Guez \, Matteo Hessel \, Volodymyr Mni
 h \, David Silver\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7311
 \n\nMost learning algorithms are not invariant to the scale of the signal 
 that is being approximated. We propose to adaptively normalize the targets
  used in the learning updates.  This is important in value-based reinforce
 ment learning\, where the magnitude of appropriate value approximations ca
 n change over time when we update the policy of behavior. Our main motivat
 ion is prior work on learning to play Atari games\, where the rewards were
  clipped to a predetermined range. This clipping facilitates learning acro
 ss many different games with a single learning algorithm\, but a clipped r
 eward function can result in qualitatively different behavior. Using adapt
 ive normalization we can remove this domain-specific heuristic without dim
 inishing overall performance.
LOCATION:Area 5+6+7+8 #83
END:VEVENT
BEGIN:VEVENT
SUMMARY:Single Pass PCA of Matrix Products | Shanshan Wu \, Srinadh Bhojan
 apalli \, Sujay Sanghavi \, Alexandros Dimakis
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Single Pass PCA of Matrix Products\nShanshan Wu \, Srin
 adh Bhojanapalli \, Sujay Sanghavi \, Alexandros Dimakis\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7312\n\nIn this paper we present a new 
 algorithm for computing a low rank approximation of the product $A^TB$ by 
 taking only a single pass of the two matrices $A$ and $B$. The straightfor
 ward way to do this is to (a) first sketch $A$ and $B$ individually\, and 
 then (b) find the top components using PCA on the sketch. Our algorithm in
  contrast retains additional summary information about $A\,B$ (e.g. row an
 d column norms etc.) and uses this additional information to obtain an imp
 roved approximation from the sketches. Our main analytical result establis
 hes a comparable spectral norm guarantee to existing two-pass methods\; in
  addition we also provide results from an Apache Spark implementation that
  shows better computational and statistical performance on real-world and 
 synthetic evaluation datasets.
LOCATION:Area 5+6+7+8 #84
END:VEVENT
BEGIN:VEVENT
SUMMARY:Convolutional Neural Fabrics | Shreyas Saxena \, Jakob Verbeek
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Convolutional Neural Fabrics\nShreyas Saxena \, Jakob V
 erbeek\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7313\n\nDespite
  the success of CNNs\, selecting the optimal architecture for a given task
  remains an open problem. Instead of aiming to select a single optimal arc
 hitecture\, we propose a ``fabric'' that embeds an exponentially large num
 ber of architectures. The fabric consists of a 3D trellis that connects re
 sponse maps at different layers\, scales\, and channels  with a sparse hom
 ogeneous local connectivity pattern. The only hyper-parameters of a fabric
  are the number of channels and layers. While individual architectures can
  be recovered as paths\, the fabric can in addition ensemble all embedded 
 architectures together\, sharing their weights where their  paths overlap.
  Parameters can be learned using standard methods based on back-propagatio
 n\, at a cost that scales linearly in the fabric size. We present benchmar
 k results competitive with the state of the art for image classification o
 n MNIST and CIFAR10\, and for semantic segmentation on the Part Labels dat
 aset.
LOCATION:Area 5+6+7+8 #85
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generative Shape Models: Joint Text Recognition and Segmentation w
 ith Very Little Training Data | Xinghua Lou \, Ken Kansky \, Wolfgang Lehr
 ach \, CC Laan \, Bhaskara Marthi \, D. Phoenix \, Dileep George
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Generative Shape Models: Joint Text Recognition and Seg
 mentation with Very Little Training Data\nXinghua Lou \, Ken Kansky \, Wol
 fgang Lehrach \, CC Laan \, Bhaskara Marthi \, D. Phoenix \, Dileep George
 \nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7314\n\nWe demonstrat
 e that a generative model for object shapes can achieve state of the art r
 esults on challenging scene text recognition tasks\, and with orders of ma
 gnitude fewer training images than required for competing discriminative m
 ethods. In addition to transcribing text from challenging images\, our met
 hod performs fine-grained instance segmentation of characters. We show tha
 t our model is more robust to both affine transformations and non-affine d
 eformations compared to previous approaches.
LOCATION:Area 5+6+7+8 #86
END:VEVENT
BEGIN:VEVENT
SUMMARY:Mixed vine copulas as joint models of spike counts and local field
  potentials | Arno Onken \, Stefano Panzeri
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Mixed vine copulas as joint models of spike counts and 
 local field potentials\nArno Onken \, Stefano Panzeri\nhttp://nips.cc/Conf
 erences/2016/Schedule?showEvent=7315\n\nConcurrent measurements of neural 
 activity at multiple scales\, sometimes performed with multimodal techniqu
 es\, become increasingly important for studying brain function. However\, 
 statistical methods for their concurrent analysis are currently lacking. H
 ere we introduce such techniques in a framework based on vine copulas with
  mixed margins to construct multivariate stochastic models. These models c
 an describe detailed mixed interactions between discrete variables such as
  neural spike counts\, and continuous variables such as local field potent
 ials. We propose efficient methods for likelihood calculation\, inference\
 , sampling and mutual information estimation within this framework. We tes
 t our methods on simulated data and demonstrate applicability on mixed dat
 a generated by a biologically realistic neural network. Our methods hold t
 he promise to considerably improve statistical analysis of neural data rec
 orded simultaneously at different scales.
LOCATION:Area 5+6+7+8 #87
END:VEVENT
BEGIN:VEVENT
SUMMARY:Optimal Black-Box Reductions Between Optimization Objectives | Zey
 uan Allen-Zhu \, Elad Hazan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Optimal Black-Box Reductions Between Optimization Objec
 tives\nZeyuan Allen-Zhu \, Elad Hazan\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=7316\n\nThe diverse world of machine learning applications
  has given rise to a plethora of algorithms and optimization methods\, fin
 ely tuned to the specific regression or classification task at hand.  We r
 educe the complexity of algorithm design for machine learning by reduction
 s:  we develop reductions that take a method developed for one setting and
  apply it to the entire spectrum of smoothness and strong-convexity in app
 lications.  Furthermore\, unlike existing results\, our new reductions are
  OPTIMAL and more PRACTICAL. We show how these new reductions give rise to
  new and faster running times on training linear classifiers for various f
 amilies of loss functions\, and conclude with experiments showing their su
 ccesses also in practice.
LOCATION:Area 5+6+7+8 #88
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dialog-based Language Learning | Jason E Weston
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Dialog-based Language Learning\nJason E Weston\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=7317\n\nA long-term goal of ma
 chine learning research is to build an intelligent dialog agent. Most rese
 arch in natural language understanding has focused on learning from fixed 
 training sets of labeled data\, with supervision either at the word level 
 (tagging\, parsing tasks) or sentence level (question answering\, machine 
 translation). This kind of supervision is not realistic of how humans lear
 n\, where language is both learned by\, and used for\, communication. In t
 his work\, we study dialog-based language learning\, where supervision is 
 given naturally and implicitly in the response of the dialog partner durin
 g the conversation. We study this setup in two domains: the bAbI dataset o
 f (Weston et al.\, 2015) and large-scale question answering from (Dodge et
  al.\, 2015). We evaluate a set of baseline learning strategies on these t
 asks\, and show that a novel model incorporating predictive lookahead is a
  promising approach for learning from a teacher's response. In particular\
 , a surprising result is that it can learn to answer questions correctly w
 ithout any reward-based supervision at all.
LOCATION:Area 5+6+7+8 #89
END:VEVENT
BEGIN:VEVENT
SUMMARY:Online Bayesian Moment Matching for Topic Modeling with Unknown Nu
 mber of Topics | Wei-Shou Hsu \, Pascal Poupart
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Online Bayesian Moment Matching for Topic Modeling with
  Unknown Number of Topics\nWei-Shou Hsu \, Pascal Poupart\nhttp://nips.cc/
 Conferences/2016/Schedule?showEvent=7318\n\nLatent Dirichlet Allocation (L
 DA) is a very popular model for topic modeling as well as many other probl
 ems with latent groups.  It is both simple and effective.  When the number
  of topics (or latent groups) is unknown\, the Hierarchical Dirichlet Proc
 ess (HDP) provides an elegant non-parametric extension\; however\, it is a
  complex model and it is difficult to incorporate prior knowledge since th
 e distribution over topics is implicit.  We propose two new models that ex
 tend LDA in a simple and intuitive fashion by directly expressing a distri
 bution over the number of topics.  We also propose a new online Bayesian m
 oment matching technique to learn the parameters and the number of topics 
 of those models based on streaming data.  The approach achieves higher log
 -likelihood than batch and online HDP with fixed hyperparameters on severa
 l corpora.
LOCATION:Area 5+6+7+8 #90
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Sparse Interactive Model for Matrix Completion with Side Informa
 tion | Jin Lu \, Guannan Liang \, Jiangwen Sun \, Jinbo Bi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Sparse Interactive Model for Matrix Completion with S
 ide Information\nJin Lu \, Guannan Liang \, Jiangwen Sun \, Jinbo Bi\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=7319\n\nMatrix completion m
 ethods can benefit from side information besides the partially observed ma
 trix. The use of side features describing the row and column entities of a
  matrix has been shown to reduce the sample complexity for completing the 
 matrix. We propose a novel sparse formulation that explicitly models the i
 nteraction between the row and column side features to approximate the mat
 rix entries. Unlike early methods\, this model does not require the low-ra
 nk condition on the model parameter matrix. We prove that when the side fe
 atures can span the latent feature space of the matrix to be recovered\, t
 he number of observed entries needed for an exact recovery is $O(\\log N)$
  where $N$ is the size of the matrix. When the side features are corrupted
  latent features of the matrix with a small perturbation\, our method can 
 achieve an $\\epsilon$-recovery with $O(\\log N)$ sample complexity\, and 
 maintains a $\\O(N^{3/2})$ rate similar to classfic methods with no side i
 nformation. An efficient linearized Lagrangian algorithm is developed with
  a strong guarantee of convergence. Empirical results show that our approa
 ch outperforms three state-of-the-art methods both in simulations and on r
 eal world datasets.
LOCATION:Area 5+6+7+8 #91
END:VEVENT
BEGIN:VEVENT
SUMMARY:Truncated Variance Reduction: A Unified Approach to Bayesian Optim
 ization and Level-Set Estimation | Ilija Bogunovic \, Jonathan Scarlett \,
  Andreas Krause \, Volkan Cevher
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Truncated Variance Reduction: A Unified Approach to Bay
 esian Optimization and Level-Set Estimation\nIlija Bogunovic \, Jonathan S
 carlett \, Andreas Krause \, Volkan Cevher\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7320\n\nWe present a new algorithm\, truncated varian
 ce reduction (TruVaR)\, that treats Bayesian optimization (BO) and level-s
 et estimation (LSE) with Gaussian processes in a unified fashion. The algo
 rithm greedily shrinks a sum of truncated variances within a set of potent
 ial maximizers (BO) or unclassified points (LSE)\, which is updated based 
 on confidence bounds.  TruVaR is effective in several important settings t
 hat are typically non-trivial to incorporate into myopic algorithms\, incl
 uding pointwise costs and heteroscedastic noise.  We provide a general the
 oretical guarantee for TruVaR covering these aspects\, and use it to recov
 er and strengthen existing results on BO and LSE.  Moreover\, we provide a
  new result for a setting where one can select from a number of noise leve
 ls having associated costs.  We demonstrate the effectiveness of the algor
 ithm on both synthetic and real-world data sets.
LOCATION:Area 5+6+7+8 #92
END:VEVENT
BEGIN:VEVENT
SUMMARY:On Mixtures of Markov Chains | Rishi Gupta \, Ravi Kumar \, Sergei
  Vassilvitskii
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:On Mixtures of Markov Chains\nRishi Gupta \, Ravi Kumar
  \, Sergei Vassilvitskii\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7321\n\nWe study the problem of reconstructing a mixture of Markov chai
 ns from the trajectories generated by random walks through the state space
 .  Under mild non-degeneracy conditions\, we show that we can uniquely rec
 onstruct the underlying chains by only considering trajectories of length 
 three\, which represent triples of states. Our algorithm is spectral in na
 ture\, and is easy to implement.
LOCATION:Area 5+6+7+8 #93
END:VEVENT
BEGIN:VEVENT
SUMMARY:High Dimensional Structured Superposition Models | Qilong Gu \, Ar
 indam Banerjee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:High Dimensional Structured Superposition Models\nQilon
 g Gu \, Arindam Banerjee\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7322\n\nHigh dimensional superposition models characterize observations
  using parameters which can be written as a sum of multiple component para
 meters\, each with its own structure\, e.g.\, sum of low rank and sparse m
 atrices. In this paper\, we consider general superposition models which al
 low sum of any number of component parameters\, and each component structu
 re can be characterized by any norm. We present a simple estimator for suc
 h models\, give a geometric condition under which the components can be ac
 curately estimated\, characterize sample complexity of the estimator\, and
  give non-asymptotic bounds on the componentwise estimation error. We use 
 tools from empirical processes and generic chaining for the statistical an
 alysis\, and our results\, which substantially generalize prior work on su
 perposition models\, are in terms of Gaussian widths of suitable spherical
  caps.
LOCATION:Area 5+6+7+8 #94
END:VEVENT
BEGIN:VEVENT
SUMMARY:Finite Sample Prediction and Recovery Bounds for Ordinal Embedding
  | Lalit Jain \, Kevin Jamieson \, Rob Nowak
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Finite Sample Prediction and Recovery Bounds for Ordina
 l Embedding\nLalit Jain \, Kevin Jamieson \, Rob Nowak\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7323\n\nThe goal of ordinal embedding is 
 to represent items as points in a low-dimensional Euclidean space given a 
 set of constraints like ``item $i$ is closer to item $j$ than item $k$''. 
  Ordinal   constraints like this often come from human judgments.  The cla
 ssic approach to solving this problem is known as non-metric multidimensio
 nal scaling.  To account for errors and variation in judgments\, we consid
 er the noisy situation in which the given constraints are independently co
 rrupted by reversing the correct constraint with some probability. The ord
 inal embedding problem has been studied for decades\, but most past work p
 ays little attention to the question of whether accurate embedding is poss
 ible\, apart from empirical studies.  This paper shows that under a genera
 tive data model it is possible to learn the correct embedding from noisy d
 istance comparisons.  In establishing this fundamental result\, the paper 
 makes several new contributions. First\, we derive prediction error bounds
  for embedding from noisy distance comparisons by exploiting the fact that
  the rank of a distance matrix of points in $\\R^d$ is at most $d+2$. Thes
 e bounds characterize how well a learned embedding predicts new comparativ
 e judgments. Second\, we show that the underlying embedding can be recover
 ed by solving a simple convex optimization.  This result is highly non-tri
 vial since we show that the linear map corresponding to distance compariso
 ns is non-invertible\, but there exists a nonlinear map that is invertible
 . Third\, two new algorithms for ordinal embedding are proposed and evalua
 ted in experiments.
LOCATION:Area 5+6+7+8 #95
END:VEVENT
BEGIN:VEVENT
SUMMARY:What Makes Objects Similar: A Unified Multi-Metric Learning Approa
 ch | Han-Jia Ye \, De-Chuan Zhan \, Xue-Min Si \, Yuan Jiang \, Zhi-Hua Zh
 ou
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:What Makes Objects Similar: A Unified Multi-Metric Lear
 ning Approach\nHan-Jia Ye \, De-Chuan Zhan \, Xue-Min Si \, Yuan Jiang \, 
 Zhi-Hua Zhou\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7324\n\nL
 inkages are essentially determined by similarity measures that may be deri
 ved from multiple perspectives. For example\, spatial linkages are usually
  generated based on localities of heterogeneous data\, whereas semantic li
 nkages can come from various properties\, such as different physical meani
 ngs behind social relations. Many existing metric learning models focus on
  spatial linkages\, but leave the rich semantic factors unconsidered. Simi
 larities based on these models are usually overdetermined on linkages. We 
 propose a Unified Multi-Metric Learning (UM2L) framework to exploit multip
 le types of metrics. In UM2L\, a type of combination operator is introduce
 d for distance characterization from multiple perspectives\, and thus can 
 introduce flexibilities for representing and utilizing both spatial and se
 mantic linkages. Besides\, we propose a uniform solver for UM2L which is g
 uaranteed to converge. Extensive experiments on diverse applications exhib
 it the superior classification performance and comprehensibility of UM2L. 
 Visualization results also validate its ability on physical meanings disco
 very.
LOCATION:Area 5+6+7+8 #96
END:VEVENT
BEGIN:VEVENT
SUMMARY:Unsupervised Learning of Spoken Language with Visual Context | Dav
 id Harwath \, Antonio Torralba \, James Glass
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Unsupervised Learning of Spoken Language with Visual Co
 ntext\nDavid Harwath \, Antonio Torralba \, James Glass\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7325\n\nHumans learn to speak before the
 y can read or write\, so why can’t computers do the same? In this paper\
 , we present a deep neural network model capable of rudimentary spoken lan
 guage acquisition using untranscribed audio training data\, whose only sup
 ervision comes in the form of contextually relevant visual images. We desc
 ribe the collection of our data comprised of over 120\,000 spoken audio ca
 ptions for the Places image dataset and evaluate our model on an image sea
 rch and annotation task. We also provide some visualizations which suggest
  that our model is learning to recognize meaningful words within the capti
 on spectrograms.
LOCATION:Area 5+6+7+8 #97
END:VEVENT
BEGIN:VEVENT
SUMMARY:Cyclades: Conflict-free Asynchronous Machine Learning | Xinghao Pa
 n \, Maximilian Lam \, Stephen Tu \, Dimitris Papailiopoulos \, Ce Zhang \
 , Michael I Jordan \, Kannan Ramchandran \, Christopher Ré \, Benjamin Re
 cht
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Cyclades: Conflict-free Asynchronous Machine Learning\n
 Xinghao Pan \, Maximilian Lam \, Stephen Tu \, Dimitris Papailiopoulos \, 
 Ce Zhang \, Michael I Jordan \, Kannan Ramchandran \, Christopher Ré \, B
 enjamin Recht\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7326\n\n
 We present Cyclades\, a general framework for parallelizing stochastic opt
 imization algorithms in a shared memory setting. Cyclades is asynchronous 
 during model updates\, and requires no memory locking mechanisms\, similar
  to Hogwild!-type algorithms. Unlike Hogwild!\, Cyclades introduces no con
 flicts during parallel execution\, and offers a black-box analysis for pro
 vable speedups across a large family of algorithms.  Due to its inherent c
 ache locality and conflict-free nature\,  our multi-core implementation of
  Cyclades consistently outperforms Hogwild!-type algorithms on sufficientl
 y sparse datasets\, leading to up to 40% speedup gains compared to Hogwild
 !\, and up to 5\\times gains over asynchronous implementations of variance
  reduction algorithms.
LOCATION:Area 5+6+7+8 #98
END:VEVENT
BEGIN:VEVENT
SUMMARY:Disease Trajectory Maps | Peter Schulam \, Raman Arora
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Disease Trajectory Maps\nPeter Schulam \, Raman Arora\n
 http://nips.cc/Conferences/2016/Schedule?showEvent=7327\n\nMedical researc
 hers are coming to appreciate that many diseases are in fact complex\, het
 erogeneous syndromes composed of subpopulations that express different var
 iants of a related complication. Longitudinal data extracted from individu
 al electronic health records (EHR) offer an exciting new way to study subt
 le differences in the way these diseases progress over time. In this paper
 \, we focus on answering two questions that can be asked using these datab
 ases of longitudinal EHR data. First\, we want to understand whether there
  are individuals with similar disease trajectories and whether there are a
  small number of degrees of freedom that account for differences in trajec
 tories across the population. Second\, we want to understand how important
  clinical outcomes are associated with disease trajectories. To answer the
 se questions\, we propose the Disease Trajectory Map (DTM)\, a novel proba
 bilistic model that learns low-dimensional representations of sparse and i
 rregularly sampled longitudinal data. We propose a stochastic variational 
 inference algorithm for learning the DTM that allows the model to scale to
  large modern medical datasets. To demonstrate the DTM\, we analyze data c
 ollected on patients with the complex autoimmune disease\, scleroderma. We
  find that DTM learns meaningful representations of disease trajectories a
 nd that the representations are significantly associated with important cl
 inical outcomes.
LOCATION:Area 5+6+7+8 #99
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast ε-free Inference of Simulation Models with Bayesian Conditio
 nal Density Estimation | George Papamakarios \, Iain Murray
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Fast ε-free Inference of Simulation Models with Bayesi
 an Conditional Density Estimation\nGeorge Papamakarios \, Iain Murray\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7328\n\nMany statistical m
 odels can be simulated forwards but have intractable likelihoods. Approxim
 ate Bayesian Computation (ABC) methods are used to infer properties of the
 se models from data. Traditionally these methods approximate the posterior
  over parameters by conditioning on data being inside an ε-ball around th
 e observed data\, which is only correct in the limit ε→0. Monte Carlo m
 ethods can then draw samples from the approximate posterior to approximate
  predictions or error bars on parameters. These algorithms critically slow
  down as ε→0\, and in practice draw samples from a broader distribution
  than the posterior. We propose a new approach to likelihood-free inferenc
 e based on Bayesian conditional density estimation. Preliminary inferences
  based on limited simulation data are used to guide later simulations. In 
 some cases\, learning an accurate parametric representation of the entire 
 true posterior distribution requires fewer model simulations than Monte Ca
 rlo ABC methods need to produce a single sample from an approximate poster
 ior.
LOCATION:Area 5+6+7+8 #100
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Structured Prediction under Bandit Feedback | Artem Sok
 olov \, Julia Kreutzer \, Stefan Riezler \, Christopher Lo
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Stochastic Structured Prediction under Bandit Feedback\
 nArtem Sokolov \, Julia Kreutzer \, Stefan Riezler \, Christopher Lo\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=7329\n\nStochastic structur
 ed prediction under bandit feedback follows a learning protocol where on e
 ach of a sequence of iterations\, the learner receives an input\, predicts
  an output structure\, and receives partial feedback in form of a task los
 s evaluation of the predicted structure. We present applications of this l
 earning scenario to convex and non-convex objectives for structured predic
 tion and analyze them as stochastic first-order methods. We present an exp
 erimental evaluation on problems of natural language processing over expon
 ential output spaces\, and compare convergence speed across different obje
 ctives under the practical criterion of optimal task performance on develo
 pment data and the optimization-theoretic criterion of minimal squared gra
 dient norm. Best results under both criteria are obtained for a non-convex
  objective for pairwise preference learning under bandit feedback.
LOCATION:Area 5+6+7+8 #101
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning under uncertainty: a comparison between R-W and Bayesian 
 approach | He Huang \, Martin Paulus
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning under uncertainty: a comparison between R-W an
 d Bayesian approach\nHe Huang \, Martin Paulus\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=7330\n\nAccurately differentiating between what a
 re truly unpredictably random and systematic changes that occur at random 
 can have profound effect on affect and cognition. To examine the underlyin
 g computational principles that guide different learning behavior in an un
 certain environment\, we compared an R-W model and a Bayesian approach in 
 a visual search task with different volatility levels. Both R-W model and 
 the Bayesian approach reflected an individual's estimation of the environm
 ental volatility\, and there is a strong correlation between the learning 
 rate in R-W model and the belief of stationarity in the Bayesian approach 
 in different volatility conditions. In a low volatility condition\, R-W mo
 del indicates that learning rate positively correlates with lose-shift rat
 e\, but not choice optimality (inverted U shape). The Bayesian approach in
 dicates that the belief of environmental stationarity positively correlate
 s with choice optimality\, but not lose-shift rate (inverted U shape). In 
 addition\, we showed that comparing to Expert learners\, individuals with 
 high lose-shift rate (sub-optimal learners) had significantly higher learn
 ing rate estimated from R-W model and lower belief of stationarity from th
 e Bayesian model.
LOCATION:Area 5+6+7+8 #102
END:VEVENT
BEGIN:VEVENT
SUMMARY:Minimax Optimal Alternating Minimization for Kernel Nonparametric 
 Tensor Learning | Taiji Suzuki \, Heishiro Kanagawa \, Hayato Kobayashi \,
  Nobuyuki Shimizu \, Yukihiro Tagami
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Minimax Optimal Alternating Minimization for Kernel Non
 parametric Tensor Learning\nTaiji Suzuki \, Heishiro Kanagawa \, Hayato Ko
 bayashi \, Nobuyuki Shimizu \, Yukihiro Tagami\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=7331\n\nWe investigate the statistical performanc
 e and computational efficiency of the  alternating minimization procedure 
 for nonparametric tensor learning. Tensor modeling has been widely used fo
 r capturing the higher order relations between  multimodal data sources. I
 n addition to a linear model\,  a nonlinear tensor model has been received
  much attention recently because of its high flexibility. We consider  an 
 alternating minimization procedure for  a general nonlinear model where th
 e true function  consists of components in a reproducing kernel Hilbert sp
 ace (RKHS). In this paper\, we show that the alternating minimization meth
 od achieves linear convergence as an optimization algorithm  and that the 
 generalization error of the resultant estimator yields the minimax optimal
 ity. We apply our algorithm to some multitask learning problems and show t
 hat the method actually shows favorable performances.
LOCATION:Area 5+6+7+8 #103
END:VEVENT
BEGIN:VEVENT
SUMMARY:On the Recursive Teaching Dimension of VC Classes | Xi Chen \, Xi 
 Chen \, Yu Cheng \, Bo Tang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:On the Recursive Teaching Dimension of VC Classes\nXi C
 hen \, Xi Chen \, Yu Cheng \, Bo Tang\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=7332\n\nThe recursive teaching dimension (RTD) of a concep
 t class $C \\subseteq {0\, 1}^n$\, introduced by Zilles et al. [ZLHZ11]\, 
 is a complexity parameter measured by the worst-case number of labeled exa
 mples needed to learn any target concept of $C$ in the recursive teaching 
 model. In this paper\, we study the quantitative relation between RTD and 
 the well-known learning complexity measure VC dimension (VCD)\, and improv
 e the best known upper and (worst-case) lower bounds on the recursive teac
 hing dimension with respect to the VC dimension.  Given a concept class $C
  \\subseteq {0\, 1}^n$ with $VCD(C) = d$\, we first show that $RTD(C)$ is 
 at most $d 2^{d+1}$. This is the first upper bound for $RTD(C)$ that depen
 ds only on $VCD(C)$\, independent of the size of the concept class $|C|$ a
 nd its~domain size $n$. Before our work\, the best known upper bound for $
 RTD(C)$ is $O(d 2^d \\log \\log |C|)$\, obtained by Moran et al. [MSWY15].
  We remove the $\\log \\log |C|$ factor.  We also improve the lower bound 
 on the worst-case ratio of $RTD(C)$ to $VCD(C)$. We present a family of cl
 asses ${ Ck }{k \\ge 1}$ with $VCD(Ck) = 3k$ and $RTD(Ck)=5k$\, which impl
 ies that the ratio of $RTD(C)$ to $VCD(C)$ in the worst case can be as lar
 ge as $5/3$. Before our work\, the largest ratio known was $3/2$ as obtain
 ed by Kuhlmann [Kuh99]. Since then\, no finite concept class $C$ has been 
 known to satisfy $RTD(C) &gt\; (3/2) VCD(C)$.
LOCATION:Area 5+6+7+8 #104
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dimension-Free Iteration Complexity of Finite Sum Optimization Pro
 blems | Yossi Arjevani \, Ohad Shamir
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Dimension-Free Iteration Complexity of Finite Sum Optim
 ization Problems\nYossi Arjevani \, Ohad Shamir\nhttp://nips.cc/Conference
 s/2016/Schedule?showEvent=7333\n\nMany canonical machine learning problems
  boil down to a convex optimization problem with a finite sum structure. H
 owever\, whereas much progress has been made in developing faster algorith
 ms for this setting\, the inherent limitations of these problems are not s
 atisfactorily addressed by existing lower bounds. Indeed\, current bounds 
 focus on first-order optimization algorithms\, and only apply in the often
  unrealistic regime where the number of iterations is less than $\\cO(d/n)
 $ (where $d$ is the dimension and $n$ is the number of samples). In this w
 ork\, we extend the framework of Arjevani et al. \\cite{arjevani2015lower\
 ,arjevani2016iteration} to provide new lower bounds\, which are dimension-
 free\, and go beyond the assumptions of current bounds\, thereby covering 
 standard finite sum optimization methods\, e.g.\, SAG\, SAGA\, SVRG\, SDCA
  without duality\, as well as stochastic coordinate-descent methods\, such
  as SDCA and accelerated proximal SDCA.
LOCATION:Area 5+6+7+8 #105
END:VEVENT
BEGIN:VEVENT
SUMMARY:f-GAN: Training Generative Neural Samplers using Variational Diver
 gence Minimization | Sebastian Nowozin \, Botond Cseke \, Ryota Tomioka
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:f-GAN: Training Generative Neural Samplers using Variat
 ional Divergence Minimization\nSebastian Nowozin \, Botond Cseke \, Ryota 
 Tomioka\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7334\n\nGenera
 tive neural networks are probabilistic models that implement sampling usin
 g feedforward neural networks: they take a random input vector and produce
  a sample from a probability distribution defined by the network weights. 
 These models are expressive and allow efficient computation of samples and
  derivatives\, but cannot be used for computing likelihoods or for margina
 lization. The generative-adversarial training method allows to train such 
 models through the use of an auxiliary discriminative neural network. We s
 how that the generative-adversarial approach is a special case of an exist
 ing more general variational divergence estimation approach. We show that 
 any $f$-divergence can be used for training generative neural networks. We
  discuss the benefits of various choices of divergence functions on traini
 ng complexity and the quality of the obtained generative models.
LOCATION:Area 5+6+7+8 #106
END:VEVENT
BEGIN:VEVENT
SUMMARY:Low-Rank Regression with Tensor Responses | Guillaume Rabusseau \,
  Hachem Kadri
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Low-Rank Regression with Tensor Responses\nGuillaume Ra
 busseau \, Hachem Kadri\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7335\n\nThis paper proposes an efficient algorithm (HOLRR) to handle reg
 ression tasks where the outputs have a tensor structure. We formulate the 
 regression problem as the minimization  of a least square criterion under 
 a multilinear rank constraint\, a difficult  non convex problem.  HOLRR co
 mputes efficiently an approximate solution of this problem\, with solid th
 eoretical guarantees. A kernel extension is also presented. Experiments on
  synthetic and real data show that HOLRR computes accurate solutions while
  being computationally very competitive.
LOCATION:Area 5+6+7+8 #107
END:VEVENT
BEGIN:VEVENT
SUMMARY:Double Thompson Sampling for Dueling Bandits | Huasen Wu \, Xin Li
 u
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Double Thompson Sampling for Dueling Bandits\nHuasen Wu
  \, Xin Liu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7336\n\nIn
  this paper\, we propose a Double Thompson Sampling (D-TS) algorithm for d
 ueling bandit problems. As its name suggests\, D-TS selects both the first
  and the second candidates according to Thompson Sampling. Specifically\, 
 D-TS maintains a posterior distribution for the preference matrix\, and ch
 ooses the pair of arms for comparison according to two sets of samples ind
 ependently drawn from the posterior distribution. This simple algorithm ap
 plies to general Copeland dueling bandits\, including Condorcet dueling ba
 ndits as its special case.  For general Copeland dueling bandits\, we show
  that D-TS achieves $O(K^2 \\log T)$ regret. Moreover\, using a back subst
 itution argument\, we refine the regret to $O(K \\log T + K^2 \\log \\log 
 T)$ in Condorcet dueling bandits and many practical Copeland dueling bandi
 ts. In addition\, we propose an enhancement of D-TS\, referred to as D-TS+
 \, that reduces the regret by carefully breaking ties. Experiments based o
 n both synthetic and real-world data demonstrate that D-TS and D-TS$^+$ si
 gnificantly improve the overall performance\, in terms of regret and robus
 tness.
LOCATION:Area 5+6+7+8 #108
END:VEVENT
BEGIN:VEVENT
SUMMARY:Linear dynamical neural population models through nonlinear embedd
 ings | Yuanjun Gao \, Evan W Archer \, Liam Paninski \, John Cunningham
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Linear dynamical neural population models through nonli
 near embeddings\nYuanjun Gao \, Evan W Archer \, Liam Paninski \, John Cun
 ningham\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7337\n\nA body
  of recent work in modeling neural activity focuses on recovering low- dim
 ensional latent features that capture the statistical structure of large-s
 cale neural populations. Most such approaches have focused on linear gener
 ative models\, where inference is computationally tractable. Here\, we pro
 pose fLDS\, a general class of nonlinear generative models that permits th
 e firing rate of each neuron to vary as an arbitrary smooth function of a 
 latent\, linear dynamical state. This extra flexibility allows the model t
 o capture a richer set of neural variability than a purely linear model\, 
 but retains an easily visualizable low-dimensional latent space. To fit th
 is class of non-conjugate models we propose a variational inference scheme
 \, along with a novel approximate posterior capable of capturing rich temp
 oral correlations across time. We show that our techniques permit inferenc
 e in a wide class of generative models.We also show in application to two 
 neural datasets that\, compared to state-of-the-art neural population mode
 ls\, fLDS captures a much larger proportion of neural variability with a s
 mall number of latent dimensions\, providing superior predictive performan
 ce and interpretability.
LOCATION:Area 5+6+7+8 #109
END:VEVENT
BEGIN:VEVENT
SUMMARY:Regret Bounds for Non-decomposable Metrics with Missing Labels | N
 agarajan Natarajan \, Prateek Jain
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Regret Bounds for Non-decomposable Metrics with Missing
  Labels\nNagarajan Natarajan \, Prateek Jain\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=7338\n\nWe consider the problem of recommending rel
 evant labels (items) for a given data point (user). In particular\, we are
  interested in the practically important setting where the evaluation is w
 ith respect to non-decomposable (over labels) performance metrics like the
  $F1$ measure\, \\emph{and} training data has missing labels. To this end\
 , we propose a generic framework that given a performance metric $\\Psi$\,
  can devise a regularized objective function and a threshold such that all
  the values in the predicted score vector above and only above the thresho
 ld are selected to be positive.  We show that the regret or generalization
  error in the given metric $\\Psi$ is bounded ultimately by estimation err
 or of certain underlying parameters. In particular\, we derive regret boun
 ds under three popular settings: a) collaborative filtering\, b) multilabe
 l classification\, and c) PU (positive-unlabeled) learning.  For each of t
 he above problems\, we can obtain precise non-asymptotic regret bound whic
 h is small even when a large fraction of labels is missing. Our empirical 
 results on synthetic and benchmark datasets demonstrate that by explicitly
  modeling for missing labels and optimizing the desired performance metric
 \, our algorithm indeed achieves significantly better performance (like $F
 1$ score) when compared to methods that do not model missing label informa
 tion carefully.
LOCATION:Area 5+6+7+8 #110
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dynamic matrix recovery from incomplete observations under an exac
 t low-rank constraint | Liangbei Xu \, Mark Davenport
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Dynamic matrix recovery from incomplete observations un
 der an exact low-rank constraint\nLiangbei Xu \, Mark Davenport\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=7339\n\nLow-rank matrix factoriz
 ations arise in a wide variety of applications -- including recommendation
  systems\, topic models\, and source separation\, to name just a few.  In 
 these and many other applications\, it has been widely noted that by incor
 porating temporal information and allowing for the possibility of time-var
 ying models\, significant improvements are possible in practice. However\,
  despite the reported superior empirical performance of these dynamic mode
 ls over their static counterparts\, there is limited theoretical justifica
 tion for introducing these more complex models. In this paper we aim to ad
 dress this gap by studying the problem of recovering a dynamically evolvin
 g low-rank matrix from incomplete observations. First\, we propose the loc
 ally weighted matrix smoothing (LOWEMS) framework as one possible approach
  to dynamic matrix recovery. We then establish error bounds for LOWEMS in 
 both the {\\em matrix sensing} and {\\em matrix completion} observation mo
 dels. Our results quantify the potential benefits of exploiting dynamic co
 nstraints both in terms of recovery accuracy and sample complexity. To ill
 ustrate these benefits we provide both synthetic and real-world experiment
 al results.
LOCATION:Area 5+6+7+8 #111
END:VEVENT
BEGIN:VEVENT
SUMMARY:Rényi Divergence Variational Inference | Yingzhen Li \, Richard E
  Turner
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Rényi Divergence Variational Inference\nYingzhen Li \,
  Richard E Turner\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7340
 \n\nThis paper introduces the variational Rényi bound (VR) that extends t
 raditional variational inference to Rényi's alpha-divergences. This new f
 amily of variational methods unifies a number of existing approaches\, and
  enables a smooth interpolation from the evidence lower-bound to the log (
 marginal) likelihood that is controlled by the value of alpha that paramet
 rises the divergence. The reparameterization trick\, Monte Carlo approxima
 tion and stochastic optimisation methods are deployed to obtain a tractabl
 e and unified framework for optimisation. We further consider negative alp
 ha values and propose a novel variational inference method as a new specia
 l case in the proposed framework. Experiments on Bayesian neural networks 
 and variational auto-encoders demonstrate the wide applicability of the VR
  bound.
LOCATION:Area 5+6+7+8 #112
END:VEVENT
BEGIN:VEVENT
SUMMARY:Confusions over Time: An Interpretable Bayesian Model to Character
 ize Trends in Decision Making | Himabindu Lakkaraju \, Jure Leskovec
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Confusions over Time: An Interpretable Bayesian Model t
 o Characterize Trends in Decision Making\nHimabindu Lakkaraju \, Jure Lesk
 ovec\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7341\n\nWe propos
 e Confusions over Time (CoT)\, a novel generative framework which facilita
 tes a multi-granular analysis of the decision making process. The CoT not 
 only models the confusions or error properties of individual decision make
 rs and their evolution over time\, but also allows us to obtain diagnostic
  insights into the collective decision making process in an interpretable 
 manner. To this end\, the CoT models the confusions of the decision makers
  and their evolution over time via time-dependent confusion matrices. Inte
 rpretable insights are obtained by grouping similar decision makers (and i
 tems being judged) into clusters and representing each such cluster with a
 n appropriate prototype and identifying the most important features charac
 terizing the cluster via a subspace feature indicator vector. Experimentat
 ion with real world data on bail decisions\, asthma treatments\, and insur
 ance policy approval decisions demonstrates that CoT can accurately model 
 and explain the confusions of decision makers and their evolution over tim
 e.
LOCATION:Area 5+6+7+8 #113
END:VEVENT
BEGIN:VEVENT
SUMMARY:Adaptive Averaging in Accelerated Descent Dynamics | Walid Krichen
 e \, Alexandre Bayen \, Peter L Bartlett
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Adaptive Averaging in Accelerated Descent Dynamics\nWal
 id Krichene \, Alexandre Bayen \, Peter L Bartlett\nhttp://nips.cc/Confere
 nces/2016/Schedule?showEvent=7342\n\nWe study accelerated descent dynamics
  for constrained convex optimization. This dynamics can be described natur
 ally as a coupling of a dual variable accumulating gradients at a given ra
 te $\\eta(t)$\, and a primal variable obtained as the weighted average of 
 the mirrored dual trajectory\, with weights $w(t)$. Using a Lyapunov argum
 ent\, we give sufficient conditions on $\\eta$ and $w$ to achieve a desire
 d convergence rate. As an example\, we show that the replicator dynamics (
 an example of mirror descent on the simplex) can be accelerated using a si
 mple averaging scheme. We then propose an adaptive averaging heuristic whi
 ch adaptively computes the weights to speed up the decrease of the Lyapuno
 v function. We provide guarantees on adaptive averaging in continuous-time
 \, prove that it preserves the quadratic convergence rate of accelerated f
 irst-order methods in discrete-time\, and give numerical experiments to co
 mpare it with existing heuristics\, such as adaptive restarting. The exper
 iments indicate that adaptive averaging performs at least as well as adapt
 ive restarting\, with significant improvements in some cases.
LOCATION:Area 5+6+7+8 #114
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian Optimization for Probabilistic Programs | Tom Rainforth \
 , Tuan-Anh Le \, Jan-Willem van de Meent \, Michael A Osborne \, Frank Woo
 d
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Bayesian Optimization for Probabilistic Programs\nTom R
 ainforth \, Tuan-Anh Le \, Jan-Willem van de Meent \, Michael A Osborne \,
  Frank Wood\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7343\n\nWe
  present the first general purpose framework for marginal maximum a poster
 iori estimation of probabilistic program variables. By using a series of c
 ode transformations\, the evidence of any probabilistic program\, and ther
 efore of any graphical model\, can be optimized with respect to an arbitra
 ry subset of its sampled variables.  To carry out this optimization\, we d
 evelop the first Bayesian optimization package to directly exploit the sou
 rce code of its target\, leading to innovations in problem-independent hyp
 erpriors\, unbounded optimization\, and implicit constraint satisfaction\;
  delivering significant performance improvements over prominent existing p
 ackages.  We present applications of our method to a number of tasks inclu
 ding engineering design and parameter optimization.
LOCATION:Area 5+6+7+8 #115
END:VEVENT
BEGIN:VEVENT
SUMMARY:Efficient Globally Convergent Stochastic Optimization for Canonica
 l Correlation Analysis | Weiran Wang \, Jialei Wang \, Dan Garber \, Dan G
 arber \, Nati Srebro
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Efficient Globally Convergent Stochastic Optimization f
 or Canonical Correlation Analysis\nWeiran Wang \, Jialei Wang \, Dan Garbe
 r \, Dan Garber \, Nati Srebro\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=7344\n\nWe study the stochastic optimization of canonical correla
 tion analysis (CCA)\, whose objective is nonconvex and does not decouple o
 ver training samples. Although several stochastic gradient based optimizat
 ion algorithms have been recently proposed to solve this problem\, no glob
 al convergence guarantee was provided by any of them. Inspired by the alte
 rnating least squares/power iterations formulation of CCA\, and the shift-
 and-invert preconditioning method for PCA\, we propose two globally conver
 gent meta-algorithms for CCA\, both of which transform the original proble
 m into sequences of least squares problems that need only be solved approx
 imately. We instantiate the meta-algorithms with state-of-the-art SGD meth
 ods and obtain time complexities that significantly improve upon that of p
 revious work. Experimental results demonstrate their superior performance.
LOCATION:Area 5+6+7+8 #116
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Unified Approach for Learning the Parameters of Sum-Product Netw
 orks | Han Zhao \, Pascal Poupart \, Geoffrey J Gordon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Unified Approach for Learning the Parameters of Sum-P
 roduct Networks\nHan Zhao \, Pascal Poupart \, Geoffrey J Gordon\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7345\n\nWe present a unified ap
 proach for learning the parameters of Sum-Product networks (SPNs). We prov
 e that any complete and decomposable SPN is equivalent to a mixture of tre
 es where each tree corresponds to a product of univariate distributions. B
 ased on the mixture model perspective\, we characterize the objective func
 tion when learning SPNs based on the maximum likelihood estimation (MLE) p
 rinciple and show that the optimization problem can be formulated as a sig
 nomial program. We construct two parameter learning algorithms for SPNs by
  using sequential monomial approximations (SMA) and the concave-convex pro
 cedure (CCCP)\, respectively. The two proposed methods naturally admit mul
 tiplicative updates\, hence effectively avoiding the projection operation.
  With the help of the unified framework\, we also show that\, in the case 
 of SPNs\, CCCP leads to the same algorithm as Expectation Maximization (EM
 ) despite the fact that they are different in general.
LOCATION:Area 5+6+7+8 #117
END:VEVENT
BEGIN:VEVENT
SUMMARY:Feature-distributed sparse regression: a screen-and-clean approach
  | Jiyan Yang \, Michael W Mahoney \, Michael Saunders \, Yuekai Sun
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Feature-distributed sparse regression: a screen-and-cle
 an approach\nJiyan Yang \, Michael W Mahoney \, Michael Saunders \, Yuekai
  Sun\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7346\n\nMost exis
 ting approaches to distributed sparse regression assume the data is partit
 ioned by samples. However\, for high-dimensional data (D &gt\;&gt\; N)\, i
 t is more natural to partition the data by features. We propose an algorit
 hm to distributed sparse regression when the data is partitioned by featur
 es rather than samples. Our approach allows the user to tailor our general
  method to various distributed computing platforms by trading-off the tota
 l amount of data (in bits) sent over the communication network and the num
 ber of rounds of communication. We show that an implementation of our appr
 oach is capable of solving L1-regularized L2 regression problems with mill
 ions of features in minutes.
LOCATION:Area 5+6+7+8 #118
END:VEVENT
BEGIN:VEVENT
SUMMARY:Backprop KF: Learning Discriminative Deterministic State Estimator
 s | Tuomas Haarnoja \, Anurag Ajay \, Sergey Levine \, Pieter Abbeel
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Backprop KF: Learning Discriminative Deterministic Stat
 e Estimators\nTuomas Haarnoja \, Anurag Ajay \, Sergey Levine \, Pieter Ab
 beel\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7347\n\nGenerativ
 e state estimators based on probabilistic filters and smoothers are one of
  the most popular classes of state estimators for robots and autonomous ve
 hicles. However\, generative models have limited capacity to handle rich s
 ensory observations\, such as camera images\, since they must model the en
 tire distribution over sensor readings. Discriminative models do not suffe
 r from this limitation\, but are typically more complex to train as latent
  variable models for state estimation. We present an alternative approach 
 where the parameters of the latent state distribution are directly optimiz
 ed as a deterministic computation graph\, resulting in a simple and effect
 ive gradient descent algorithm for training discriminative state estimator
 s. We show that this procedure can be used to train state estimators that 
 use complex input\, such as raw camera images\, which must be processed us
 ing expressive nonlinear function approximators such as convolutional neur
 al networks. Our model can be viewed as a type of recurrent neural network
 \, and the connection to probabilistic filtering allows us to design a net
 work architecture that is particularly well suited for state estimation. W
 e evaluate our approach on synthetic tracking task with raw image inputs a
 nd on the visual odometry task in the KITTI dataset. The results show sign
 ificant improvement over both standard generative approaches and regular r
 ecurrent neural networks.
LOCATION:Area 5+6+7+8 #119
END:VEVENT
BEGIN:VEVENT
SUMMARY:Swapout: Learning an ensemble of deep architectures | Saurabh Sing
 h \, Derek Hoiem \, David Forsyth
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Swapout: Learning an ensemble of deep architectures\nSa
 urabh Singh \, Derek Hoiem \, David Forsyth\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7348\n\nWe describe Swapout\, a new stochastic train
 ing method\, that outperforms ResNets of identical network structure yield
 ing impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a r
 ich set of architectures including dropout\, stochastic depth and residual
  architectures as special cases. When viewed as a regularization method sw
 apout not only inhibits co-adaptation of units in a layer\, similar to dro
 pout\, but also across network layers. We conjecture that swapout achieves
  strong regularization by implicitly tying the parameters across layers. W
 hen viewed as an ensemble training method\, it samples a much richer set o
 f architectures than existing methods such as dropout or stochastic depth.
  We propose a parameterization that reveals connections to exiting archite
 ctures and suggests a much richer set of architectures to be explored. We 
 show that our formulation suggests an efficient training method and valida
 te our conclusions on CIFAR-10 and CIFAR-100 matching state of the art acc
 uracy. Remarkably\, our 32 layer wider model performs similar to a 1001 la
 yer ResNet model.
LOCATION:Area 5+6+7+8 #120
END:VEVENT
BEGIN:VEVENT
SUMMARY:Assortment Optimization Under the Mallows model | Antoine Desir \,
  Vineet Goyal \, Srikanth Jagabathula \, Danny Segev
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Assortment Optimization Under the Mallows model\nAntoin
 e Desir \, Vineet Goyal \, Srikanth Jagabathula \, Danny Segev\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7349\n\nWe consider the assortmen
 t optimization problem when customer preferences follow a mixture of Mallo
 ws distributions. The assortment optimization problem focuses on determini
 ng the revenue/profit maximizing subset of products from a large universe 
 of products\; it is an important decision that is commonly faced by retail
 ers in determining what to offer their customers. There are two key challe
 nges: (a) the Mallows distribution lacks a closed-form expression (and req
 uires summing an exponential number of terms) to compute the choice probab
 ility and\, hence\, the expected revenue/profit per customer\; and (b) fin
 ding the best subset may require an exhaustive search. Our key contributio
 ns are an efficiently computable closed-form expression for the choice pro
 bability under the Mallows model and a compact mixed integer linear progra
 m (MIP) formulation for the assortment problem.
LOCATION:Area 5+6+7+8 #121
END:VEVENT
BEGIN:VEVENT
SUMMARY:Operator Variational Inference | Rajesh Ranganath \, Dustin Tran \
 , Jaan Altosaar \, David Blei
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Operator Variational Inference\nRajesh Ranganath \, Dus
 tin Tran \, Jaan Altosaar \, David Blei\nhttp://nips.cc/Conferences/2016/S
 chedule?showEvent=7350\n\nVariational inference is an umbrella term for al
 gorithms which cast Bayesian inference as optimization. Classically\, vari
 ational inference uses the Kullback-Leibler divergence to define the optim
 ization. Though this divergence has been widely used\, the resultant poste
 rior approximation can suffer from undesirable statistical properties. To 
 address this\, we reexamine variational inference from its roots as an opt
 imization problem. We use operators\, or functions of functions\, to desig
 n variational objectives. As one example\, we design a variational objecti
 ve with a Langevin-Stein operator. We develop a black box algorithm\, oper
 ator variational inference (OPVI)\, for optimizing any operator objective.
  Importantly\, operators enable us to make explicit the statistical and co
 mputational tradeoffs for variational inference. We can characterize diffe
 rent properties of variational objectives\, such as objectives that admit 
 data subsampling---allowing inference to scale to massive data---as well a
 s objectives that admit variational programs---a rich class of posterior a
 pproximations that does not require a tractable density. We illustrate the
  benefits of OPVI on a mixture model and a generative model of images.
LOCATION:Area 5+6+7+8 #122
END:VEVENT
BEGIN:VEVENT
SUMMARY:Select-and-Sample for Spike-and-Slab Sparse Coding | Abdul-Saboor 
 Sheikh \, Jörg Lücke
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Select-and-Sample for Spike-and-Slab Sparse Coding\nAbd
 ul-Saboor Sheikh \, Jörg Lücke\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7351\n\nProbabilistic inference serves as a popular model for n
 eural processing. It is still unclear\, however\, how approximate probabil
 istic inference can be accurate and scalable to very high-dimensional cont
 inuous latent spaces. Especially as typical posteriors for sensory data ca
 n be expected to exhibit complex latent dependencies including multiple mo
 des. Here\, we study an approach that can efficiently be scaled while main
 taining a richly structured posterior approximation under these conditions
 . As example model we use spike-and-slab sparse coding for V1 processing\,
  and combine latent subspace selection with Gibbs sampling (select-and-sam
 ple). Unlike factored variational approaches\, the method can maintain lar
 ge numbers of posterior modes and complex latent dependencies. Unlike pure
  sampling\, the method is scalable to very high-dimensional latent spaces.
  Among all sparse coding approaches with non-trivial posterior approximati
 ons (MAP or ICA-like models)\, we report the largest-scale results. In app
 lications we firstly verify the approach by showing competitiveness in sta
 ndard denoising benchmarks. Secondly\, we use its scalability to\, for the
  first time\, study highly-overcomplete settings for V1 encoding using sop
 histicated posterior representations. More generally\, our study shows tha
 t very accurate probabilistic inference for multi-modal posteriors with co
 mplex dependencies is tractable\, functionally desirable and consistent wi
 th models for neural inference.
LOCATION:Area 5+6+7+8 #123
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast recovery from a union of subspaces | Chinmay Hegde \, Piotr I
 ndyk \, Ludwig Schmidt
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Fast recovery from a union of subspaces\nChinmay Hegde 
 \, Piotr Indyk \, Ludwig Schmidt\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7352\n\nWe address the problem of recovering a high-dimensional
  but structured vector from linear observations in a general setting where
  the vector can come from an arbitrary union of subspaces. This setup incl
 udes well-studied problems such as compressive sensing and low-rank matrix
  recovery. We show how to design more efficient algorithms for the union-o
 f subspace recovery problem by usingapproximateprojections. Instantiating 
 our general framework for the low-rank matrix recovery problem gives the f
 astest provable running time for an algorithm with optimal sample complexi
 ty. Moreover\, we give fast approximate projections for 2D histograms\, an
 other well-studied low-dimensional model of data. We complement our theore
 tical results with experiments demonstrating that our framework also leads
  to improved time and sample complexity empirically.
LOCATION:Area 5+6+7+8 #124
END:VEVENT
BEGIN:VEVENT
SUMMARY:Ladder Variational Autoencoders | Casper Kaae Sønderby \, Tapani 
 Raiko \, Lars Maaløe \, Søren Kaae Sønderby \, Ole Winther
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Ladder Variational Autoencoders\nCasper Kaae Sønderby 
 \, Tapani Raiko \, Lars Maaløe \, Søren Kaae Sønderby \, Ole Winther\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7353\n\nVariational auto
 encoders are powerful models for unsupervised learning. However deep model
 s with several layers of dependent stochastic variables are difficult to t
 rain which limits the improvements obtained using these highly expressive 
 models. We propose a new inference model\, the Ladder Variational Autoenco
 der\, that recursively corrects the generative distribution by a data depe
 ndent approximate likelihood in a process resembling the recently proposed
  Ladder Network. We show that this model provides state of the art predict
 ive log-likelihood and tighter log-likelihood lower bound compared to the 
 purely bottom-up inference in layered Variational Autoencoders and other g
 enerative models. We provide a detailed analysis of the learned hierarchic
 al latent representation and show that our new inference model is qualitat
 ively different and utilizes a deeper more distributed hierarchy of latent
  variables. Finally\, we observe that batch-normalization and deterministi
 c warm-up (gradually turning on the KL-term) are crucial for training vari
 ational models with many stochastic layers.
LOCATION:Area 5+6+7+8 #125
END:VEVENT
BEGIN:VEVENT
SUMMARY:SPALS: Fast Alternating Least Squares via Implicit Leverage Scores
  Sampling | Dehua Cheng \, Richard Peng \, Yan Liu \, Ioakeim Perros
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:SPALS: Fast Alternating Least Squares via Implicit Leve
 rage Scores Sampling\nDehua Cheng \, Richard Peng \, Yan Liu \, Ioakeim Pe
 rros\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7354\n\nTensor CA
 NDECOMP/PARAFAC (CP) decomposition is a powerful but computationally chall
 enging tool in modern data analytics. In this paper\, we show ways of samp
 ling intermediate steps of alternating minimization algorithms for computi
 ng low rank tensor CP decompositions\, leading to the sparse alternating l
 east squares (SPALS) method. Specifically\, we sample the the Khatri-Rao p
 roduct\, which arises as an intermediate object during the iterations of a
 lternating least squares. This product captures the interactions between d
 ifferent tensor modes\, and form the main computational bottleneck for sol
 ving many tensor related tasks. By exploiting the spectral structures of t
 he matrix Khatri-Rao product\, we provide efficient access to its statisti
 cal leverage scores. When applied to the tensor CP decomposition\, our met
 hod leads to the first algorithm that runs in sublinear time per-iteration
  and approximates the output of deterministic alternating least squares al
 gorithms. Empirical evaluations of this approach show significantly speedu
 ps over existing randomized and deterministic routines for performing CP d
 ecomposition. On a tensor of the size 2.4m by 6.6m by 92k with over 2 bill
 ion nonzeros formed by Amazon product reviews\, our routine converges in t
 wo minutes to the same error as deterministic ALS.
LOCATION:Area 5+6+7+8 #126
END:VEVENT
BEGIN:VEVENT
SUMMARY:CRF-CNN: Modeling Structured Information in Human Pose Estimation 
 | Xiao Chu \, Wanli Ouyang \, hongsheng Li \, Xiaogang Wang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:CRF-CNN: Modeling Structured Information in Human Pose 
 Estimation\nXiao Chu \, Wanli Ouyang \, hongsheng Li \, Xiaogang Wang\nhtt
 p://nips.cc/Conferences/2016/Schedule?showEvent=7355\n\nDeep convolutional
  neural networks (CNN) have achieved great success. On the other hand\, mo
 deling structural information has been proved critical in many vision prob
 lems. It is of great interest to integrate them effectively. In a classica
 l neural network\, there is no message passing between neurons in the same
  layer. In this paper\, we propose a CRF-CNN framework which can simultane
 ously model structural information in both output and hidden feature layer
 s in a probabilistic way\, and it is applied to human pose estimation. A m
 essage passing scheme is proposed\, so that in various layers each body jo
 int receives messages from all the others in an efficient way. Such messag
 e passing can be implemented with convolution between features maps in the
  same layer\, and it is also integrated with feedforward propagation in ne
 ural networks. Finally\, a neural network implementation of end-to-end lea
 rning CRF-CNN is provided. Its effectiveness is demonstrated through exper
 iments on two benchmark datasets.
LOCATION:Area 5+6+7+8 #127
END:VEVENT
BEGIN:VEVENT
SUMMARY:A Consistent Regularization Approach for Structured Prediction | C
 arlo Ciliberto \, Lorenzo Rosasco \, Alessandro Rudi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:A Consistent Regularization Approach for Structured Pre
 diction\nCarlo Ciliberto \, Lorenzo Rosasco \, Alessandro Rudi\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7356\n\nWe propose and analyze a 
 regularization approach for structured prediction problems. We characteriz
 e a large class of loss functions that allows to naturally embed structure
 d outputs in a linear space.  We exploit this fact to  design learning  al
 gorithms using a surrogate loss approach and regularization techniques.   
 We prove universal consistency and finite sample bounds characterizing the
  generalization properties of the proposed method. Experimental results ar
 e provided to demonstrate the practical usefulness of the proposed approac
 h.
LOCATION:Area 5+6+7+8 #128
END:VEVENT
BEGIN:VEVENT
SUMMARY:Refined Lower Bounds for Adversarial Bandits | Sébastien Gerchino
 vitz \, Tor Lattimore
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Refined Lower Bounds for Adversarial Bandits\nSébastie
 n Gerchinovitz \, Tor Lattimore\nhttp://nips.cc/Conferences/2016/Schedule?
 showEvent=7357\n\nWe provide new lower bounds on the regret that must be s
 uffered by adversarial bandit algorithms. The new results show that recent
  upper bounds that either (a) hold with high-probability or (b) depend on 
 the total loss of the best arm or (c) depend on the quadratic variation of
  the losses\, are close to tight. Besides this we prove two impossibility 
 results. First\, the existence of a single arm that is optimal in every ro
 und cannot improve the regret in the worst case. Second\, the regret canno
 t scale with the effective range of the losses. In contrast\, both results
  are possible in the full-information setting.
LOCATION:Area 5+6+7+8 #129
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Deep Embeddings with Histogram Loss | Evgeniya Ustinova \
 , Victor Lempitsky
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning Deep Embeddings with Histogram Loss\nEvgeniya 
 Ustinova \, Victor Lempitsky\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7358\n\nWe suggest a new loss for learning deep embeddings. The key
  characteristics of the new loss is the absence of tunable parameters and 
 very good results obtained across a range of datasets and problems. The lo
 ss is computed by estimating two distribution of similarities for positive
  (matching) and negative (non-matching) point pairs\, and then computing t
 he probability of a positive pair to have a lower similarity score than a 
 negative pair based on these probability estimates. We show that these ope
 rations can be performed in a simple and piecewise-differentiable manner u
 sing 1D histograms with soft assignment operations. This makes the propose
 d loss suitable for learning deep embeddings using stochastic optimization
 . The experiments reveal favourable results compared to recently proposed 
 loss functions.
LOCATION:Area 5+6+7+8 #130
END:VEVENT
BEGIN:VEVENT
SUMMARY:Solving Marginal MAP Problems with NP Oracles and Parity Constrain
 ts | Yexiang Xue \, zhiyuan li \, Stefano Ermon \, Carla P Gomes \, Bart S
 elman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Solving Marginal MAP Problems with NP Oracles and Parit
 y Constraints\nYexiang Xue \, zhiyuan li \, Stefano Ermon \, Carla P Gomes
  \, Bart Selman\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7359\n
 \nArising from many applications at the intersection of decision-making an
 d machine learning\, Marginal Maximum A Posteriori (Marginal MAP) problems
  unify the two main classes of inference\, namely maximization (optimizati
 on) and marginal inference (counting)\, and are believed to have higher co
 mplexity than both of them. We propose XORMMAP\, a novel approach to solve
  the Marginal MAP problem\, which represents the intractable counting subp
 roblem with queries to NP oracles\, subject to additional parity constrain
 ts. XORMMAP provides a constant factor approximation to the Marginal MAP p
 roblem\, by encoding it as a single optimization in a  polynomial size of 
 the original problem. We evaluate our approach in several machine learning
  and decision-making applications\, and show that our approach outperforms
  several state-of-the-art Marginal MAP solvers.
LOCATION:Area 5+6+7+8 #131
END:VEVENT
BEGIN:VEVENT
SUMMARY:Kernel Bayesian Inference with Posterior Regularization | Yang Son
 g \, Jun Zhu \, Yong Ren
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Kernel Bayesian Inference with Posterior Regularization
 \nYang Song \, Jun Zhu \, Yong Ren\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=7360\n\nWe propose a vector-valued regression problem whose s
 olution is equivalent to the reproducing kernel Hilbert space (RKHS) embed
 ding of the Bayesian posterior distribution. This equivalence provides a n
 ew understanding of kernel Bayesian inference. Moreover\, the optimization
  problem induces a new regularization for the posterior embedding estimato
 r\, which is faster and has comparable performance to the squared regulari
 zation in kernel Bayes' rule. This regularization coincides with a former 
 thresholding approach used in kernel POMDPs whose consistency remains to b
 e established. Our theoretical work solves this open problem and provides 
 consistency analysis in regression settings. Based on our optimizational f
 ormulation\, we propose a flexible Bayesian posterior regularization frame
 work which for the first time enables us to put regularization at the dist
 ribution level. We apply this method to nonparametric state-space filterin
 g tasks with extremely nonlinear dynamics and show performance gains over 
 all other baselines.
LOCATION:Area 5+6+7+8 #132
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Influence Functions from Incomplete Observations | Xinran
  He \, Ke Xu \, David Kempe \, Yan Liu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning Influence Functions from Incomplete Observatio
 ns\nXinran He \, Ke Xu \, David Kempe \, Yan Liu\nhttp://nips.cc/Conferenc
 es/2016/Schedule?showEvent=7361\n\nWe study the problem of learning influe
 nce functions under incomplete observations of node activations. Incomplet
 e observations are a major concern as most (online and real-world) social 
 networks are not fully observable. We establish both proper and improper P
 AC learnability of influence functions under randomly missing observations
 . Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) a
 nd Discrete-Time Independent Cascade (DIC) models is established by reduci
 ng incomplete observations to complete observations in a modified graph. O
 ur improper PAC learnability result applies for the DLT and DIC models as 
 well as the Continuous-Time Independent Cascade (CIC) model.  It is based 
 on a parametrization in terms of reachability features\, and also gives ri
 se to an efficient and practical heuristic. Experiments on synthetic and r
 eal-world datasets demonstrate the ability of our method to compensate eve
 n for a fairly large fraction of missing observations.
LOCATION:Area 5+6+7+8 #133
END:VEVENT
BEGIN:VEVENT
SUMMARY:General Tensor Spectral Co-clustering for Higher-Order Data | Tao 
 Wu \, Austin R Benson \, David Gleich
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:General Tensor Spectral Co-clustering for Higher-Order 
 Data\nTao Wu \, Austin R Benson \, David Gleich\nhttp://nips.cc/Conference
 s/2016/Schedule?showEvent=7362\n\nSpectral clustering and co-clustering ar
 e well-known techniques in data analysis\, and recent work has extended sp
 ectral clustering to square\, symmetric tensors and hypermatrices derived 
 from a network.  We develop a new tensor spectral co-clustering method tha
 t simultaneously clusters the rows\, columns\, and slices of a nonnegative
  three-mode tensor and generalizes to tensors with any number of modes.  T
 he algorithm is based on a new random walk model which we call the super-s
 pacey random surfer.  We show that our method out-performs state-of-the-ar
 t co-clustering methods on several synthetic datasets with ground truth cl
 usters and then use the algorithm to analyze several real-world datasets.
LOCATION:Area 5+6+7+8 #134
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian latent structure discovery from multi-neuron recordings |
  Scott Linderman \, Ryan P Adams \, Jonathan W Pillow
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Bayesian latent structure discovery from multi-neuron r
 ecordings\nScott Linderman \, Ryan P Adams \, Jonathan W Pillow\nhttp://ni
 ps.cc/Conferences/2016/Schedule?showEvent=7363\n\nNeural circuits contain 
 heterogeneous groups of neurons that differ in type\, location\, connectiv
 ity\, and basic response properties. However\, traditional methods for dim
 ensionality reduction and clustering are ill-suited to recovering the stru
 cture underlying the organization of neural circuits. In particular\, they
  do not take advantage of the rich temporal dependencies in multi-neuron r
 ecordings and fail to account for the noise in neural spike trains. Here w
 e describe new tools for inferring latent structure from simultaneously re
 corded spike train data using a hierarchical extension of a multi-neuron p
 oint process model commonly known as the generalized linear model (GLM). O
 ur approach combines the GLM with flexible graph-theoretic priors governin
 g the relationship between latent features and neural connectivity pattern
 s. Fully Bayesian inference via Pólya-gamma augmentation of the resulting
  model allows us to classify neurons and infer latent dimensions of circui
 t organization from correlated spike trains.  We demonstrate the effective
 ness of our method with applications to synthetic data and multi-neuron re
 cordings in primate retina\, revealing latent patterns of neural types and
  locations from spike trains alone.
LOCATION:Area 5+6+7+8 #135
END:VEVENT
BEGIN:VEVENT
SUMMARY:Estimating the Size of a Large Network and its Communities from a 
 Random Sample | Lin Chen \, Amin Karbasi \, Forrest W. Crawford
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Estimating the Size of a Large Network and its Communit
 ies from a Random Sample\nLin Chen \, Amin Karbasi \, Forrest W. Crawford\
 nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7364\n\nMost real-worl
 d networks are too large to be measured or studied directly and there is s
 ubstantial interest in estimating global network properties from smaller s
 ub-samples. One of the most important global properties is the number of v
 ertices/nodes in the network. Estimating the number of vertices in a large
  network is a major challenge in computer science\, epidemiology\, demogra
 phy\, and intelligence analysis. In this paper we consider a population ra
 ndom graph G = (V\;E) from the stochastic block model (SBM) with K communi
 ties/blocks. A sample is obtained by randomly choosing a subset W and lett
 ing G(W) be the induced subgraph in G of the vertices in W. In addition to
  G(W)\, we observe the total degree of each sampled vertex and its block m
 embership. Given this partial information\, we propose an efficient PopULa
 tion Size Estimation algorithm\, called PULSE\, that accurately estimates 
 the size of the whole population as well as the size of each community. To
  support our theoretical analysis\, we perform an exhaustive set of experi
 ments to study the effects of sample size\, K\, and SBM model parameters o
 n the accuracy of the estimates. The experimental results also demonstrate
  that PULSE significantly outperforms a widely-used method called the netw
 ork scale-up estimator in a wide variety of scenarios.
LOCATION:Area 5+6+7+8 #136
END:VEVENT
BEGIN:VEVENT
SUMMARY:Wasserstein Training of Restricted Boltzmann Machines | Grégoire 
 Montavon \, Klaus-Robert Müller \, Marco Cuturi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Wasserstein Training of Restricted Boltzmann Machines\n
 Grégoire Montavon \, Klaus-Robert Müller \, Marco Cuturi\nhttp://nips.cc
 /Conferences/2016/Schedule?showEvent=7365\n\nBoltzmann machines are able t
 o learn highly complex\, multimodal\, structured and multiscale real-world
  data distributions. Parameters of the model are usually learned by minimi
 zing the Kullback-Leibler (KL) divergence from training samples to the lea
 rned model. We propose in this work a novel approach for Boltzmann machine
  training which assumes that a meaningful metric between observations is g
 iven. This metric can be represented by the Wasserstein distance between d
 istributions\, for which we derive a gradient with respect to the model pa
 rameters. Minimization of this new objective leads to generative models wi
 th different statistical properties. We demonstrate their practical potent
 ial on data completion and denoising\, for which the metric between observ
 ations plays a crucial role.
LOCATION:Area 5+6+7+8 #137
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep ADMM-Net for Compressive Sensing MRI | yan yang \, Jian Sun \
 , Huibin Li \, Zongben Xu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Deep ADMM-Net for Compressive Sensing MRI\nyan yang \, 
 Jian Sun \, Huibin Li \, Zongben Xu\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7366\n\nCompressive Sensing (CS) is an effective approach fo
 r fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR imag
 e from a small number of  under-sampled data in k-space\, and accelerating
  the data acquisition in MRI.  To improve the current MRI system in recons
 truction accuracy and computational speed\,  in this paper\, we propose a 
 novel deep architecture\, dubbed ADMM-Net.  ADMM-Net is defined over a dat
 a flow graph\, which is derived from the iterative  procedures in Alternat
 ing Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-b
 ased MRI model. In the training phase\, all parameters of the net\, e.g.\,
  image transforms\, shrinkage functions\, etc.\, are discriminatively trai
 ned end-to-end using L-BFGS algorithm. In the testing phase\, it has compu
 tational overhead similar to ADMM but uses optimized parameters learned fr
 om the  training data for CS-based reconstruction task. Experiments on MRI
  image  reconstruction under different sampling ratios in k-space demonstr
 ate that it significantly improves the baseline ADMM algorithm and achieve
 s high reconstruction  accuracies with fast computational speed.
LOCATION:Area 5+6+7+8 #138
END:VEVENT
BEGIN:VEVENT
SUMMARY:Maximization of Approximately Submodular Functions | Thibaut Horel
  \, Yaron Singer
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Maximization of Approximately Submodular Functions\nThi
 baut Horel \, Yaron Singer\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7367\n\nWe study the problem of maximizing a function that is approxi
 mately submodular under a cardinality constraint. Approximate submodularit
 y implicitly appears in a wide range of applications as in many cases erro
 rs in evaluation of a submodular function break submodularity. Say that $F
 $ is $\\eps$-approximately submodular if there exists a submodular functio
 n $f$ such that $(1-\\eps)f(S) \\leq F(S)\\leq (1+\\eps)f(S)$ for all subs
 ets $S$. We are interested in characterizing the query-complexity of maxim
 izing $F$ subject to a cardinality constraint $k$ as a function of the err
 or level $\\eps &gt\; 0$.  We provide both lower and upper bounds: for $\\
 eps &gt\; n^{-1/2}$ we show an exponential query-complexity lower bound.  
 In contrast\, when $\\eps &lt\; {1}/{k}$ or under a stronger bounded curva
 ture assumption\, we give constant approximation algorithms.
LOCATION:Area 5+6+7+8 #139
END:VEVENT
BEGIN:VEVENT
SUMMARY:Combining Low-Density Separators with CNNs | Yu-Xiong Wang \, Mart
 ial Hebert
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Combining Low-Density Separators with CNNs\nYu-Xiong Wa
 ng \, Martial Hebert\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7
 368\n\nThis work explores CNNs for the recognition of novel categories fro
 m few examples. Inspired by the transferability analysis of CNNs\, we intr
 oduce an additional unsupervised meta-training stage that exposes multiple
  top layer units to a large amount of unlabeled real-world images. By enco
 uraging these units to learn diverse sets of low-density separators across
  the unlabeled data\, we capture a more generic\, richer description of th
 e visual world\, which decouples these units from ties to a specific set o
 f categories. We propose an unsupervised margin maximization that jointly 
 estimates compact high-density regions and infers low-density separators. 
 The low-density separator (LDS) modules can be plugged into any or all of 
 the top layers of a standard CNN architecture. The resulting CNNs\, with e
 nhanced generality\, significantly improve the performance in scene classi
 fication\, fine-grained recognition\, and action recognition with small tr
 aining samples.
LOCATION:Area 5+6+7+8 #140
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Sensor Multiplexing Design through Back-propagation | Aya
 n Chakrabarti
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning Sensor Multiplexing Design through Back-propag
 ation\nAyan Chakrabarti\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7369\n\nRecent progress on many imaging and vision tasks has been driven
  by the use of deep feed-forward neural networks\, which are trained by pr
 opagating gradients of a loss defined on the final output\, back through t
 he network up to the first layer that operates directly on the image. We p
 ropose back-propagating one step further---to learn camera sensor designs 
 jointly with networks that carry out inference on the images they capture.
  In this paper\, we specifically consider the design and inference problem
 s in a typical color camera---where the sensor is able to measure only one
  color channel at each pixel location\, and computational inference is req
 uired to reconstruct a full color image. We learn the camera sensor's colo
 r multiplexing pattern by encoding it as layer whose learnable weights det
 ermine which color channel\, from among a fixed set\, will be measured at 
 each location. These weights are jointly trained with those of a reconstru
 ction network that operates on the corresponding sensor measurements to pr
 oduce a full color image. Our network achieves significant improvements in
  accuracy over the traditional Bayer pattern used in most color cameras. I
 t automatically learns to employ a sparse color measurement approach simil
 ar to that of a recent design\, and moreover\, improves upon that design b
 y learning an optimal layout for these measurements.
LOCATION:Area 5+6+7+8 #141
END:VEVENT
BEGIN:VEVENT
SUMMARY:High resolution neural connectivity from incomplete tracing data u
 sing nonnegative spline regression | Kameron D Harris \, Stefan Mihalas \,
  Eric Shea-Brown
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:High resolution neural connectivity from incomplete tra
 cing data using nonnegative spline regression\nKameron D Harris \, Stefan 
 Mihalas \, Eric Shea-Brown\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7370\n\nWhole-brain neural connectivity data are now available from v
 iral tracing experiments\, which reveal the connections between a source i
 njection site and elsewhere in the brain. These hold the promise of reveal
 ing spatial patterns of connectivity throughout the mammalian brain. To ac
 hieve this goal\, we seek to fit a weighted\, nonnegative adjacency matrix
  among 100 μm brain “voxels” using viral tracer data. Despite a multi
 -year experimental effort\, injections provide incomplete coverage\, and t
 he number of voxels in our data is orders of magnitude larger than the num
 ber of injections\, making the problem severely underdetermined. Furthermo
 re\, projection data are missing within the injection site because local c
 onnections there are not separable from the injection signal.  We use a no
 vel machine-learning algorithm to meet these challenges and develop a spat
 ially explicit\, voxel-scale connectivity map of the mouse visual system. 
 Our method combines three features: a matrix completion loss for missing d
 ata\, a smoothing spline penalty to regularize the problem\, and (optional
 ly) a low rank factorization. We demonstrate the consistency of our estima
 tor using synthetic data and then apply it to newly available Allen Mouse 
 Brain Connectivity Atlas data for the visual system. Our algorithm is sign
 ificantly more predictive than current state of the art approaches which a
 ssume regions to be homogeneous. We demonstrate the efficacy of a low rank
  version on visual cortex data and discuss the possibility of extending th
 is to a whole-brain connectivity matrix at the voxel scale.
LOCATION:Area 5+6+7+8 #142
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning a Probabilistic Latent Space of Object Shapes via 3D Gene
 rative-Adversarial Modeling | Chengkai Zhang \, Jiajun Wu \, Tianfan Xue \
 , Bill Freeman \, Josh Tenenbaum
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning a Probabilistic Latent Space of Object Shapes 
 via 3D Generative-Adversarial Modeling\nChengkai Zhang \, Jiajun Wu \, Tia
 nfan Xue \, Bill Freeman \, Josh Tenenbaum\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7371\n\nWe study the problem of 3D object generation.
  We propose a novel framework\, namely 3D Generative Adversarial Network (
 3D-GAN)\, which generates 3D objects from a probabilistic space by leverag
 ing recent advances in volumetric convolutional networks and generative ad
 versarial nets. The benefits of our model are three-fold: first\, the use 
 of an adversarial criterion\, instead of traditional heuristic criteria\, 
 enables the generator to capture object structure implicitly and to synthe
 size high-quality 3D objects\; second\, the generator establishes a mappin
 g from a low-dimensional probabilistic space to the space of 3D objects\, 
 so that we can sample objects without a reference image or CAD models\, an
 d explore the 3D object manifold\; third\, the adversarial discriminator p
 rovides a powerful 3D shape descriptor which\, learned without supervision
 \, has wide applications in 3D object recognition. Experiments demonstrate
  that our method generates high-quality 3D objects\, and our unsupervisedl
 y learned features achieve impressive performance on 3D object recognition
 \, comparable with those of supervised learning methods.
LOCATION:Area 5+6+7+8 #143
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Sparse Gaussian Graphical Models with Overlapping Blocks 
 | Seyed Mohammad Javad Hosseini \, Su-In Lee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning Sparse Gaussian Graphical Models with Overlapp
 ing Blocks\nSeyed Mohammad Javad Hosseini \, Su-In Lee\nhttp://nips.cc/Con
 ferences/2016/Schedule?showEvent=7372\n\nWe present a novel framework\, ca
 lled GRAB (GRaphical models with overlApping Blocks)\, to capture densely 
 connected components in a network estimate. GRAB takes as input a data mat
 rix of p variables and n samples\, and jointly learns both a network among
  p variables and densely connected groups of variables (called `blocks'). 
 GRAB has four major novelties as compared to existing network estimation m
 ethods: 1) It does not require the blocks to be given a priori. 2) Blocks 
 can overlap. 3) It can jointly learn a network structure and overlapping b
 locks. 4) It solves a joint optimization problem with the block coordinate
  descent method that is convex in each step. We show that GRAB reveals the
  underlying network structure substantially better than four state-of-the-
 art competitors on synthetic data. When applied to cancer gene expression 
 data\, GRAB outperforms its competitors in revealing known functional gene
  sets and potentially novel genes that drive cancer.
LOCATION:Area 5+6+7+8 #144
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multi-step learning and underlying structure in statistical models
  | Maia Fraser
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Multi-step learning and underlying structure in statist
 ical models\nMaia Fraser\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7373\n\nIn multi-step learning\, where a final learning task is accompl
 ished via a sequence of intermediate learning tasks\, the intuition is tha
 t successive steps or levels transform the initial data into representatio
 ns more and moresuited" to the final learning task. A related principle ar
 ises in transfer-learning where Baxter (2000) proposed a theoretical frame
 work to study how learning multiple tasks transforms the inductive bias of
  a learner. The most widespread multi-step learning approach is semi-super
 vised learning with two steps: unsupervised\, then supervised. Several aut
 hors (Castelli-Cover\, 1996\; Balcan-Blum\, 2005\; Niyogi\, 2008\; Ben-Dav
 id et al\, 2008\; Urner et al\, 2011) have analyzed SSL\, with Balcan-Blum
  (2005) proposing a version of the PAC learning framework augmented by aco
 mpatibility function" to link concept class and unlabeled data distributio
 n. We propose to analyze SSL and other multi-step learning approaches\, mu
 ch in the spirit of Baxter's framework\, by defining a learning problem ge
 neratively as a joint statistical model on $X \\times Y$. This determines 
 in a natural way the class of conditional distributions that are possible 
 with each marginal\, and amounts to an abstract form of compatibility func
 tion. It also allows to analyze both discrete and non-discrete settings. A
 s tool for our analysis\, we define a notion of $\\gamma$-uniform shatteri
 ng for statistical models. We use this to give conditions on the marginal 
 and conditional models which imply an advantage for multi-step learning ap
 proaches. In particular\, we recover a more general version of a result of
  Poggio et al (2012): under mild hypotheses a multi-step approach which le
 arns features invariant under successive factors of a finite group of inva
 riances has sample complexity requirements that are additive rather than m
 ultiplicative in the size of the subgroups.
LOCATION:Area 5+6+7+8 #145
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dynamic Network Surgery for Efficient DNNs | Yiwen Guo \, Anbang Y
 ao \, Yurong Chen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Dynamic Network Surgery for Efficient DNNs\nYiwen Guo \
 , Anbang Yao \, Yurong Chen\nhttp://nips.cc/Conferences/2016/Schedule?show
 Event=7374\n\nDeep learning has become a ubiquitous technology to improve 
 machine intelligence. However\, most of the existing deep models are struc
 turally very complex\, making them difficult to be deployed on the mobile 
 platforms with limited computational power. In this paper\, we propose a n
 ovel network compression method called dynamic network surgery\, which can
  remarkably reduce the network complexity by making on-the-fly connection 
 pruning. Unlike the previous methods which accomplish this task in a greed
 y way\, we properly incorporate connection splicing into the whole process
  to avoid incorrect pruning and make it as a continual network maintenance
 . The effectiveness of our method is proved with experiments. Without any 
 accuracy loss\, our method can efficiently compress the number of paramete
 rs in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7
 }\\times$ respectively\, proving that it outperforms the recent pruning me
 thod by considerable margins. Code and some models are available at https:
 //github.com/yiwenguo/Dynamic-Network-Surgery.
LOCATION:Area 5+6+7+8 #146
END:VEVENT
BEGIN:VEVENT
SUMMARY:Active Nearest-Neighbor Learning in Metric Spaces | Aryeh Kontorov
 ich \, Sivan Sabato \, Ruth Urner
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Active Nearest-Neighbor Learning in Metric Spaces\nArye
 h Kontorovich \, Sivan Sabato \, Ruth Urner\nhttp://nips.cc/Conferences/20
 16/Schedule?showEvent=7375\n\nWe propose a pool-based non-parametric activ
 e learning algorithm for general metric spaces\, called MArgin Regularized
  Metric Active Nearest Neighbor (MARMANN)\, which outputs a nearest-neighb
 or classifier. We give prediction error guarantees that depend on the nois
 y-margin properties of the input sample\, and are competitive with those o
 btained by previously proposed passive learners. We prove that the label c
 omplexity of MARMANN is significantly lower than that of any passive learn
 er with similar error guarantees. Our algorithm is based on a generalized 
 sample compression scheme and a new label-efficient active model-selection
  procedure.
LOCATION:Area 5+6+7+8 #147
END:VEVENT
BEGIN:VEVENT
SUMMARY:Discriminative Gaifman Models | Mathias Niepert
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Discriminative Gaifman Models\nMathias Niepert\nhttp://
 nips.cc/Conferences/2016/Schedule?showEvent=7376\n\nWe present discriminat
 ive Gaifman models\, a novel family of relational machine learning models.
  Gaifman models learn feature representations bottom up from representatio
 ns of locally connected and bounded-size regions of knowledge bases (KBs).
   Considering local and bounded-size neighborhoods of knowledge bases rend
 ers logical inference and learning tractable\, mitigates the problem of ov
 erfitting\, and facilitates weight sharing. Gaifman models sample neighbor
 hoods of knowledge bases so as to make the learned relational models more 
 robust to missing objects and relations which is a common situation in ope
 n-world KBs. We present the core ideas of Gaifman models and apply them to
  large-scale relational learning problems. We also discuss the ways in whi
 ch Gaifman models relate to some existing relational machine learning appr
 oaches.
LOCATION:Area 5+6+7+8 #148
END:VEVENT
BEGIN:VEVENT
SUMMARY:Professor Forcing: A New Algorithm for Training Recurrent Networks
  | Alex M Lamb \, Anirudh Goyal ALIAS PARTH GOYAL \, Ying Zhang \, Saizhen
 g Zhang \, Aaron C Courville \, Yoshua Bengio
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Professor Forcing: A New Algorithm for Training Recurre
 nt Networks\nAlex M Lamb \, Anirudh Goyal ALIAS PARTH GOYAL \, Ying Zhang 
 \, Saizheng Zhang \, Aaron C Courville \, Yoshua Bengio\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7377\n\nThe Teacher Forcing algorithm tr
 ains recurrent networks by supplying observed sequence values as inputs du
 ring training and using the network’s own one-step-ahead predictions to 
 do multi-step sampling. We introduce the Professor Forcing algorithm\, whi
 ch uses adversarial domain adaptation to encourage the dynamics of the rec
 urrent network to be the same when training the network and when sampling 
 from the network over multiple time steps. We apply Professor Forcing to l
 anguage modeling\, vocal synthesis on raw waveforms\, handwriting generati
 on\, and image generation. Empirically we find that Professor Forcing acts
  as a regularizer\, improving test likelihood on character level Penn Tree
 bank and sequential MNIST. We also find that the model qualitatively impro
 ves samples\, especially when sampling for a large number of time steps.  
 This is supported by human evaluation of sample quality.  Trade-offs betwe
 en Professor Forcing and Scheduled Sampling are discussed. We produce T-SN
 Es showing that Professor Forcing successfully makes the dynamics of the n
 etwork during training and sampling more similar.
LOCATION:Area 5+6+7+8 #149
END:VEVENT
BEGIN:VEVENT
SUMMARY:Pruning Random Forests for Prediction on a Budget | Feng Nan \, Jo
 seph Wang \, Venkatesh Saligrama
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Pruning Random Forests for Prediction on a Budget\nFeng
  Nan \, Joseph Wang \, Venkatesh Saligrama\nhttp://nips.cc/Conferences/201
 6/Schedule?showEvent=7378\n\nWe propose to prune a random forest (RF) for 
 resource-constrained prediction. We first construct a RF and then prune it
  to optimize expected feature cost &amp\; accuracy. We pose pruning RFs as
  a novel 0-1 integer program with linear constraints that encourages featu
 re re-use. We establish total unimodularity of the constraint set to prove
  that the corresponding LP relaxation solves the original integer program.
  We then exploit connections to combinatorial optimization and develop an 
 efficient primal-dual algorithm\, scalable to large datasets. In contrast 
 to our bottom-up approach\, which benefits from good RF initialization\, c
 onventional methods are top-down acquiring features based on their utility
  value and is generally intractable\, requiring heuristics.  Empirically\,
  our pruning algorithm outperforms existing state-of-the-art resource-cons
 trained algorithms.
LOCATION:Area 5+6+7+8 #150
END:VEVENT
BEGIN:VEVENT
SUMMARY:Multistage Campaigning in Social Networks | Mehrdad Farajtabar \, 
 Xiaojing Ye \, Sahar Harati \, Le Song \, Hongyuan Zha
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Multistage Campaigning in Social Networks\nMehrdad Fara
 jtabar \, Xiaojing Ye \, Sahar Harati \, Le Song \, Hongyuan Zha\nhttp://n
 ips.cc/Conferences/2016/Schedule?showEvent=7379\n\nWe consider control pro
 blems for multi-stage campaigning over social networks. The dynamic progra
 mming framework is employed to balance the high present reward and large p
 enalty on low future outcome in the presence of extensive uncertainties. I
 n particular\, we establish theoretical foundations of optimal campaigning
  over social networks where the user activities are modeled as a multivari
 ate Hawkes process\, and we derive a time dependent linear relation betwee
 n the intensity of exogenous events and several commonly used objective fu
 nctions of campaigning. We further develop a convex dynamic programming fr
 amework for determining the optimal intervention policy that prescribes th
 e required level of external drive at each stage for the desired campaigni
 ng result. Experiments on both synthetic data and the real-world MemeTrack
 er dataset show that our algorithm can steer the user activities for optim
 al campaigning much more accurately than baselines.
LOCATION:Area 5+6+7+8 #151
END:VEVENT
BEGIN:VEVENT
SUMMARY:Coevolutionary Latent Feature Processes for Continuous-Time User-I
 tem Interactions | Yichen Wang \, Nan Du \, Rakshit Trivedi \, Le Song
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Coevolutionary Latent Feature Processes for Continuous-
 Time User-Item Interactions\nYichen Wang \, Nan Du \, Rakshit Trivedi \, L
 e Song\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7380\n\nMatchin
 g users to the right items at the right time is a fundamental task in reco
 mmendation systems. As users interact with different items over time\, use
 rs' and items' feature may evolve and co-evolve over time. Traditional mod
 els based on static latent features or discretizing time into epochs can b
 ecome ineffective for capturing the fine-grained temporal dynamics in the 
 user-item interactions. We propose a coevolutionary latent feature process
  model that accurately captures the coevolving nature of users' and items'
  feature. To learn parameters\, we design an efficient convex optimization
  algorithm with a novel low rank space sharing constraints. Extensive expe
 riments on diverse real-world datasets demonstrate significant improvement
 s in user behavior prediction compared to state-of-the-arts.
LOCATION:Area 5+6+7+8 #152
END:VEVENT
BEGIN:VEVENT
SUMMARY:Coordinate-wise Power Method | Qi Lei \, Kai Zhong \, Inderjit S D
 hillon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Coordinate-wise Power Method\nQi Lei \, Kai Zhong \, In
 derjit S Dhillon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7381\
 n\nIn this paper\, we propose a coordinate-wise version of the power metho
 d from an optimization viewpoint. The vanilla power method simultaneously 
 updates all the coordinates of the iterate\, which is essential for its co
 nvergence analysis. However\, different coordinates converge to the optima
 l value at different speeds. Our proposed algorithm\, which we call coordi
 nate-wise power method\, is able to select and update the most important k
  coordinates in O(kn) time at each iteration\, where n is the dimension of
  the matrix and k &lt\;= n is the size of the active set. Inspired by the 
 ''greedy'' nature of our method\, we further propose a greedy coordinate d
 escent algorithm applied on a non-convex objective function specialized fo
 r symmetric matrices. We provide convergence analyses for both methods. Ex
 perimental results on both synthetic and real data show that our methods a
 chieve up to 20 times speedup over the basic power method. Meanwhile\, due
  to their coordinate-wise nature\, our methods are very suitable for the i
 mportant case when data cannot fit into memory. Finally\, we introduce how
  the coordinate-wise mechanism could be applied to other iterative methods
  that are used in machine learning.
LOCATION:Area 5+6+7+8 #153
END:VEVENT
BEGIN:VEVENT
SUMMARY:Barzilai-Borwein Step Size for Stochastic Gradient Descent | Congh
 ui Tan \, Shiqian Ma \, Yu-Hong Dai \, Yuqiu Qian
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Barzilai-Borwein Step Size for Stochastic Gradient Desc
 ent\nConghui Tan \, Shiqian Ma \, Yu-Hong Dai \, Yuqiu Qian\nhttp://nips.c
 c/Conferences/2016/Schedule?showEvent=7382\n\nOne of the major issues in s
 tochastic gradient descent (SGD) methods is how to choose an appropriate s
 tep size while running the algorithm. Since the traditional line search te
 chnique does not apply for stochastic optimization methods\, the common pr
 actice in SGD is either to use a diminishing step size\, or to tune a step
  size by hand\, which can be time consuming in practice. In this paper\, w
 e propose to use the Barzilai-Borwein (BB) method to automatically compute
  step sizes for SGD and its variant: stochastic variance reduced gradient 
 (SVRG) method\, which leads to two algorithms: SGD-BB and SVRG-BB. We prov
 e that SVRG-BB converges linearly for strongly convex objective functions.
  As a by-product\, we prove the linear convergence result of SVRG with Opt
 ion I proposed in [10]\, whose convergence result has been missing in the 
 literature. Numerical experiments on standard data sets show that the perf
 ormance of SGD-BB and SVRG-BB is comparable to and sometimes even better t
 han SGD and SVRG with best-tuned step sizes\, and is superior to some adva
 nced SGD variants.
LOCATION:Area 5+6+7+8 #154
END:VEVENT
BEGIN:VEVENT
SUMMARY:Fast learning rates with heavy-tailed losses | Vu C Dinh \, Lam S 
 Ho \, Binh Nguyen \, Duy Nguyen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Fast learning rates with heavy-tailed losses\nVu C Dinh
  \, Lam S Ho \, Binh Nguyen \, Duy Nguyen\nhttp://nips.cc/Conferences/2016
 /Schedule?showEvent=7383\n\nWe study fast learning rates when the losses a
 re not necessarily bounded and may have a distribution with heavy tails. T
 o enable such analyses\, we introduce two new conditions: (i)  the envelop
 e function $\\sup_{f \\in \\mathcal{F}}|\\ell \\circ f|$\, where $\\ell$ i
 s the loss function and $\\mathcal{F}$ is the hypothesis class\, exists an
 d is $L^r$-integrable\, and (ii) $\\ell$ satisfies the multi-scale Bernste
 in's condition on $\\mathcal{F}$. Under these assumptions\, we prove that 
 learning rate faster than $O(n^{-1/2})$ can be obtained and\, depending on
  $r$ and the multi-scale Bernstein's powers\, can be arbitrarily close to 
 $O(n^{-1})$. We then verify these assumptions and derive fast learning rat
 es for the problem of vector quantization by $k$-means clustering with hea
 vy-tailed distributions. The analyses enable us to obtain novel learning r
 ates that extend and complement existing results in the literature from bo
 th theoretical and practical viewpoints.
LOCATION:Area 5+6+7+8 #155
END:VEVENT
BEGIN:VEVENT
SUMMARY:CliqueCNN: Deep Unsupervised Exemplar Learning | Miguel A Bautista
  \, Artsiom Sanakoyeu \, Ekaterina Tikhoncheva \, Bjorn Ommer
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:CliqueCNN: Deep Unsupervised Exemplar Learning\nMiguel 
 A Bautista \, Artsiom Sanakoyeu \, Ekaterina Tikhoncheva \, Bjorn Ommer\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7384\n\nExemplar learnin
 g is a powerful paradigm for discovering visual similarities in an unsuper
 vised manner. In this context\, however\, the recent breakthrough in deep 
 learning could not yet unfold its full potential. With only a single posit
 ive sample\, a great imbalance between one positive and many negatives\, a
 nd unreliable relationships between most samples\, training of convolution
 al neural networks is impaired. Given weak estimates of local distance we 
 propose a single optimization problem to extract batches of samples with m
 utually consistent relations. Conflicting relations are distributed over d
 ifferent batches and similar samples are grouped into compact cliques. Lea
 rning exemplar similarities is framed as a sequence of clique categorizati
 on tasks. The CNN then consolidates transitivity relations within and betw
 een cliques and learns a single representation for all samples without the
  need for labels. The proposed unsupervised approach has shown competitive
  performance on detailed posture analysis and object classification.
LOCATION:Area 5+6+7+8 #156
END:VEVENT
BEGIN:VEVENT
SUMMARY:Guided Policy Search via Approximate Mirror Descent | William H Mo
 ntgomery \, Sergey Levine
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Guided Policy Search via Approximate Mirror Descent\nWi
 lliam H Montgomery \, Sergey Levine\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7385\n\nGuided policy search algorithms can be used to optim
 ize complex nonlinear policies\, such as deep neural networks\, without di
 rectly computing policy gradients in the high-dimensional parameter space.
  Instead\, these methods use supervised learning to train the policy to mi
 mic a “teacher” algorithm\, such as a trajectory optimizer or a trajec
 tory-centric reinforcement learning method. Guided policy search methods p
 rovide asymptotic local convergence guarantees by construction\, but it is
  not clear how much the policy improves within a small\, finite number of 
 iterations. We show that guided policy search algorithms can be interprete
 d as an approximate variant of mirror descent\, where the projection onto 
 the constraint manifold is not exact. We derive a new guided policy search
  algorithm that is simpler and provides appealing improvement and converge
 nce guarantees in simplified convex and linear settings\, and show that in
  the more general nonlinear setting\, the error in the projection step can
  be bounded. We provide empirical results on several simulated robotic man
 ipulation tasks that show that our method is stable and achieves similar o
 r better performance when compared to prior guided policy search methods\,
  with a simpler formulation and fewer hyperparameters.
LOCATION:Area 5+6+7+8 #157
END:VEVENT
BEGIN:VEVENT
SUMMARY:Structured Sparse Regression via Greedy Hard Thresholding | Pratee
 k Jain \, Nikhil Rao \, Inderjit S Dhillon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Structured Sparse Regression via Greedy Hard Thresholdi
 ng\nPrateek Jain \, Nikhil Rao \, Inderjit S Dhillon\nhttp://nips.cc/Confe
 rences/2016/Schedule?showEvent=7386\n\nSeveral learning applications requi
 re solving high-dimensional regression problems where the relevant feature
 s belong to a small number of (overlapping) groups. For very large dataset
 s and under standard sparsity constraints\, hard thresholding methods have
  proven to be extremely efficient\, but such methods require NP hard proje
 ctions when dealing with overlapping groups. In this paper\, we show that 
 such NP-hard projections can not only be avoided by appealing to submodula
 r optimization\, but such methods come with strong theoretical guarantees 
 even in the presence of poorly conditioned data (i.e. say when two feature
 s have correlation  $\\geq 0.99$)\, which existing analyses cannot handle.
  These methods exhibit an interesting computation-accuracy trade-off and c
 an be extended to significantly harder problems such as sparse overlapping
  groups. Experiments on both real and synthetic data validate our claims a
 nd demonstrate that the proposed methods are orders of magnitude faster th
 an other greedy and convex relaxation techniques for learning with group-s
 tructured sparsity.
LOCATION:Area 5+6+7+8 #158
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning in Games: Robustness of Fast Convergence | Dylan J Foster
  \, zhiyuan li \, Thodoris Lykouris \, Karthik Sridharan \, Eva Tardos
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning in Games: Robustness of Fast Convergence\nDyla
 n J Foster \, zhiyuan li \, Thodoris Lykouris \, Karthik Sridharan \, Eva 
 Tardos\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7387\n\nWe show
  that learning algorithms satisfying a low approximate regret property exp
 erience fast convergence to approximate optimality in a large class of rep
 eated games. Our property\, which simply requires that each learner has sm
 all regret compared to a (1+eps)-multiplicative approximation to the best 
 action in hindsight\, is ubiquitous among learning algorithms\; it is sati
 sfied even by the vanilla Hedge forecaster. Our results improve upon recen
 t work of Syrgkanis et al. in a number of ways. We require only that playe
 rs observe payoffs under other players' realized actions\, as opposed to e
 xpected payoffs. We further show that convergence occurs with high probabi
 lity\, and show convergence under bandit feedback. Finally\, we improve up
 on the speed of convergence by a factor of n\, the number of players. Both
  the scope of settings and the class of algorithms for which our analysis 
 provides fast convergence are considerably broader than in previous work. 
 Our framework applies to dynamic population games via a low approximate re
 gret property for shifting experts. Here we strengthen the results of Lyko
 uris et al. in two ways: We allow players to select learning algorithms fr
 om a larger class\, which includes a minor variant of the basic Hedge algo
 rithm\, and we increase the maximum churn in players for which approximate
  optimality is achieved. In the bandit setting we present a new algorithm 
 which provides a "small loss"-type bound with improved dependence on the n
 umber of actions in utility settings\, and is both simple and efficient. T
 his result may be of independent interest.
LOCATION:Area 5+6+7+8 #159
END:VEVENT
BEGIN:VEVENT
SUMMARY:Measuring the reliability of MCMC inference with bidirectional Mon
 te Carlo | Roger B Grosse \, Siddharth Ancha \, Daniel Roy
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Measuring the reliability of MCMC inference with bidire
 ctional Monte Carlo\nRoger B Grosse \, Siddharth Ancha \, Daniel Roy\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=7388\n\nMarkov chain Monte 
 Carlo (MCMC) is one of the main workhorses of probabilistic inference\, bu
 t it is notoriously hard to measure the quality of approximate posterior s
 amples. This challenge is particularly salient in black box inference meth
 ods\, which can hide details and obscure inference failures. In this work\
 , we extend the recently introduced bidirectional Monte Carlo technique to
  evaluate MCMC-based posterior inference algorithms. By running annealed i
 mportance sampling (AIS) chains both from prior to posterior and vice vers
 a on simulated data\, we upper bound in expectation the symmetrized KL div
 ergence between the true posterior distribution and the distribution of ap
 proximate samples. We integrate our method into two probabilistic programm
 ing languages\, WebPPL and Stan\, and validate it on several models and da
 tasets. As an example of how our method be used to guide the design of inf
 erence algorithms\, we apply it to study the effectiveness of different mo
 del representations in WebPPL and Stan.
LOCATION:Area 5+6+7+8 #160
END:VEVENT
BEGIN:VEVENT
SUMMARY:Average-case hardness of RIP certification | Tengyao Wang \, Quent
 in Berthet \, Yaniv Plan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Average-case hardness of RIP certification\nTengyao Wan
 g \, Quentin Berthet \, Yaniv Plan\nhttp://nips.cc/Conferences/2016/Schedu
 le?showEvent=7389\n\nThe restricted isometry property (RIP) for design mat
 rices gives guarantees for optimal recovery in sparse linear models.  It i
 s of high interest in compressed sensing and statistical learning. This pr
 operty is particularly important for computationally efficient recovery me
 thods. As a consequence\, even though it is in general NP-hard to check th
 at RIP holds\, there have been substantial efforts to find tractable proxi
 es for it.  These would allow the construction of RIP matrices and the pol
 ynomial-time verification of RIP given an arbitrary matrix. We consider th
 e framework of average-case certifiers\, that never wrongly declare that a
  matrix is RIP\, while being often correct for random instances. While the
 re are such functions which are tractable in a suboptimal parameter regime
 \, we show that this is a computationally hard task in any better regime. 
  Our results are based on a new\, weaker assumption on the problem of dete
 cting dense subgraphs.
LOCATION:Area 5+6+7+8 #161
END:VEVENT
BEGIN:VEVENT
SUMMARY:Provable Efficient Online Matrix Completion via Non-convex Stochas
 tic Gradient Descent | Chi Jin \, Sham Kakade \, Praneeth Netrapalli
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Provable Efficient Online Matrix Completion via Non-con
 vex Stochastic Gradient Descent\nChi Jin \, Sham Kakade \, Praneeth Netrap
 alli\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7390\n\nMatrix co
 mpletion\, where we wish to recover a low rank matrix by observing a few e
 ntries from it\, is a widely studied problem in both theory and practice w
 ith wide applications. Most of the provable algorithms so far on this prob
 lem have been restricted to the offline setting where they provide an esti
 mate of the unknown matrix using all observations simultaneously. However\
 , in many applications\, the online version\, where we observe one entry a
 t a time and dynamically update our estimate\, is more appealing. While ex
 isting algorithms are efficient for the offline setting\, they could be hi
 ghly inefficient for the online setting.  In this paper\, we propose the f
 irst provable\, efficient online algorithm for matrix completion. Our algo
 rithm starts from an initial estimate of the matrix and then performs non-
 convex stochastic gradient descent (SGD). After every observation\, it per
 forms a fast update involving only one row of two tall matrices\, giving n
 ear linear total runtime. Our algorithm can be naturally used in the offli
 ne setting as well\, where it gives competitive sample complexity and runt
 ime to state of the art algorithms. Our proofs introduce a general framewo
 rk to show that SGD updates tend to stay away from saddle surfaces and cou
 ld be of broader interests to other non-convex problems.
LOCATION:Area 5+6+7+8 #162
END:VEVENT
BEGIN:VEVENT
SUMMARY:Infinite Hidden Semi-Markov Modulated Interaction Point Process | 
 matt zhang \, Peng Lin \, Peng Lin \, Ting Guo \, Yang Wang \, Yang Wang \
 , Fang Chen
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Infinite Hidden Semi-Markov Modulated Interaction Point
  Process\nmatt zhang \, Peng Lin \, Peng Lin \, Ting Guo \, Yang Wang \, Y
 ang Wang \, Fang Chen\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=
 7391\n\nThe correlation between events is ubiquitous and important for tem
 poral events modelling. In many cases\, the correlation exists between not
  only events' emitted observations\, but also their arrival times. State s
 pace models (e.g.\, hidden Markov model) and stochastic interaction point 
 process models (e.g.\, Hawkes process) have been studied extensively yet s
 eparately for the two types of correlations in the past. In this paper\, w
 e propose a Bayesian nonparametric approach that considers both types of c
 orrelations via unifying and generalizing hidden semi-Markov model and int
 eraction point process model. The proposed approach can simultaneously mod
 el both the observations and arrival times of temporal events\, and determ
 ine the number of latent states from data. A Metropolis-within-particle-Gi
 bbs sampler with ancestor resampling is developed for efficient posterior 
 inference. The approach is tested on both synthetic and real-world data wi
 th promising outcomes.
LOCATION:Area 5+6+7+8 #163
END:VEVENT
BEGIN:VEVENT
SUMMARY:Linear Contextual Bandits with Knapsacks | Shipra Agrawal \, Nikhi
 l Devanur
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Linear Contextual Bandits with Knapsacks\nShipra Agrawa
 l \, Nikhil Devanur\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=73
 92\n\nWe consider the linear contextual bandit problem with resource consu
 mption\, in addition to reward generation. In each round\, the outcome of 
 pulling an arm is a reward as well as a vector of resource consumptions. T
 he expected values of these outcomes depend linearly on the context of tha
 t arm. The budget/capacity constraints require that the sum of these vecto
 rs doesn't exceed the budget in each dimension. The objective is once agai
 n to maximize the total reward. This problem turns out to be a common gene
 ralization of classic linear contextual bandits  (linContextual)\,  bandit
 s with knapsacks (BwK)\, and the online stochastic packing problem (OSPP).
  We present algorithms with near-optimal regret bounds for this problem. O
 ur bounds compare favorably to results on the unstructured version of the 
 problem\, where the relation between the contexts and the outcomes could b
 e arbitrary\, but the algorithm only competes against a fixed set of polic
 ies accessible through  an optimization oracle. We combine techniques from
  the work on linContextual\, BwK and OSPP in a nontrivial manner while als
 o tackling new difficulties that are not present in any of these special c
 ases.
LOCATION:Area 5+6+7+8 #164
END:VEVENT
BEGIN:VEVENT
SUMMARY:Selective inference for group-sparse linear models | Fan Yang \, R
 ina Foygel Barber \, Prateek Jain \, John Lafferty
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Selective inference for group-sparse linear models\nFan
  Yang \, Rina Foygel Barber \, Prateek Jain \, John Lafferty\nhttp://nips.
 cc/Conferences/2016/Schedule?showEvent=7393\n\nWe develop tools for select
 ive inference in the setting of group sparsity\, including the constructio
 n of confidence intervals and p-values for testing selected groups of vari
 ables. Our main technical result gives the precise distribution of the mag
 nitude of the projection of the data onto a given subspace\, and enables u
 s to develop inference procedures for a broad class of group-sparse select
 ion methods\, including the group lasso\, iterative hard thresholding\, an
 d forward stepwise regression. We give numerical results to illustrate the
 se tools on simulated data and on health record data.
LOCATION:Area 5+6+7+8 #165
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Neural Networks with Inexact Matching for Person Re-Identific
 ation | Arulkumar Subramaniam \, Moitreya Chatterjee \, Anurag Mittal
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Deep Neural Networks with Inexact Matching for Person R
 e-Identification\nArulkumar Subramaniam \, Moitreya Chatterjee \, Anurag M
 ittal\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7394\n\nPerson R
 e-Identification is the task of matching images of a person across multipl
 e camera views. Almost all prior approaches address this challenge by atte
 mpting to learn the possible transformations that relate the different vie
 ws of a person from a training corpora. Then\, they utilize these transfor
 mation patterns for matching a query image to those in a gallery image ban
 k at test time. This necessitates learning good feature representations of
  the images and having a robust feature matching technique. Deep learning 
 approaches\, such as Convolutional Neural Networks (CNN)\, simultaneously 
 do both and have shown great promise recently. In this work\, we propose t
 wo CNN-based architectures for Person Re-Identification. In the first\, gi
 ven a pair of images\, we extract feature maps from these images via multi
 ple stages of convolution and pooling. A novel inexact matching technique 
 then matches pixels in the first representation with those of the second. 
 Furthermore\, we search across a wider region in the second representation
  for matching. Our novel matching technique allows us to tackle the challe
 nges posed by large viewpoint variations\, illumination changes or partial
  occlusions. Our approach shows a promising performance and requires only 
 about half the parameters as a current state-of-the-art technique. Nonethe
 less\, it also suffers from false matches at times. In order to mitigate t
 his issue\, we propose a fused architecture that combines our inexact matc
 hing pipeline with a state-of-the-art exact matching technique. We observe
  substantial gains with the fused model over the current state-of-the-art 
 on multiple challenging datasets of varying sizes\, with gains of up to ab
 out 21%.
LOCATION:Area 5+6+7+8 #166
END:VEVENT
BEGIN:VEVENT
SUMMARY:Accelerating Stochastic Composition Optimization | Mengdi Wang \, 
 Ji Liu \, Ethan Fang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Accelerating Stochastic Composition Optimization\nMengd
 i Wang \, Ji Liu \, Ethan Fang\nhttp://nips.cc/Conferences/2016/Schedule?s
 howEvent=7395\n\nConsider the stochastic composition optimization problem 
 where the objective is a composition of two expected-value functions. We p
 ropose a new stochastic first-order method\, namely the accelerated stocha
 stic compositional proximal gradient (ASC-PG) method\, which updates based
  on queries to the sampling oracle using two different timescales. The ASC
 -PG is the first proximal gradient method for the stochastic composition p
 roblem that can deal with nonsmooth regularization penalty. We show that t
 he ASC-PG exhibits faster convergence than the best known algorithms\, and
  that it achieves the optimal sample-error complexity in several important
  special cases. We further demonstrate the application of ASC-PG to reinfo
 rcement learning  and conduct numerical experiments.
LOCATION:Area 5+6+7+8 #167
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning Bound for Parameter Transfer Learning | Wataru Kumagai
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning Bound for Parameter Transfer Learning\nWataru 
 Kumagai\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7396\n\nWe con
 sider a transfer-learning problem by using the parameter transfer approach
 \, where a suitable parameter of feature mapping is learned through one ta
 sk and applied to another objective task. Then\, we introduce the notion o
 f the local stability of parametric feature mapping and  parameter transfe
 r learnability\, and thereby derive a learning bound for parameter transfe
 r algorithms. As an application of parameter transfer learning\, we discus
 s the performance of sparse coding in self-taught learning. Although self-
 taught learning algorithms with plentiful unlabeled data often show excell
 ent empirical performance\, their theoretical analysis has not been studie
 d. In this paper\, we also provide the first theoretical learning bound fo
 r self-taught learning.
LOCATION:Area 5+6+7+8 #168
END:VEVENT
BEGIN:VEVENT
SUMMARY:Can Active Memory Replace Attention? | Łukasz Kaiser \, Samy Beng
 io
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Can Active Memory Replace Attention?\nŁukasz Kaiser \,
  Samy Bengio\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7397\n\nS
 everal mechanisms to focus attention of a neural network on selected parts
  of its input or memory have been used successfully in deep learning model
 s in recent years. Attention has improved image classification\, image cap
 tioning\, speech recognition\, generative models\, and learning algorithmi
 c tasks\, but it had probably the largest impact on neural machine transla
 tion.  Recently\, similar improvements have been obtained using  alternati
 ve mechanisms that do not focus on a single part of a memory but operate o
 n all of it in parallel\, in a uniform way. Such mechanism\, which we call
  active memory\, improved over attention in algorithmic tasks\, image proc
 essing\, and in generative modelling.  So far\, however\, active memory ha
 s not improved over attention for most natural language processing tasks\,
  in particular for machine translation. We analyze this shortcoming in thi
 s paper and propose an extended model of active memory that matches existi
 ng attention models on neural machine translation and generalizes better t
 o longer sentences. We investigate this model and explain why previous act
 ive memory models did not succeed. Finally\, we discuss when active memory
  brings most benefits and where attention can be a better choice.
LOCATION:Area 5+6+7+8 #169
END:VEVENT
BEGIN:VEVENT
SUMMARY:Understanding the Effective Receptive Field in Deep Convolutional 
 Neural Networks | Wenjie Luo \, Yujia Li \, Raquel Urtasun \, Richard Zeme
 l
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Understanding the Effective Receptive Field in Deep Con
 volutional Neural Networks\nWenjie Luo \, Yujia Li \, Raquel Urtasun \, Ri
 chard Zemel\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7398\n\nWe
  study characteristics of receptive fields of units in deep convolutional 
 networks. The receptive field size is a crucial issue in many visual tasks
 \, as the output must respond to large enough areas in the image to captur
 e information about large objects. We introduce the notion of an effective
  receptive field size\, and show that it both has a Gaussian distribution 
 and   only occupies a fraction of the full theoretical receptive field siz
 e. We analyze the effective receptive field in several architecture design
 s\, and the effect of sub-sampling\, skip connections\, dropout and nonlin
 ear activations on it. This leads to suggestions for ways to address its t
 endency to be too small.
LOCATION:Area 5+6+7+8 #170
END:VEVENT
BEGIN:VEVENT
SUMMARY:Local Similarity-Aware Deep Feature Embedding | Chen Huang \, Chen
  Change Loy \, Xiaoou Tang
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Local Similarity-Aware Deep Feature Embedding\nChen Hua
 ng \, Chen Change Loy \, Xiaoou Tang\nhttp://nips.cc/Conferences/2016/Sche
 dule?showEvent=7399\n\nExisting deep embedding methods in vision tasks are
  capable of learning a compact Euclidean space from images\, where Euclide
 an distances correspond to a similarity metric. To make learning more effe
 ctive and efficient\, hard sample mining is usually employed\, with sample
 s identified through computing the Euclidean feature distance. However\, t
 he global Euclidean distance cannot faithfully characterize the true featu
 re similarity in a complex visual feature space\, where the intraclass dis
 tance in a high-density region may be larger than the interclass distance 
 in low-density regions. In this paper\, we introduce a Position-Dependent 
 Deep Metric (PDDM) unit\, which is capable of learning a similarity metric
  adaptive to local feature structure. The metric can be used to select gen
 uinely hard samples in a local neighborhood to guide the deep embedding le
 arning in an online and robust manner. The new layer is appealing in that 
 it is pluggable to any convolutional networks and is trained end-to-end. O
 ur local similarity-aware feature embedding not only demonstrates faster c
 onvergence and boosted performance on two complex image retrieval datasets
 \, its large margin nature also leads to superior generalization results u
 nder the large and open set scenarios of transfer learning and zero-shot l
 earning on ImageNet 2010 and ImageNet-10K datasets.
LOCATION:Area 5+6+7+8 #171
END:VEVENT
BEGIN:VEVENT
SUMMARY:End-to-End Kernel Learning with Supervised Convolutional Kernel Ne
 tworks | Julien Mairal
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:End-to-End Kernel Learning with Supervised Convolutiona
 l Kernel Networks\nJulien Mairal\nhttp://nips.cc/Conferences/2016/Schedule
 ?showEvent=7400\n\nIn this paper\, we introduce a new image representation
  based on a multilayer kernel machine. Unlike traditional kernel methods w
 here data representation is decoupled from the prediction task\, we learn 
 how to shape the kernel with supervision. We proceed by first proposing im
 provements of the recently-introduced convolutional kernel networks (CKNs)
  in the context of unsupervised learning\; then\, we derive backpropagatio
 n rules to take advantage of labeled training data. The resulting model is
  a new type of convolutional neural network\, where optimizing the filters
  at each layer is equivalent to learning a linear subspace in a reproducin
 g kernel Hilbert space (RKHS). We show that our method achieves reasonably
  competitive performance for image classification on some standard ``deep 
 learning'' datasets such as CIFAR-10 and SVHN\, and also for image super-r
 esolution\, demonstrating the applicability of our approach to a large var
 iety of image-related tasks.
LOCATION:Area 5+6+7+8 #172
END:VEVENT
BEGIN:VEVENT
SUMMARY:Single-Image Depth Perception in the Wild | Weifeng Chen \, Zhao F
 u \, Dawei Yang \, Jia Deng
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Single-Image Depth Perception in the Wild\nWeifeng Chen
  \, Zhao Fu \, Dawei Yang \, Jia Deng\nhttp://nips.cc/Conferences/2016/Sch
 edule?showEvent=7401\n\nThis paper studies single-image depth perception i
 n the wild\, i.e.\, recovering depth from a single image taken in unconstr
 ained settings. We introduce a new dataset “Depth in the Wild” consist
 ing of images in the wild annotated with relative depth between pairs of r
 andom points. We also propose a new algorithm that learns to estimate metr
 ic depth using annotations of relative depth. Compared to the state of the
  art\, our algorithm is simpler and performs better. Experiments show that
  our algorithm\, combined with existing RGB-D data and our new relative de
 pth annotations\, significantly improves single-image depth perception in 
 the wild.
LOCATION:Area 5+6+7+8 #173
END:VEVENT
BEGIN:VEVENT
SUMMARY:Toward Deeper Understanding of Neural Networks: The Power of Initi
 alization and a Dual View on Expressivity | Amit Daniely \, Roy Frostig \,
  Yoram Singer
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Toward Deeper Understanding of Neural Networks: The Pow
 er of Initialization and a Dual View on Expressivity\nAmit Daniely \, Roy 
 Frostig \, Yoram Singer\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7402\n\nWe develop a general duality between neural networks and composi
 tional kernel Hilbert spaces. We introduce the notion of a computation ske
 leton\, an acyclic graph that succinctly describes both a family of neural
  networks and a kernel space. Random neural networks are generated from a 
 skeleton through node replication followed by sampling from a normal distr
 ibution to assign weights. The kernel space consists of functions that ari
 se by compositions\, averaging\, and non-linear transformations governed b
 y the skeleton's graph topology and activation functions. We prove that ra
 ndom networks induce representations which approximate the kernel space. I
 n particular\, it follows that random weight initialization often yields a
  favorable starting point for optimization despite the worst-case intracta
 bility of training neural networks.
LOCATION:Area 5+6+7+8 #174
END:VEVENT
BEGIN:VEVENT
SUMMARY:Without-Replacement Sampling for Stochastic Gradient Methods | Oha
 d Shamir
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Without-Replacement Sampling for Stochastic Gradient Me
 thods\nOhad Shamir\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=740
 5\n\nStochastic gradient methods for machine learning and optimization pro
 blems are usually analyzed assuming data points are sampledwithreplacement
 . In contrast\, samplingwithoutreplacement is far less understood\, yet in
  practice it is very common\, often easier to implement\, and usually perf
 orms better. In this paper\, we provide competitive convergence guarantees
  for without-replacement sampling under several scenarios\, focusing on th
 e natural regime of few passes over the data. Moreover\, we describe a use
 ful application of these results in the context of distributed optimizatio
 n with randomly-partitioned data\, yielding a nearly-optimal algorithm for
  regularized least squares (in terms of both communication complexity and 
 runtime complexity) under broad parameter regimes. Our proof techniques co
 mbine ideas from stochastic optimization\, adversarial online learning and
  transductive learning theory\, and can potentially be applied to other st
 ochastic optimization and learning problems.
LOCATION:Area 5+6+7+8 #174
END:VEVENT
BEGIN:VEVENT
SUMMARY:Probabilistic Modeling of Future Frames from a Single Image | Tian
 fan Xue \, Jiajun Wu \, Katherine Bouman \, Bill Freeman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Probabilistic Modeling of Future Frames from a Single I
 mage\nTianfan Xue \, Jiajun Wu \, Katherine Bouman \, Bill Freeman\nhttp:/
 /nips.cc/Conferences/2016/Schedule?showEvent=7406\n\nWe study the problem 
 of synthesizing a number of likely future frames from a single input image
 . In contrast to traditional methods\, which have tackled this problem in 
 a deterministic or non-parametric way\, we propose a novel approach which 
 models future frames in a probabilistic manner. Our proposed method is the
 refore able to synthesize multiple possible next frames using the same mod
 el. Solving this challenging problem involves low- and high-level image an
 d motion understanding for successful image synthesis. Here\, we propose a
  novel network structure\, namely a Cross Convolutional Network\, that enc
 odes images as feature maps and motion information as convolutional kernel
 s to aid in synthesizing future frames. In experiments\, our model perform
 s well on both synthetic data\, such as 2D shapes and animated game sprite
 s\, as well as on real-wold video data. We show that our model can also be
  applied to tasks such as visual analogy-making\, and present analysis of 
 the learned network representations.
LOCATION:Area 5+6+7+8 #175
END:VEVENT
BEGIN:VEVENT
SUMMARY:R-FCN: Object Detection via Region-based Fully Convolutional Netwo
 rks | jifeng dai \, Yi Li \, Kaiming He \, Jian Sun
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:R-FCN: Object Detection via Region-based Fully Convolut
 ional Networks\njifeng dai \, Yi Li \, Kaiming He \, Jian Sun\nhttp://nips
 .cc/Conferences/2016/Schedule?showEvent=7403\n\nWe present region-based\, 
 fully convolutional networks for accurate and efficient object detection. 
 In contrast to previous region-based detectors such as Fast/Faster R-CNN t
 hat apply a costly per-region subnetwork hundreds of times\, our region-ba
 sed detector is fully convolutional with almost all computation shared on 
 the entire image. To achieve this goal\, we propose position-sensitive sco
 re maps to address a dilemma between translation-invariance in image class
 ification and translation-variance in object detection. Our method can thu
 s naturally adopt fully convolutional image classifier backbones\, such as
  the latest Residual Networks (ResNets)\, for object detection. We show co
 mpetitive results on the PASCAL VOC datasets (e.g.\, 83.6% mAP on the 2007
  set) with the 101-layer ResNet. Meanwhile\, our result is achieved at a t
 est-time speed of 170ms per image\, 2.5-20 times faster than the Faster R-
 CNN counterpart. Code is made publicly available at: https://github.com/da
 ijifeng001/r-fcn.
LOCATION:Area 5+6+7+8 #175
END:VEVENT
BEGIN:VEVENT
SUMMARY:Consistent Estimation of Functions of Data Missing Non-Monotonical
 ly and Not at Random | Ilya Shpitser
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Consistent Estimation of Functions of Data Missing Non-
 Monotonically and Not at Random\nIlya Shpitser\nhttp://nips.cc/Conferences
 /2016/Schedule?showEvent=7404\n\nMissing records are a perennial problem i
 n analysis of complex data of all types\, when the target of inference is 
 some function of the full data law.   In simple cases\, where data is miss
 ing at random or completely at random (Rubin\, 1976)\, well-known adjustme
 nts exist that result in consistent estimators of target quantities.  Assu
 mptions underlying these estimators are generally not realistic in practic
 al missing data problems.  Unfortunately\, consistent estimators in more c
 omplex cases where data is missing not at random\, and where no ordering o
 n variables induces monotonicity of missingness status are not known in ge
 neral\, with some notable exceptions (Robins\, 1997)\, (Tchetgen Tchetgen 
 et al\, 2016)\, (Sadinle and Reiter\, 2016).  In this paper\, we propose a
  general class of consistent estimators for cases where data is missing no
 t at random\, and missingness status is non-monotonic.  Our estimators\, w
 hich are generalized inverse probability weighting estimators\, make no as
 sumptions on the underlying full data law\, but instead place independence
  restrictions\, and certain other fairly mild assumptions\, on the distrib
 ution of missingness status conditional on the data.  The assumptions we p
 lace on the distribution of missingness status conditional on the data  ca
 n be viewed as a version of a conditional Markov random field (MRF) corres
 ponding to a chain graph.  Assumptions embedded in our model permit identi
 fication from the observed data law\, and admit a natural fitting procedur
 e based on the pseudo likelihood approach of (Besag\, 1975).  We illustrat
 e our approach with a simple simulation study\, and an analysis of risk of
  premature birth in women in Botswana exposed to highly active anti-retrov
 iral therapy.
LOCATION:Area 5+6+7+8 #176
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning What and Where to Draw | Scott E Reed \, Zeynep Akata \, 
 Santosh Mohan \, Samuel Tenka \, Bernt Schiele \, Honglak Lee
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning What and Where to Draw\nScott E Reed \, Zeynep
  Akata \, Santosh Mohan \, Samuel Tenka \, Bernt Schiele \, Honglak Lee\nh
 ttp://nips.cc/Conferences/2016/Schedule?showEvent=7407\n\nGenerative Adver
 sarial Networks (GANs) have recently demonstrated the capability to synthe
 size compelling real-world images\, such as room interiors\, album covers\
 , manga\, faces\, birds\, and flowers. While existing models can synthesiz
 e images based on global constraints such as a class label or caption\, th
 ey do not provide control over pose or object location. We propose a new m
 odel\, the Generative Adversarial What-Where Network (GAWWN)\, that synthe
 sizes images given instructions describing what content to draw in which l
 ocation. We show high-quality 128 × 128 image synthesis on the Caltech-UC
 SD Birds dataset\, conditioned on both informal text descriptions and also
  object location. Our system exposes control over both the bounding box ar
 ound the bird and its constituent parts. By modeling the conditional distr
 ibutions over part locations\, our system also enables conditioning on arb
 itrary subsets of parts (e.g. only the beak and tail)\, yielding an effici
 ent interface for picking part locations.
LOCATION:Area 5+6+7+8 #176
END:VEVENT
BEGIN:VEVENT
SUMMARY:Stochastic Online AUC Maximization | Yiming Ying \, Longyin Wen \,
  Siwei Lyu
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Stochastic Online AUC Maximization\nYiming Ying \, Long
 yin Wen \, Siwei Lyu\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7
 408\n\nArea under ROC (AUC) is a metric which is widely used for measuring
  the classification performance for imbalanced data. It is of theoretical 
 and practical interest to develop online learning algorithms that maximize
 s AUC for large-scale data. A specific challenge in developing online AUC 
 maximization algorithm is that the learning objective function is usually 
 defined over a pair of training examples of opposite classes\, and existin
 g methods achieves on-line processing with higher space and time complexit
 y. In this work\, we propose a new stochastic online algorithm for AUC max
 imization. In particular\, we show that AUC optimization can  be equivalen
 tly formulated as a convex-concave saddle point problem. From this saddle 
 representation\, a stochastic online algorithm (SOLAM) is proposed which h
 as time and space complexity of one datum. We establish theoretical conver
 gence of SOLAM with high probability and demonstrate its effectiveness and
  efficiency on standard benchmark datasets.
LOCATION:Area 5+6+7+8 #177
END:VEVENT
BEGIN:VEVENT
SUMMARY:Deep Learning without Poor Local Minima | Kenji Kawaguchi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Deep Learning without Poor Local Minima\nKenji Kawaguch
 i\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7409\n\nIn this pape
 r\, we prove a conjecture published in 1989 and also partially address an 
 open problem announced at the Conference on Learning Theory (COLT) 2015. F
 or an expected loss function of a deep nonlinear neural network\, we prove
  the following statements under the independence assumption adopted from r
 ecent work: 1) the function is non-convex and non-concave\, 2) every local
  minimum is a global minimum\, 3) every critical point that is not a globa
 l minimum is a saddle point\, and 4) the property of saddle points differs
  for shallow networks (with three layers) and deeper networks (with more t
 han three layers). Moreover\, we prove that the same four statements hold 
 for deep linear neural networks with any depth\, any widths and no unreali
 stic assumptions. As a result\, we present an instance\, for which we can 
 answer to the following question: how difficult to directly train a deep m
 odel in theory? It is more difficult than the classical machine learning m
 odels (because of the non-convexity)\, but not too difficult (because of t
 he nonexistence of poor local minima and the property of the saddle points
 ). We note that even though we have advanced the theoretical foundations o
 f deep learning\, there is still a gap between theory and practice.
LOCATION:Area 5+6+7+8 #178
END:VEVENT
BEGIN:VEVENT
SUMMARY:Regularized Nonlinear Acceleration | Damien Scieur \, Alexandre d'
 Aspremont \, Francis Bach
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Regularized Nonlinear Acceleration\nDamien Scieur \, Al
 exandre d'Aspremont \, Francis Bach\nhttp://nips.cc/Conferences/2016/Sched
 ule?showEvent=7410\n\nWe describe a convergence acceleration technique for
  generic optimization problems. Our scheme computes estimates of the optim
 um from a nonlinear average of the iterates produced by any optimization m
 ethod. The weights in this average are computed via a simple and small lin
 ear system\, whose solution can be updated online. This acceleration schem
 e runs in parallel to the base algorithm\, providing improved estimates of
  the solution on the fly\, while the original optimization method is runni
 ng. Numerical experiments are detailed on classical classification problem
 s.
LOCATION:Area 5+6+7+8 #179
END:VEVENT
BEGIN:VEVENT
SUMMARY:Learning to Poke by Poking: Experiential Learning of Intuitive Phy
 sics | Pulkit Agrawal \, Ashvin V Nair \, Pieter Abbeel \, Jitendra Malik 
 \, Sergey Levine
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Learning to Poke by Poking: Experiential Learning of In
 tuitive Physics\nPulkit Agrawal \, Ashvin V Nair \, Pieter Abbeel \, Jiten
 dra Malik \, Sergey Levine\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7411\n\nWe investigate an experiential learning paradigm for acquirin
 g an internal model of intuitive physics. Our model is evaluated on a real
 -world robotic manipulation task that requires displacing objects to targe
 t locations by poking. The robot gathered over 400 hours of experience by 
 executing more than 50K pokes on different objects. We propose a novel app
 roach based on deep neural networks for modeling the dynamics of robot's i
 nteractions directly from images\, by jointly estimating forward and inver
 se models of dynamics. The inverse model objective provides supervision to
  construct informative visual features\, which the forward model can then 
 predict and in turn regularize the feature space for the inverse model. Th
 e interplay between these two objectives creates useful\, accurate models 
 that can then be used for multi-step decision making. This formulation has
  the additional benefit that it is possible to learn forward models in an 
 abstract feature space and thus alleviate the need of predicting pixels. O
 ur experiments show that this joint modeling approach outperforms alternat
 ive methods. We also demonstrate that active data collection using the lea
 rned model further improves performance.
LOCATION:Area 5+6+7+8 #180
END:VEVENT
BEGIN:VEVENT
SUMMARY:Weight Normalization: A Simple Reparameterization to Accelerate Tr
 aining of Deep Neural Networks | Tim Salimans \, Diederik P Kingma
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Weight Normalization: A Simple Reparameterization to Ac
 celerate Training of Deep Neural Networks\nTim Salimans \, Diederik P King
 ma\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7412\n\nWe present 
 weight normalization: a reparameterization of the weight vectors in a neur
 al network that decouples the length of those weight vectors from their di
 rection. By reparameterizing the weights in this way we improve the condit
 ioning of the optimization problem and we speed up convergence of stochast
 ic gradient descent. Our reparameterization is inspired by batch normaliza
 tion but does not introduce any dependencies between the examples in a min
 ibatch. This means that our method can also be applied successfully to rec
 urrent models such as LSTMs and to noise-sensitive applications such as de
 ep reinforcement learning or generative models\, for which batch normaliza
 tion is less well suited. Although our method is much simpler\, it still p
 rovides much of the speed-up of full batch normalization. In addition\, th
 e computational overhead of our method is lower\, permitting more optimiza
 tion steps to be taken in the same amount of time. We demonstrate the usef
 ulness of our method on applications in supervised image recognition\, gen
 erative modelling\, and deep reinforcement learning.
LOCATION:Area 5+6+7+8 #181
END:VEVENT
BEGIN:VEVENT
SUMMARY:Linear-Memory and Decomposition-Invariant Linearly Convergent Cond
 itional Gradient Algorithm for Structured Polytopes | Dan Garber \, Dan Ga
 rber \, Ofer Meshi
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Linear-Memory and Decomposition-Invariant Linearly Conv
 ergent Conditional Gradient Algorithm for Structured Polytopes\nDan Garber
  \, Dan Garber \, Ofer Meshi\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7413\n\nRecently\, several works have shown that natural modificati
 ons of the classical conditional gradient method (aka Frank-Wolfe algorith
 m) for constrained convex optimization\, provably converge with a linear r
 ate when the feasible set is a polytope\, and the objective is smooth and 
 strongly-convex. However\, all of these results suffer from two significan
 t shortcomings: i) large memory requirement due to the need to store an ex
 plicit convex decomposition of the current iterate\, and as a consequence\
 , large running-time overhead per iteration ii) the worst case convergence
  rate depends unfavorably on the dimension In this work we present a new c
 onditional gradient variant and a corresponding analysis that improves on 
 both of the above shortcomings. In particular\, both memory and computatio
 n overheads are only linear in the dimension\, and in addition\, in case t
 he optimal solution is sparse\, the new convergence rate replaces a factor
  which is at least linear in the dimension in previous works\, with a line
 ar dependence on the number of non-zeros in the optimal solution At the he
 art of our method\, and corresponding analysis\, is a novel way to compute
  decomposition-invariant away-steps. While our theoretical guarantees do n
 ot apply to any polytope\, they apply to several important structured poly
 topes that capture central concepts such as paths in graphs\, perfect matc
 hings in bipartite graphs\, marginal distributions that arise in structure
 d prediction tasks\, and more. Our theoretical findings are complemented b
 y empirical evidence that shows that our method delivers state-of-the-art 
 performance.
LOCATION:Area 5+6+7+8 #182
END:VEVENT
BEGIN:VEVENT
SUMMARY:Achieving the KS threshold in the general stochastic block model w
 ith linearized acyclic belief propagation | Emmanuel Abbe \, Colin Sandon
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Achieving the KS threshold in the general stochastic bl
 ock model with linearized acyclic belief propagation\nEmmanuel Abbe \, Col
 in Sandon\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7414\n\nThe 
 stochastic block model (SBM) has long been studied in machine learning and
  network science as a canonical model for clustering and community detecti
 on. In the recent years\, new developments have demonstrated the presence 
 of threshold phenomena for this model\, which have set new challenges for 
 algorithms. For the {\\it detection} problem in symmetric SBMs\, Decelle e
 t al.\\ conjectured that the so-called Kesten-Stigum (KS) threshold can be
  achieved efficiently. This was proved for two communities\, but remained 
 open from three communities. We prove this conjecture here\, obtaining a m
 ore general result that applies to arbitrary SBMs with linear size communi
 ties. The developed algorithm is a linearized acyclic belief propagation (
 ABP) algorithm\, which mitigates the effects of cycles while provably achi
 eving the KS threshold in $O(n \\ln n)$ time. This extends prior methods b
 y achieving universally the KS threshold while reducing or preserving the 
 computational complexity. ABP is also connected to a power iteration metho
 d on a generalized nonbacktracking operator\, formalizing the spectral-mes
 sage passing interplay described in Krzakala et al.\, and extending result
 s from Bordenave et al.
LOCATION:Area 5+6+7+8 #183
END:VEVENT
BEGIN:VEVENT
SUMMARY:Orthogonal Random Features | Felix X Yu \, Ananda Theertha Suresh 
 \, Krzysztof M Choromanski \, Daniel N Holtmann-Rice \, Sanjiv Kumar
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Orthogonal Random Features\nFelix X Yu \, Ananda Theert
 ha Suresh \, Krzysztof M Choromanski \, Daniel N Holtmann-Rice \, Sanjiv K
 umar\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7415\n\nWe presen
 t an intriguing discovery related to Random Fourier Features: replacing mu
 ltiplication by a random Gaussian matrix with multiplication by a properly
  scaled random orthogonal matrix significantly decreases kernel approximat
 ion error. We call this technique Orthogonal Random Features (ORF)\, and p
 rovide theoretical and empirical justification for its effectiveness. Moti
 vated by the discovery\, we further propose Structured Orthogonal Random F
 eatures (SORF)\, which uses a class of structured discrete orthogonal matr
 ices to speed up the computation. The method reduces the time cost from $\
 \mathcal{O}(d^2)$ to $\\mathcal{O}(d \\log d)$\, where $d$ is the data dim
 ensionality\, with almost no compromise in kernel approximation quality co
 mpared to ORF. Experiments on several datasets verify the effectiveness of
  ORF and SORF over the existing methods. We also provide discussions on us
 ing the same type of discrete orthogonal structure for a broader range of 
 kernels and applications.
LOCATION:Area 5+6+7+8 #184
END:VEVENT
BEGIN:VEVENT
SUMMARY:Universal Correspondence Network | Christopher B Choy \, Manmohan 
 Chandraker \, JunYoung Gwak \, Silvio Savarese
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Universal Correspondence Network\nChristopher B Choy \,
  Manmohan Chandraker \, JunYoung Gwak \, Silvio Savarese\nhttp://nips.cc/C
 onferences/2016/Schedule?showEvent=7416\n\nWe present a deep learning fram
 ework for accurate visual correspondences and demonstrate its effectivenes
 s for both geometric and semantic matching\, spanning across rigid motions
  to intra-class shape or appearance variations. In contrast to previous CN
 N-based approaches that optimize a surrogate patch similarity objective\, 
 we use deep metric learning to directly learn a feature space that preserv
 es either geometric or semantic similarity. Our fully convolutional archit
 ecture\, along with a novel correspondence contrastive loss allows faster 
 training by effective reuse of computations\, accurate gradient computatio
 n through the use of thousands of examples per image pair and faster testi
 ng with $O(n)$ feedforward passes for n keypoints\, instead of $O(n^2)$ fo
 r typical patch similarity methods. We propose a convolutional spatial tra
 nsformer to mimic patch normalization in traditional features like SIFT\, 
 which is shown to dramatically boost accuracy for semantic correspondences
  across intra-class shape variations. Extensive experiments on KITTI\, PAS
 CAL and CUB-2011 datasets demonstrate the significant advantages of our fe
 atures over prior works that use either hand-constructed or learned featur
 es.
LOCATION:Area 5+6+7+8 #185
END:VEVENT
BEGIN:VEVENT
SUMMARY:The Multiscale Laplacian Graph Kernel | Risi Kondor \, Horace Pan
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:The Multiscale Laplacian Graph Kernel\nRisi Kondor \, H
 orace Pan\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7417\n\nMany
  real world graphs\, such as the graphs of molecules\, exhibit structure a
 t multiple different scales\, but most existing kernels between graphs are
  either purely local or purely global in character. In contrast\, by build
 ing a hierarchy of nested subgraphs\, the Multiscale Laplacian Graph kerne
 ls (MLG kernels) that we define in this paper can account for structure at
  a range of different scales. At the heart of the MLG construction is anot
 her new graph kernel\, called the Feature Space Laplacian Graph kernel (FL
 G kernel)\, which has the property that it can lift a base kernel defined 
 on the vertices of two graphs to a kernel between the graphs. The MLG kern
 el applies such FLG kernels to subgraphs recursively. To make the MLG kern
 el computationally feasible\, we also introduce a randomized projection pr
 ocedure\, similar to the Nystro ̈m method\, but for RKHS operators.
LOCATION:Area 5+6+7+8 #186
END:VEVENT
BEGIN:VEVENT
SUMMARY:Generalization of ERM in Stochastic Convex Optimization: The Dimen
 sion Strikes Back | Vitaly Feldman
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Generalization of ERM in Stochastic Convex Optimization
 : The Dimension Strikes Back\nVitaly Feldman\nhttp://nips.cc/Conferences/2
 016/Schedule?showEvent=7418\n\nIn stochastic convex optimization the goal 
 is to minimize a convex function $F(x) \\doteq \\E{f\\sim D}[f(x)]$ over a
  convex set $\\K \\subset \\R^d$ where $D$ is some unknown distribution an
 d each $f(\\cdot)$ in the support of $D$ is convex over $\\K$. The optimiz
 ation is based on i.i.d.~samples $f^1\,f^2\,\\ldots\,f^n$ from $D$. A comm
 on approach to such problems is empirical risk minimization (ERM) that opt
 imizes $FS(x) \\doteq \\frac{1}{n}\\sum{i\\leq n} f^i(x)$. Here we conside
 r the question of how many samples are necessary for ERM to succeed and th
 e closely related question of uniform convergence of $FS$ to $F$ over $\\K
 $. We demonstrate that in the standard $\\ellp/\\ellq$ setting of Lipschit
 z-bounded functions over a $\\K$ of bounded radius\, ERM requires sample s
 ize that scales linearly with the dimension $d$. This nearly matches stand
 ard upper bounds and improves on $\\Omega(\\log d)$ dependence proved for 
 $\\ell2/\\ell2$ setting in (Shalev-Shwartz et al.  2009). In stark contras
 t\, these problems can be solved using dimension-independent number of sam
 ples for $\\ell2/\\ell2$ setting and $\\log d$ dependence for $\\ell1/\\el
 l\\infty$ setting using other approaches. We also demonstrate that for a m
 ore general class of range-bounded (but not Lipschitz-bounded) stochastic 
 convex programs an even stronger gap appears already in dimension 2.
LOCATION:Area 5+6+7+8 #187
END:VEVENT
BEGIN:VEVENT
SUMMARY:Large-Scale Price Optimization via Network Flow | Shinji Ito \, Ry
 ohei Fujimaki
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Large-Scale Price Optimization via Network Flow\nShinji
  Ito \, Ryohei Fujimaki\nhttp://nips.cc/Conferences/2016/Schedule?showEven
 t=7419\n\nThis paper deals with price optimization\, which is to find the 
 best pricing strategy that maximizes revenue or profit\, on the basis of d
 emand forecasting models. Though recent advances in regression technologie
 s have made it possible to reveal price-demand relationship of a number of
  multiple products\, most existing price optimization methods\, such as mi
 xed integer programming formulation\, cannot handle tens or hundreds of pr
 oducts because of their high computational costs. To cope with this proble
 m\, this paper proposes a novel approach based on network flow algorithms.
  We reveal a connection between supermodularity of the revenue and cross e
 lasticity of demand. On the basis of this connection\, we propose an effic
 ient algorithm that employs network flow algorithms. The proposed algorith
 m can handle hundreds or thousands of products\, and returns an exact opti
 mal solution under an assumption regarding cross elasticity of demand. Eve
 n in case in which the assumption does not hold\, the proposed algorithm c
 an efficiently find approximate solutions as good as can other state-of-th
 e-art methods\, as empirical results show.
LOCATION:Area 5+6+7+8 #188
END:VEVENT
BEGIN:VEVENT
SUMMARY:Bayesian Optimization with Robust Bayesian Neural Networks | Jost 
 Tobias Springenberg \, Aaron Klein \, Stefan Falkner \, Frank Hutter
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Bayesian Optimization with Robust Bayesian Neural Netwo
 rks\nJost Tobias Springenberg \, Aaron Klein \, Stefan Falkner \, Frank Hu
 tter\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7420\n\nBayesian 
 optimization is a prominent method for optimizing expensive to evaluate bl
 ack-box functions that is prominently applied to tuning the hyperparameter
 s of machine learning algorithms. Despite its successes\, the prototypical
  Bayesian optimization approach - using Gaussian process models - does not
  scale well to either many hyperparameters or many function evaluations. A
 ttacking this lack of scalability and flexibility is thus one of the key c
 hallenges of the field. We present a general approach for using flexible p
 arametric models (neural networks) for Bayesian optimization\, staying as 
 close to a truly Bayesian treatment as possible. We obtain scalability thr
 ough stochastic gradient Hamiltonian Monte Carlo\, whose robustness we imp
 rove via a scale adaptation. Experiments including multi-task Bayesian opt
 imization with 21 tasks\, parallel optimization of deep neural networks an
 d deep reinforcement learning show the power and flexibility of this appro
 ach.
LOCATION:Area 5+6+7+8 #189
END:VEVENT
BEGIN:VEVENT
SUMMARY:Protein contact prediction from amino acid co-evolution using conv
 olutional networks for graph-valued images | Vladimir Golkov \, Marcin J S
 kwark \, Antonij Golkov \, Alexey Dosovitskiy \, Thomas Brox \, Jens Meile
 r \, Daniel Cremers
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Protein contact prediction from amino acid co-evolution
  using convolutional networks for graph-valued images\nVladimir Golkov \, 
 Marcin J Skwark \, Antonij Golkov \, Alexey Dosovitskiy \, Thomas Brox \, 
 Jens Meiler \, Daniel Cremers\nhttp://nips.cc/Conferences/2016/Schedule?sh
 owEvent=7421\n\nProteins are the "building blocks of life"\, the most abun
 dant organic molecules\, and the central focus of most areas of biomedicin
 e. Protein structure is strongly related to protein function\, thus struct
 ure prediction is a crucial task on the way to solve many biological quest
 ions. A contact map is a compact representation of the three-dimensional s
 tructure of a protein via the pairwise contacts between the amino acid con
 stituting the protein. We use a convolutional network to calculate protein
  contact maps from inferred statistical coupling between positions in the 
 protein sequence. The input to the network has an image-like structure ame
 nable to convolutions\, but every "pixel" instead of color channels contai
 ns a bipartite undirected edge-weighted graph. We propose several methods 
 for treating such "graph-valued images" in a convolutional network. The pr
 oposed method outperforms state-of-the-art methods by a large margin. It a
 lso allows for a great flexibility with regard to the input data\, which m
 akes it useful for studying a wide range of problems.
LOCATION:Area 5+6+7+8 #190
END:VEVENT
BEGIN:VEVENT
SUMMARY:Supervised Word Mover's Distance | Gao Huang \, Chuan Guo \, Matt 
 J Kusner \, Yu Sun \, Fei Sha \, Kilian Q Weinberger
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Supervised Word Mover's Distance\nGao Huang \, Chuan Gu
 o \, Matt J Kusner \, Yu Sun \, Fei Sha \, Kilian Q Weinberger\nhttp://nip
 s.cc/Conferences/2016/Schedule?showEvent=7422\n\nAccurately measuring the 
 similarity between text documents lies at the core of many real world appl
 ications of machine learning. These include web-search ranking\, document 
 recommendation\, multi-lingual document matching\, and article categorizat
 ion. Recently\, a new document metric\, the word mover's distance (WMD)\, 
 has been proposed with unprecedented results on kNN-based document classif
 ication. The WMD elevates high quality word embeddings to document metrics
  by formulating the distance between two documents as an optimal transport
  problem between the embedded words. However\, the document distances are 
 entirely unsupervised and lack a mechanism to incorporate supervision when
  available. In this paper we propose an efficient technique to learn a sup
 ervised metric\, which we call the Supervised WMD (S-WMD) metric. Our algo
 rithm learns document distances that measure the underlying semantic diffe
 rences between documents by leveraging semantic differences between indivi
 dual words discovered during supervised training. This is achieved with an
  linear transformation of the underlying word embedding space and tailored
  word-specific weights\, learned to minimize the stochastic leave-one-out 
 nearest neighbor classification error on a per-document level. We evaluate
  our metric on eight real-world text classification tasks on which S-WMD c
 onsistently  outperforms almost all of our 26 competitive baselines.
LOCATION:Area 5+6+7+8 #191
END:VEVENT
BEGIN:VEVENT
SUMMARY:Beyond Exchangeability: The Chinese Voting Process | Moontae Lee \
 , Seok Hyun Jin \, David Mimno
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Beyond Exchangeability: The Chinese Voting Process\nMoo
 ntae Lee \, Seok Hyun Jin \, David Mimno\nhttp://nips.cc/Conferences/2016/
 Schedule?showEvent=7423\n\nMany online communities present user-contribute
 d responses\, such as reviews of products and answers to questions. User-p
 rovided helpfulness votes can highlight the most useful responses\, but vo
 ting is a social process that can gain momentum based on the popularity of
  responses and the polarity of existing votes. We propose the Chinese Voti
 ng Process (CVP) which models the evolution of helpfulness votes as a self
 -reinforcing process dependent on position and presentation biases. We eva
 luate this model on Amazon product reviews and more than 80 StackExchange 
 forums\, measuring the intrinsic quality of individual responses and behav
 ioral coefficients of different communities.
LOCATION:Area 5+6+7+8 #192
END:VEVENT
BEGIN:VEVENT
SUMMARY:Poisson-Gamma dynamical systems | Aaron Schein \, Hanna Wallach \,
  Mingyuan Zhou
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Poisson-Gamma dynamical systems\nAaron Schein \, Hanna 
 Wallach \, Mingyuan Zhou\nhttp://nips.cc/Conferences/2016/Schedule?showEve
 nt=7424\n\nThis paper presents a dynamical system based on the Poisson-Gam
 ma construction for sequentially observed multivariate count data.  Inhere
 nt to the model is a novel Bayesian nonparametric prior that ties and shri
 nks parameters in a powerful way. We develop theory about the model's infi
 nite limit and its steady-state.  The model's inductive bias is demonstrat
 ed on a variety of real-world datasets where it is shown to learn interpre
 table structure and have superior predictive performance.
LOCATION:Area 5+6+7+8 #193
END:VEVENT
BEGIN:VEVENT
SUMMARY:Interpretable Distribution Features with Maximum Testing Power | W
 ittawat Jitkrittum \, Zoltán Szabó \, Kacper P Chwialkowski \, Arthur Gr
 etton
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Interpretable Distribution Features with Maximum Testin
 g Power\nWittawat Jitkrittum \, Zoltán Szabó \, Kacper P Chwialkowski \,
  Arthur Gretton\nhttp://nips.cc/Conferences/2016/Schedule?showEvent=7425\n
 \nTwo semimetrics on probability distributions are proposed\, given as the
  sum of differences of expectations of analytic functions evaluated at spa
 tial or frequency locations (i.e\, features). The features are chosen so a
 s to maximize the distinguishability of the distributions\, by optimizing 
 a lower bound on test power for a statistical test using these features. T
 he result is a parsimonious and interpretable indication of how and where 
 two distributions differ locally. An empirical estimate of the test power 
 criterion converges with increasing sample size\, ensuring the quality of 
 the returned features. In real-world benchmarks on high-dimensional text a
 nd image data\, linear-time tests using the proposed semimetrics achieve c
 omparable performance to the state-of-the-art quadratic-time maximum mean 
 discrepancy test\, while returning human-interpretable features that expla
 in the test results.
LOCATION:Area 5+6+7+8 #194
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dense Associative Memory for Pattern Recognition | Dmitry Krotov \
 , John J. Hopfield
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Dense Associative Memory for Pattern Recognition\nDmitr
 y Krotov \, John J. Hopfield\nhttp://nips.cc/Conferences/2016/Schedule?sho
 wEvent=7426\n\nWe propose a model of associative memory having an unusual 
 mathematical structure. Contrary to the standard case\, which works well o
 nly in the limit when the number of stored memories is much smaller than t
 he number of neurons\, our model stores and reliably retrieves many more p
 atterns than the number of neurons in the network.  We propose a simple du
 ality between this dense associative memory and neural networks commonly u
 sed in models of deep learning. On the associative memory side of this dua
 lity\, a family of models that smoothly interpolates between two limiting 
 cases can be constructed.  One limit is referred to as the feature-matchin
 g mode of pattern recognition\, and the other one as the prototype regime.
  On the deep learning side of the duality\, this family corresponds to neu
 ral networks with one hidden layer and various activation functions\, whic
 h transmit the activities of the visible neurons to the hidden layer. This
  family of activation functions includes logistics\, rectified linear unit
 s\, and rectified polynomials of higher degrees. The proposed duality make
 s it possible to apply energy-based intuition from associative memory to a
 nalyze computational properties of neural networks with unusual activation
  functions - the higher rectified polynomials which until now have not bee
 n used for training neural networks. The utility of the dense memories is 
 illustrated for two test cases: the logical gate XOR and the recognition o
 f handwritten digits from the MNIST data set.
LOCATION:Area 5+6+7+8 #195
END:VEVENT
BEGIN:VEVENT
SUMMARY:Relevant sparse codes with variational information bottleneck | Ma
 tthew Chalk \, Olivier Marre \, Gasper Tkacik
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Relevant sparse codes with variational information bott
 leneck\nMatthew Chalk \, Olivier Marre \, Gasper Tkacik\nhttp://nips.cc/Co
 nferences/2016/Schedule?showEvent=7427\n\nIn many applications\, it is des
 irable to extract only the relevant aspects of data. A principled way to d
 o this is the information bottleneck (IB) method\, where one seeks a code 
 that maximises information about  a relevance variable\, Y\, while constra
 ining the information encoded about the original data\, X. Unfortunately h
 owever\, the IB method is computationally demanding when data are high-dim
 ensional and/or non-gaussian. Here we propose an approximate variational s
 cheme for maximising a lower bound on the IB objective\, analogous to vari
 ational EM. Using this method\, we derive an IB algorithm to recover featu
 res that are both relevant and sparse. Finally\, we demonstrate how kernel
 ised versions of the algorithm can be used to address a broad range of pro
 blems with non-linear relation between X and Y.
LOCATION:Area 5+6+7+8 #196
END:VEVENT
BEGIN:VEVENT
SUMMARY:Examples are not enough\, learn to criticize! Criticism for Interp
 retability | Been Kim \, Oluwasanmi Koyejo \, Rajiv Khanna
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Examples are not enough\, learn to criticize! Criticism
  for Interpretability\nBeen Kim \, Oluwasanmi Koyejo \, Rajiv Khanna\nhttp
 ://nips.cc/Conferences/2016/Schedule?showEvent=7428\n\nExample-based expla
 nations are widely used in the effort to improve the interpretability of h
 ighly complex distributions. However\, prototypes alone are rarely suffici
 ent to represent the gist of the complexity. In order for users to constru
 ct better mental models and understand complex data distributions\, we als
 o need {\\em criticism} to explain what are \\textit{not} captured by prot
 otypes.  Motivated by the Bayesian model criticism framework\, we develop 
 \\texttt{MMD-critic} which efficiently learns prototypes and criticism\, d
 esigned to aid human interpretability. A human subject pilot study shows t
 hat the \\texttt{MMD-critic} selects prototypes and criticism that are use
 ful to facilitate human understanding and reasoning. We also evaluate the 
 prototypes selected by \\texttt{MMD-critic} via a nearest prototype classi
 fier\, showing competitive performance compared to baselines.
LOCATION:Area 5+6+7+8 #197
END:VEVENT
BEGIN:VEVENT
SUMMARY:Showing versus doing: Teaching by demonstration | Mark K Ho \, Mic
 hael Littman \, James MacGlashan \, Fiery Cushman \, Joe Austerweil \, Jos
 eph L Austerweil
DTSTART;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T180000
DTEND;TZID=Europe/Madrid;VALUE=DATE-TIME:20161207T213000
DESCRIPTION:Poster:Showing versus doing: Teaching by demonstration\nMark K
  Ho \, Michael Littman \, James MacGlashan \, Fiery Cushman \, Joe Austerw
 eil \, Joseph L Austerweil\nhttp://nips.cc/Conferences/2016/Schedule?showE
 vent=7429\n\nPeople often learn from others' demonstrations\, and classic 
 inverse reinforcement learning (IRL) algorithms have brought us closer to 
 realizing this capacity in machines. In contrast\, teaching by demonstrati
 on has been less well studied computationally. Here\, we develop a novel B
 ayesian model for teaching by demonstration. Stark differences arise when 
 demonstrators are intentionally teaching a task versus simply performing a
  task. In two experiments\, we show that human participants systematically
  modify their teaching behavior consistent with the predictions of our mod
 el. Further\, we show that even standard IRL algorithms benefit when learn
 ing from behaviors that are intentionally pedagogical. We conclude by disc
 ussing IRL algorithms that can take advantage of intentional pedagogy.
LOCATION:Area 5+6+7+8 #198
END:VEVENT
END:VCALENDAR
